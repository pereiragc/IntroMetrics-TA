#+TITLE: Fall 19: Intro to Econometrics TA material  
#+AUTHOR: Gustavo Pereira
#+STARTUP: beamer


* README page
  :PROPERTIES: 
  :EXPORT_FILE_NAME: README.org
  :EXPORT_TITLE: 
  :END:
** Intro to Econometrics: TA repo
   Welcome! This repository was created to store and maintain the materials
   used or referred to in the recitations. 
  
   Schedule: 
   | what           | when                | where                           |
   |----------------+---------------------+---------------------------------|
   | *Recitation*   | Monday 13:10-14:25  | 227 Seeley W. Mudd building     |
   | *Office hours* | Tuesday 09:00-10:00 | Lehman Library group study area |
  
   You can find lectures, slides and problem sets in the [[https://jm4474.github.io/Courses-IntroEconometrics-Ph.D/][class page]]. 

   Pull requests are encouraged!
  
*** Questions outside OH
    See [[outside_oh_questions.pdf][here]]!

      

   

* Questions outside office hours
  :PROPERTIES: 
  :EXPORT_FILE_NAME: outside_oh_questions.pdf
  :EXPORT_TITLE: Out-of-OH Q&A   
  :EXPORT_AUTHOR: Gustavo Pereira
  :EXPORT_OPTIONS: ^:nil
  :END: 
** Why
   The ideal time to ask questions about the material and problem sets is during
   office hours. However, many of you can't make it to office hours for valid
   reasons, so I also try to respond to questions sent by e-mail or other
   means.

   Keeping this document helps you (and yours truly) for a few reasons: 
   - More clarifications/explanations available
   - There is no `unfair advantage' to those who ask many questions
   - I don't have to answer the same thing over and over :)

     #+LATEX: \clearpage
** Pset 1
    
*** About limits of sets, and Durrett and Billingsley being wrong
    #+begin_quote
    ``Hi Gustavo,
     
    Thank you for the class today. Can I further clarify Q4 you briefly
    discussed in class? I initially directly used $\lim(-\infty,x) \to \Omega$ in
    the steps, but you pointed out it was wrong. I'm actually still a bit
    confused about why it is wrong. I referred to *Durrett* (who directly used
    $\lim(-\infty,x) \to \Omega$) and *Billingsley* (who states "clearly"). So,
    unfortunately, they both would get the proof wrong. Would it be possible
    for you to give me some hints on which theorems would be useful in the
    proof? Thank you so much!
    
    [screenshot of Durrett's book, theorem 1.1]
     
    Best,
    
    xx''
    
    (I kept the bold face from the e-mail)
    #+end_quote

    Dear xx, 
    
    I wouldn't dare to say that Durrett would get it wrong if he was answering
    the problem set! But definitely, if he just copied and pasted from his
    own book, he would be discounted.
    
    Here's the reason: earlier in the book, he states the continuity from above
    and below of probability measures in terms of collections of sets /indexed
    by natural numbers/. In order to do that, he defines what it means to say
    \[  A_i \uparrow A \]
    for that class of collections.
    
    Later, in the context of proving the limits of CDFs, he applies a
    similar statement to the collection $\{ (-\infty, x] \}_{x\in \mathbf R}$.
    The problem is that it's not indexed by natural numbers. So if he wanted
    full credit, he'd need to clarify he meant by
    \[ (-\infty, x] \uparrow \mathbf R \]
    and why continuity of $\mathbf P$ -- defined only for limits of
    ``increasing'' sets indexed by natural numbers -- also applies for these
    limits.

    
    The hint is the same one I gave in the recitation. Use the fact (no need to
    prove it) that
    \[ \lim_{x\to\infty} F(x) = 1 \]
    if, and only if, for every /increasing/ diverging sequence $x_n \uparrow \infty$, 
    \[  F(x_n) \to 1 \] 
    and try to apply countable additivity.
    

    Sincerely, 
    
    Gustavo

** Pset 4
   
*** Clarifying the meaning of posterior in Q3
    #+begin_quote
    In Q3 of PS4, we are asked to compute the posterior mean for $\beta$ and
    $\sigma$, and I assumed that usually meant the expectation of the conditional
    distribution of $\beta$, conditioned on data, and similarly, the conditional
    distribution of $\sigma$ on the data.

    In the hint, I'm questioning my understanding because $\beta$ was conditioned on
    $\sigma$ as well. Going forward, does that mean posterior distributions condition
    on data and all other parameters except the parameter in question? More
    specifically, do you know of any resources where I could read up on the
    mechanics behind this?
    #+end_quote
    
    Computing the distribution of beta given Y and sigma is only supposed to be an intermediate step to make calculations easiser. 

    The end goal is to find the joint distribution of beta and sigma given data. 
    

    
*** Q4 - all $\beta$ vs some $\beta$
    #+BEGIN_quote
    I was working on question 4 and I realized that I have a bit of a gap in my
    understanding about admissible decision rules. I know that a Ridge estimator
    is a Bayesian estimator, and thus is admissible. I know that an admissible
    decision rule is not dominated, so since the OLS estimator is another
    decision rule, it is not the case that the risk of the OLS estimator is less
    than or equal to the risk of the Ridge estimator for all beta and strictly
    less than for some beta. So then, as I understand it, this implies that
    there is *some* beta for which the OLS estimator has strictly higher risk
    than the Ridge estimator (but I know nothing about how they compare for any
    other beta). But, I don't think this tells me anything about how the risks
    compare for all the other betas, right?

    Since in this problem we'd have that the risk for each estimator is the mean
    squared error, and thus bias + tr(variance), I think I'd want to have an
    inequality the compares the MSE of the Ridge estimator to the MSE of the OLS
    estimator, but from the fact that the Ridge estimator is admissible I only
    see how to write that inequality for *some* beta -- how can I extrapolate to
    being able to write an inequality for *all* beta, in order to be able to
    make a statement about how tr(var(beta_ols)) compares to tr(var(beta_ridge))
    compare?
    #+end_quote
    
    You're in the right track, but let me point something out first. 
    
    Admissibility guarantees that 
    \[ \text{MSE}(\hat\beta^{\text{Ridge}}, \beta) \leq \text{MSE}(\hat\beta^{\text{OLS}}, \beta) \] 
    holds for at least one $\beta$. Rigorously speaking, the inequality is not
    necessarily strict, because they could have the same risk all over the parameter
    space and that wouldn't violate admissibility. 

    Now with that caveat, I would suggest that instead of trying to get a
    comparison that holds for every possible $\beta$, try to use the one $\beta$
    you know exists for which the ranking of MSEs above holds. Write down both
    sides of the inequality for that particular $\beta$, and see if you can
    compare either side with $V_\beta(\hat\beta^{\text{Ridge}})$ for that $\beta$.
    
    
* Notes
** Recitation 1
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation1.pdf
   :EXPORT_TITLE: Recitation 1
   :EXPORT_OPTIONS: toc:nil
   :EXPORT_LATEX_HEADER: \input{auxfiles/header_basic.tex}
   :END: 

   In this recitation, I review the material presented in lectures 1 and 2. I
   also cover some things that might be challenging in the first problem sets. 
   
*** Review: lectures 1 and 2
    - Definition of probability space: $(\Omega, \mathcal F, \mathbf P:\mathcal F \to [0,1])$
      - The point of $(\Omega, \mathcal F)$ is to provide a model for the
        /randomness of some outcome/.
      - Remember: we don't observe randomness. We observe some outcome. Then, we
        use a model to infer what are more or less likely ``states of the world'',
        because that allows us to predict things
      - The reason we keep $\Omega$ abstract (instead of focusing on say
        $\Omega=[0,1]$) is that it allows us to deal with a variety of possible
        structures for the outcome space!
    - Random variables: /measurable/ functions $X:\Omega \to S$ where $S$ is some
      space of outcomes.
    - Probability space induced by a random variable
      - Original space: $(\Omega, \mathcal F, \mathbf P)$
      - RV `measurably' maps original space to $(S, \mathcal S)$
      - Induced measure: $\mathbf P_X(F) = \mathbf P\left\{ \omega: X(\omega) \in F \right\}$ for $F \in \mathcal S$
        - Curiosity: this is called a push-forward measure in mesasure theory
      - Probability space $(S, \mathcal S, \mathbf P_X)$ is typically some
        Euclidean space (though it can be more complicated)
    - Let's now focus on the case when $X:\Omega \to S$ is real valued, ie, $S=\mathbf R$.
    - CDF of a random variable: $F_X(x) = \mathbf P\left\{ \omega: X(\omega) \leq x \right\} = \mathbf P_X((-\infty, x])$
      - Result: all information in $\mathbf P_X$ is in $F_X$ and vice-versa.
      - Properties of CDF
        1. $F_X$ is non-decreasing
        2. $\lim_{x\to\infty} F_X(x) = 1$
        3. $\lim_{x\to-\infty} F_X(x) = 0$
        4. $F_X$ is right continuous
      - *First main result*: every function $F$ satisfying all four properties
        above is the CDF of some random variable.
    - Absolutely continuous random variable: $\exists f_X$ such that
      \[ F_X(x) = \int_{-\infty}^x f_X(z) dz \]
      + Weirdly enough, the non-obvious thing about the statement above is not
        the $\exists f_X$ but the $dz$. 
      + Measure theoretic details aside, the important thing is that $dz$ is
        never a jump.
        + If $X$ has a mass at some point $x_0$ in the real line -- meaning that
          the $\mathbf P_X(\{x_0\}) > 0$, there will be a jump in $F_X$ at $x_0$. 
        + We can't have that becasuse $F_X(x_0) - F_X(x_0 - \epsilon) \approx f_X(x_0)\epsilon$
        + For $\epsilon > 0$ small enough, mass at $x_0$ would imply the LHS is
          $\mathbf P\{x_0\}$ while the RHS should be zero
      + Optional comment: in fact every $F_X$ has an associated $f_X$ with
        respect to /some/ (generally non-uniform) measure. This is the
        consequence of a more general result called the /Radon-Nikodym theorem/.
    - Expectation of absolutely continuous RV: 
      \[ \mathbf E[g(X)] = \int_{\mathbf R} g(z) f_X(z) dz  \]
      + ``Law of the unconscious statistician''
    - Moment generating function
      \[ m_X(t) = \mathbf E\left[ e^{tX}\right]=\int_{\mathbf R} e^{tx} f_X(x)dx\]
      + The i-th moment of $X$ can be found by taking the $i-th$ derivative of
        $m_X(t)$ and evaluating it at zero.
        + For this to be meaningful, the MGF must be well defined in $(-\epsilon, \epsilon)$ for some $\epsilon$
        + Then for example $m_X'(t) = \mathbf E[X e^{tX}]$
    - *Second main result.* Let $X_1$ and $X_2$ be st 
      \[ m_{X_1}(t) = m_{X_2}(t) \]
      for all $t$. Then $F_{X_1} = F_{X_2}$.  
      + This essentially means that all information contained in $F_X$ is also
        contained in $m_X(t)$
    - Note: take the Taylor series of exponential around $0$ and take
      expectations,
      \[m_X(t) = \sum_{n=0}^\infty \frac{t^n \mathbf E(X^n)}{n!}\]
      + It is tempting to that knowledge of moments determines the distribution
        of $X$. This is not the case, however, because sometimes the series
        above doesn't converge even when all moments exist. 
        
    # Examples. 
    # 1) $\Omega = \{1,2,3\}, S=\{a,b,c\}$.

    #    What is the measurability requirement doing? Suppose we have
    #    $\sigma-\text{algebras}$ $\mathcal F=\{\emptyset, \{1\}, \{2,3\}, \Omega\}$ and $\mathcal S = 2^S$.
       
    #    Because neither $2$ nor $3$ show up separately in $\mathcal F$, observing
    #    a random variable $X:\Omega\to S$ should not allow us to distinguish them.

    #    For example, a random variable such as
    #    \[X(1) = a, X(2) = b, X(3)=c\]
    #    would allow us to distinguish $2$ and $3$! Indeed, if $2$ is observed, we
    #    know for sure that $\omega=2$, but $\{2\}$ isn't in $\mathcal F$.
       
    #    In a sense, the measurability requirement is imposing consistency in what
    #    we can learn about the underlying state based on observing an outcome.
    #    In the above example, measurability implies that $X(2) = X(3)$.
       
    # 2) Take $\Omega$ to be the $[0,1]$ interval with the uniform probability $\lambda$, ie, 
    #    \[ \lambda( [a,b] )  = b - a \]
    #    for all intervals $[a,b]$.  

*** Problem 4 is not as easy as it might seem
    
    Consider the proof, for example, that $F_X \to 1$ as $x\to\infty$. (The case
    of $x\to0$ is similar.)
    
    We know that: 
    1) $F(x) = \mathbf P\{\omega: X(\omega) \leq x \}$
    2) $\{\omega: X(\omega) \leq x\} \uparrow \Omega$
    3) $\mathbf P(\Omega) = 1$
       
    So it must be the case that $F(x) = P\{\omega: X(\omega) \leq x\} \uparrow \mathbf P(\Omega) = 1$,
    isn't that right? Well, *no*. While that reasoning is in some sense in the
    right direction, at the very least it's an incomplete argument for two reasons.
    
    - We haven't defined convergence of sets as in (2). Unless you can make that
      statement rigorous somehow, using it is not fair game. 
    - More importantly, when we took the statements together, we missed an
      important step: proving that (whatever the first arrow means)
      \[ A_x \uparrow \Omega \implies \mathbf P(A_x) \uparrow \mathbf P(\Omega) \]
    
    The second step above is essentially the point of the exercise. Hint for
    actually solving the problem:
    - Use the fact that 
      \[ \lim_{x\to\infty} F(x) = L\] 
      if, and only if $F(x_n) \to L$ for all increasing sequences $x_n \to \infty$
    - Show that for any probability measure, if $x_n \uparrow \infty$
      \[ \mathbf P\{ \omega: X(\omega) \leq x_n \} \to \mathbf P(\Omega) = 1 \] 
      
      You will need to use /countable/ additivity for this.
      
    For the right-continuity part, one useful way of checking your proof is to
    make sure you understand why your proof doesn't apply to the left limit. 
** Recitation 2
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation2.pdf
   :EXPORT_TITLE: Intro to Econometrics: Recitation 2
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_LATEX_HEADER: \input{auxfiles/header_beamer.tex}
   :END: 
*** Review Part
**** Review
     #+BEAMER: \framesubtitle{Random variables - \emph{univariate} case} 
     #+BEAMER: \begin{center} $(\Omega, \mathcal F, \mathbf P)$ \end{center}
     
     - $X:\Omega\to\mathbf R$
     - CDF:
       \[ F_X(x) = \mathbf P( \left\{\omega: X(\omega) \leq x\right\}) \]
       + Completely characterizes $\mathbf P\{X \in B\}$ for $B \subset \mathbf R$
     - Absolutely continuous: 
       \[F_X(x) = \int_{-\infty}^x f_X(x) dx\]
**** Review
     #+BEAMER: \framesubtitle{Random variables - \emph{multivariate} case} 
     #+BEAMER: \begin{center} $(\Omega, \mathcal F, \mathbf P)$ \end{center}
     
     - $X:\Omega\to\mathbf R^S$ where $X(\omega) = (X_1(\omega),\ldots, X_S(\omega))'$
     - CDF:
       \[ F_X(x_1, \ldots, x_S) = \mathbf P( \{\omega: X_1(\omega) \leq x_1, \ldots, X_S(\omega) \leq x_S  \}) \]
       + Completely characterizes $\mathbf P\{X \in B \}$ for $B\subset \mathbf R^S$
     - Absolutely continuous: 
       \[F_X(x_1, \ldots, x_S) = \int_{-\infty}^{x_1}\cdots\int_{-\infty}^{x_S} f_X(x_1, \ldots, x_S) dx_S \cdots dx_1\]
**** Review 
     #+BEAMER: \framesubtitle{Random variables - \emph{multivariate} case} 
     - <1-> Result: if $F:\mathbf R\to[0,1]$ is
       1. Increasing
       2. Right-continuous
       3. Satisfies $\lim_{x\to\infty} F(x) = 1 - \lim_{x\to-\infty} F(x) = 1$
       Then it is the CDF of some random variable $X:\Omega\to\mathbf R$
     - <2-> Can you think of (or prove?) an S-dimensional analog of the statement above?
**** Review 
     #+BEAMER: \framesubtitle{Random variables - \emph{multivariate} case} 
     - If $F:\mathbf R^2\to[0,1]$ is
       1. <1-> Increasing
       2. <1-> ``Continuous from above''
       3. <1-> Has the following limits:
          1. $\lim_{x_1 \to -\infty} F(x_1, x_2) = 0$ for all $x_2$
          2. $\lim_{x_2 \to -\infty} F(x_1, x_2) = 0$ for all $x_1$
          3. $\lim_{x_1 \to \infty} \lim_{x_2 \to \infty} F(x_1, x_2) = 1$
       4. <2-> Satisfies, for $x_1^* \geq x_1$ and $x_2^* \geq x_2$,
          \[ F(x_1^*, x_2^*) - F(x_1^*, x_2) - F(x_1, x_2^*) + F(x_1, x_2) \geq 0 \]
       Then $F$ is the CDF of a random variable $X:\Omega\to\mathbf R^2$
       
     (Durrett, sec 2.9)
**** Review
     #+BEAMER: \framesubtitle{Marginals} 
     
     - <1-> Marginal with respect to coordinate $s$, $F_s : \mathbf R \to [0,1]$
       \[ F_s(x) = \mathbf P(\left\{ \omega: X_s(\omega) \leq x \right\})  \] 
     - <2-> How do you obtain it?
     - <3-> Just take limits. Suppose $S=2$ and we want to recover first coordinate:
       \[ F_1(x_1) = \lim_{x_2 \to \infty}  F(x_1,x_2)  \]
       
       Proof? 
**** Review
     #+BEAMER: \framesubtitle{Marginals} 
     
     - How do you recover a marginal pdf? Suppose $X:\Omega\to\mathbf R^2$ has pdf $f(x_1,x_2)$:
       \[f_1(x_1) = \int_{-\infty}^\infty f(x_1, x_2) dx_2\]
     - Proof? 
**** Review
     #+BEAMER: \framesubtitle{Digression: marginals don't determine joints} 
     
     - A very useful counterexample: 
       - <1-> Let $X \sim N(0,1)$
       - <2-> Let $W$ be independent of $X$; 
         \[ \mathbf P(W = 1) = \mathbf P(W = -1) = \tfrac{1}{2}\]
       - <3-> Define $Y = WX$. Claim: $(X,Y)$ has normal marginals, but $(X,Y)$ is not jointly normal.
         \begin{align*}F_Y(y) = \mathbf P(WX \leq y) &= \frac{1}{2} \mathbf P(X \leq y) + \frac{1}{2} \mathbf P(-X\leq y) \\ 
           &= F_X(y)\end{align*}
         So marginals of $(X,Y)$ are the same
       - <4-> $(X,Y)$ is not multivariate normal. Why? 
       - <5-> $X+Y$ has a  mass at zero, with probability $\frac{1}{2}$!
**** Review
     #+BEAMER: \framesubtitle{Digression: marginals don't determine joints} 

     \centering \includegraphics[scale=0.4]{./codes/Notes_PS2_simunormal.pdf}    
**** Review
     #+BEAMER: \framesubtitle{Moments of multivariate RVs} 
     - Focus on the case when there is a pdf
     - <1-> ``Definition''
       \[  \mathbf Eg(X) =  \int_{\mathbf R^S} g(x) f_X(x)dx   \]
     - <2-> First moment: 
       \[ \mu_X =  \mathbf EX \]
     - <3-> Second moment: 
       \[ V(X) = \mathbf E \left[ (X - \mu_X)(X - \mu_X)' \right] \]
       #+BEAMER: \vspace{-0.3cm}
       - When is $V(X)$ finite?
     - <4-> Covariance btw X and Y: 
       \[ \cov(X,Y) = \mathbf E \left[ (X - \mu_X)(Y-\mu_Y)' \right] \]
**** Review
     #+BEAMER: \framesubtitle{Moment generating functions of multivariate RVs} 
     - <1-> MGF: 
       \[  m_X(\mathbf t) = \mathbf E\left[ e^{\mathbf t'X} \right] = \mathbf E\left[ e^{\sum_{i=1}^S t_i X_i} \right]  \]
     - <2-> Result: suppose  $X$ and $Y$ have a moment generating function, and 
       \[ m_X(\mathbf t) = m_Y(\mathbf t)\]
       for all $\mathbf t$. Then $F_X(\mathbf t) = F_Y(\mathbf t)$ for all $\mathbf t$.
     - <3-> Result (stronger):  suppose that, for all $\mathbf t \in \mathbf R^S$, $\alpha \in \mathbf R$, 
       \[ \mathbf P\{ \mathbf t'X \leq \alpha \} = \mathbf P\{ \mathbf t'Y \leq \alpha \} \]
       then $F_X(z) = F_Y(z)$ for all $z\in\mathbf R^S$
*** PSet
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections} 

     Let $(V, \langle\cdot,\cdot\rangle)$ be a vector space with an inner product. 
     - <2-> Orthogonal projection of $v$ into (closed) $W\subseteq V$:
       \[ v - \proj_W(v)\perp w \]
       for all $w\in W$
***** Projection in a Hilbert Space 
      :PROPERTIES: 
      :BEAMER_env: theorem
      :BEAMER_opt: shadow=true
      :BEAMER_act: 3
      :END:
      
      Let $W\subset V$ be a closed vector subspace of $V$. 

      For any $v \in V$, the distance minimization problem
      \[\min_{w\in W} \| v - w \|\]
      has a unique solution $w^* \in W$. Moreover, $w^* = \proj_W(v)$.
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections} 
     What if $W$ has a finite basis? 
     \[ W = \vsp \{w_1, \ldots, w_K\}\]
     - Orthogonal projection of $v$ into $W$ is 
      \[  \proj_W(v) = \sum_{i=1}^K \frac{\langle w_i, v\rangle}{\langle w_i, w_i\rangle} w_i  \]

     Using this result in the pset is fair game 
     
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections} 
     
     Space $V = \{ X:\Omega\to\mathbf R^S: \mathbf E\|X\|^2 < \infty \}$ is a Hilbert
     space with 
     \[ \langle X, Y\rangle = \mathbf E XY\]
      
     - <2-> Fix variables $X$, $Y$ in $V$ and consider the subspace
       \[ W = \{ Z: \Omega \to \mathbf R : Z = \alpha + \beta (X - \mu_X)\} \] 
       (Is there a finite basis for $W$?)
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections}
     The problem
     \[  \min_{(\alpha, \beta)} \left[ Y - \alpha - \beta(X-\mu_X) \right]^2 \]

     is equivalent to some norm minimization problem involving $Y, X$ and $W$.

     What is it?
** Recitation 3
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation3.pdf
   :EXPORT_TITLE: Intro to Econometrics: Recitation 3
   :EXPORT_BEAMER_THEME: Boadilla
   :EXPORT_LATEX_CLASS_OPTIONS: [presentation, smaller]
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_beamer.tex}
   :END:
   
*** Outline
**** Outline
     - Review: 
       + Statistical model
         * Definition
         * Examples
         * Identification, sufficiency 
       + Statistical decision problem
         * Definition
         * Examples
*** Statistical model
**** Statistical model
     #+BEAMER: \framesubtitle{Definition}
     - <1-> Idea: formalize statements such as
       1. Let $\{h_1, \ldots, h_{10}\}$ denote the outcome of $10$ independent
          coin flips with probability $p$ of landing heads
       2. <2-> ``Let ${X_1, X_2, X_3}$ be iid uniform in $[0,\theta]$ where $\theta$ is an unknown positive real number''
       3. <3-> ``Let $\{Y_t\}_{t\in1,2,\ldots, T}$ be an AR(1) process with gaussian innovations''

     - <4-> *Claim.* All statements equivalent to: ``let $\mathbf X$ be
       a draw from some cdf $F:\mathbf R^S \to [0,1]$ where $F$ is taken from some restricted set of CDFs, 
         \[F \in \mathfrak F\text{ ''}\]
**** Statistical model
     #+BEAMER: \framesubtitle{Definition}
     - <1-> It's common to write 
       \[ \mathfrak F = \{ F_\theta \}_{\theta \in \Theta} \]
     - <2-> For example: 
       \[\mathfrak F = \left\{ F:\mathbf R\to\mathbf R | F\text{ is the cdf of }  U[a,b] \text{ for some }a\leq b\right\}\]
       #+BEAMER: \vspace{-0.5cm}
       - Does this  represent a statistical model?
     - <3-> We can define for $\theta = (a,b)$, 
       \[ F_{\theta} = \frac{t-a}{b-a} \mathbf 1_{[a,b]}(t) \]
     - <4->  With that indexing, 
       \[ \mathfrak F = \{F_{\theta} \}_{\theta \in \Theta}\]
       where $\Theta = \{(x,y) \in \mathbf R^2 : x \leq y\}$
**** Statistical model 
     #+BEAMER: \framesubtitle{Comment}
     - <1-> Why do we specify models with CDFs?
     - <2-> Reason: in Euclidean spaces, distribution of random variables is fully characterized by CDF
     - <3-> However, if all CDFs in your model are absolutely continuous, it's
       equivalent to specify a family of PDFs
     - <4-> In the course, we will do this interchangeably; if a model is
       specified in terms of PDFs, it's understood that we're considering only absolutely continuous distributions
     - <5-> We can also specify the model with more general probability distributions: 
       \[ \{P_\theta: \mathcal B(\mathfrak X) \to [0,1]\}_{\theta \in \Theta} \]
       where $\mathfrak X$ a possibly more general space (e.g., a space of bounded continuous functions) 
**** Statistical model
     #+BEAMER: \framesubtitle{Example 1: ten coin flips}
     - <1-> Single coin flip: 
       \[ F_p^1(x) = \begin{cases} 0 & \text{if } x < 0 \\ 1 - p & \text{if } x \in [0,1) \\ 1 & \text{otherwise} \end{cases}\]
     - <2-> Then the joint is  $F_p(h_1, h_2, \ldots, h_{10}) = F_p^1(h_1) \cdots F_p^1(h_{10})$
     - <3-> Model: 
       \[ \{F_p\}_{p \in [0,1]} \]
       + What is $\Theta$ ? 
**** Statistical model
     #+BEAMER: \framesubtitle{Example 2: Uniform $[0,\theta]$}
     - <1-> Three independent uniform $[0,\theta]$. We know that for a given $\theta$
       \[ F_\theta^2(t) = \frac{t}{\theta} \mathbf 1_{[0,\theta]}(t) \]
       is the cdf of $U[0,\theta]$ for non-negative $\theta$.
     - <2-> Thus joint is 
       \[ F_\theta(x_1, x_2, x_3) = F^2_\theta(x_1) F^2_\theta(x_2) F^2_\theta(x_3)\]
       and statistical model is \[ \{ F_\theta \}_{\theta \in (0,\infty)} \]
**** Statistical model
     #+BEAMER: \framesubtitle{Example 3: AR(1) with Gaussian innovations}
     - <1-> An ``AR(1) with Gaussian innovations'' means that 
       \[ Y_t - \mu = \rho (Y_{t-1} - \mu) + \epsilon_t \] 
       where $\epsilon_t$ are drawn iid $N(0, \sigma^2)$. 
       - <1-> Note: need to make assumption about $Y_0$. Assume fixed.
     - <2-> Equivalently,
       \[ Y_t | Y_{t-1}, \ldots, Y_1 \sim N(\mu + \rho(Y_{t-1} - \mu) , \sigma^2) \] 
     - <3-> How do you write the joint CDF? By what parameters will it be indexed?
**** Statistical model 
     #+BEAMER: \framesubtitle{Identification \& sufficiency}
     
     - <1-> Summary of previous discussion: a statistical model is a family of
       distributions, $\{F_\theta: \mathbf R^S \to [0,1]\}_{\theta\in\Theta}$.
     - <2-> If each $\theta \in \Theta$ induces a unique distribution, the model is called *identified*.
     
       + <3-> Mathematically: the model is identified iff for every $\theta \ne
         \theta'$,
         there exists $x \in \mathbf R^S$ such that $F_\theta(x) \ne F_{\theta'}(x)$
       + <4-> What if the model was specified in terms of PDFs? What about general probability distributions?
     - <5-> A /statistic/ is any function $T:\mathbf R^S \to \mathbf R^K$. We
       say that $T$ is *sufficient* if \[ \mathbf P_\theta( \cdot | T(\cdot)) \]
       does not depend on $\theta$. Intuitively, if you condition on $T(X)$, the
       full data become uninformative about $\theta$.
       
**** Statistical model 
     #+BEAMER: \framesubtitle{Identification \& sufficiency}
     - <1->  Example: let $X_1$ and $X_2$ be iid $N(\mu, 1)$.
       + <2-> Model here is $\{F_\mu\}_{\mu \in \mathbf R}$ where $F_\mu$ is cdf
         of independent joint normal with mean $(\mu,\mu)$ and identity variance matrix
     - <2->  Then $T(X_1, X_2) = X_1 + X_2$ is sufficient.
     - <3-> Before proof: note that crucially the data is 2 dimensional, but the sufficient statistic is 1d 
     - <4-> Now: 
       \[\begin{bmatrix} X_1 \\ X_2 \\ T(X_1, X_2) \end{bmatrix} \sim \mathcal N_3 \left(  \begin{bmatrix} \mu \\ \mu \\ 2\mu \end{bmatrix}, 
         \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 1 & 2 \end{bmatrix} \right) \]
     - <5-> To find conditional distribution of $X_1$ and $X_2$ given $T(X_1, X_2)$, use the BLP trick. 
**** Statistical model 
     #+BEAMER: \framesubtitle{Identification \& sufficiency}
     - <1-> Math: 
       \[ E[X_1 | X_1 + X_2] = E[X_2 | X_1 + X_2] = \frac{X_1  + X_2 }{2} \] 
       moreover, conditional variance also doesn't depend on $\mu$ 
       
*** Statistical decision problem
**** Statistical decision problem
     #+BEAMER: \framesubtitle{Definition}
     - Definition: statistical decision problem is 
        \[ (\Theta, A, \mathcal L, \{F_{\theta}\}_{\theta\in\Theta}) \]
       where 
       1. <2-> $\Theta$ is a parameter space
       2. <3-> $A$ is a space of actions
       3. <4-> $\mathcal L$ is a utility/loss function
       4. <5-> $\{F_\theta\}$ is a statistical model
          + Remember: this can be alternatively specified as $\{P_\theta\}_{\theta\in\Theta}$ or $\{f_\theta\}_{\theta\in\Theta}$
**** Statistical decision problem 
     #+BEAMER: \framesubtitle{Interpretation}
      + Statistician is supposed to decide something. Examples: 
        1. <2-> Pick the $\theta$ that she thinks generated the data
           \[  A  = \Theta \] 
        2. <3-> Given a split $\Theta = \Theta_0 \sqcup \Theta_1$, pick which of
           $\Theta_0$ or $\Theta_1$ is more likely to contain the parameter that generated data
           \[ A = \{0, 1\} \] 
        3. <4-> Pick a subset $C \subseteq \Theta$ where she thinks the true $\theta$ falls in
           \[ A = \text{reasonable subsets of }\Theta \] 
**** Statistical decision problem 
     #+BEAMER: \framesubtitle{Interpretation}
     + <1-> Model this as a sequential game. 
       - <2-> /First stage:/ Nature picks $\theta \in \Theta$. This is not observable by statistician
       - <3-> /Stage $1\frac{1}{2}$:/ Nature randomly draws $X \sim F_\theta$
       - <4-> /Second stage:/ Statistician chooses action $a$
     + <5-> At the terminal nodes, statistician gets the loss $\mathcal L(a, \theta)$
     + <6-> Let $\mathfrak X \subset \mathbf R^S$ denote the (common) support of $F_\theta$. A
       *strategy* for the statistician in this game is a function 
       \[ d : \mathfrak X \to A  \]
       I.e. a specification of an action for every possible decision node she faces
       
       This strategy is called a decision rule in the mathematical statistical jargon 
**** Statistical decision problem 
     #+BEAMER: \framesubtitle{Risk function} 
     - <1-> What sort of criterion should we use to rank decision rules?
     - <2-> We use the expected utility paradigm. For fixed $\theta$, we postulate that
       \[  d_1(\cdot) \precsim_\theta d_2(\cdot) \iff \mathbf E_\theta \left[  \mathcal L(d_1(X), \theta)   \right]
       \geq \mathbf E_\theta \left[  \mathcal L(d_2(X), \theta)   \right]\] 
       #+BEAMER: \vspace{-0.5cm}
       - With respect to what are we taking the expectation?
     - <3-> This expectation is called /risk/. Notation: 
       \[ R(d, \theta) := \mathbf E_\theta \left[ \mathcal L(d(X, \theta) \right] = \int_{\mathbf R^S} d(x, \theta) dF_\theta(x) \]
     - <4-> Analogy  with game theory: /dominated/ strategies
       - A decision rule that is not weakly dominated is called admissible
** Recitation 4
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation4.pdf
   :EXPORT_TITLE: Intro to Econometrics: Recitation 4
   :EXPORT_BEAMER_THEME: Boadilla
   :EXPORT_LATEX_CLASS_OPTIONS: [presentation, smaller, handout]
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_beamer.tex}
   :END:
*** Recitation 4
**** Roadmap for today
     - Review:
       1) Statistical problem
       2) Bayes rules, expected posterior loss
     - PS3

**** Review: Statistical Problem 
     #+ATTR_beamer: :overlay +-
     - Components of a decision problem:
       + Statistical model: $\{P_\theta\}_{\theta \in \Theta}$
       + Action space $\mathcal A$
       + Loss function $\mathcal L:\mathcal A\times \Theta \to \mathbf R$
       + Decision rules: $d:\mathfrak X\to\mathcal A$
     - Risk function: expected loss from decision $d$ when parameter is $\theta$: 
       \[R(d(\cdot), \theta) = \int_{\mathfrak X} \mathcal L(d(x), \theta) f_\theta(x)dx \]
     
***** Comments                                                     :noexport:
      - Note that $\mathcal L(a, \theta)$ ranks actions for fixed $\theta$
      - Note that $R(d(\cdot), \theta)$ ranks decision rules (functions) for fixed $\theta$
        \[ d_1(\cdot) \succsim d_2(\cdot) \iff R(d_1(\cdot), \theta) \leq R(d_2(\cdot), \theta) \]
      - In that sense, "risk" is just like a "generalized loss" where the
        "generalized action space" is the space of all strategies
**** Review: Admissibility      
     #+ATTR_beamer: :overlay +-
     - Decision rule $d_1$ is /dominated/ by $d_2$ iff, for all $\theta \in \Theta$,
       \[ d_1 \precsim_\theta d_2  \]
       and $d_1 \prec_{\theta_0} d_2$
       for at least one $\theta_0$
       + What does it mean for a rule to be /not dominated/ by another rule? 
     - A rule $d$ that is not dominated by any other rule is called /admissible/
       + Expand the definition of admissible 
     - It is generally hard to find admissible rules.
**** Review: priors, posteriors, etc...  
     #+ATTR_beamer: :overlay +-
     - Suppose model is $\{f_\theta\}_{\theta \in \Theta}$, i.e., data has a density for all possible parameters
     - Suppose also $\Theta \subseteq \mathbf R^k$, and pdf $\pi(\theta)$ summarizes some prior belief about $\theta$
       - With this, we're interpreting the parameter $\theta$ as a /random variable/
       - Before the prior was introduced, $\theta$ was merely an index
     - With this structure, we can define the induced joint density of data and parameters,
       \[  f(x, \theta; \pi) = f_\theta(x) \pi(\theta)  \]
       + Does this integrate to one?
**** Review: priors, posteriors, etc...
     #+ATTR_beamer: :overlay +-
     - Given induced joint density, 
       \[ f(x | \theta; \pi) = \frac{f_\theta(x) \pi(\theta)}{\pi(\theta)} = f_\theta(x)\]
       #+BEAMER: \vspace{-0.3cm}
     - What about the marginal of data?
       - Recover it by integrating $\theta$ out:
         \[ f(x; \pi)  = \int_{\theta \in \Theta} f_\theta(x) \pi(\theta) d\theta \]
       #+BEAMER: \vspace{-0.3cm}
     - Conditional density of parameter given data?
       \[f(\theta | x; \pi) = \frac{f(x,\theta; \pi)}{f(x;\pi)} = \frac{f_\theta(x)\pi(\theta)}{\int_{\theta\in\Theta} f_\theta(x)\pi(\theta)d\theta}\]
       #+BEAMER: \vspace{-0.3cm}
       - This is called /posterior density/ in Bayesian jargon
**** Review: Bayes rules
     #+ATTR_beamer: :overlay +-
     - Let's go back to the statistical decision problem
     - Let $d(\cdot)$ be a decision rule, and $\pi$ a prior density over $\Theta$
     - Bayes risk of $d(\cdot)$ given $\pi$ is
       \[\begin{aligned} r(d(\cdot), \pi) &= \int_\Theta  R(d(\cdot), \theta) \pi(\theta)d\theta \\
                                          &= \int_\Theta \int_{\mathfrak X} \mathcal L(d(x), \theta) f(x, \theta; \pi)  dx \,d\theta  \end{aligned}\]
     - A /Bayes decision rule/ $d^*$ is one that minimizes Bayes risk given a prior $\pi$. 
       \[ d^*_\pi(\cdot) = \arg\min_{d(\cdot)} r(d(\cdot), \pi) \]
     - Important feature: /under mild assumptions, Bayes rules are admissible/
     
**** Review: finding Bayes rules
     #+ATTR_beamer: :overlay +-
     - Rewrite the Bayes risk using Fubini's theorem
       \[\begin{aligned} r(d(\cdot), \pi) &= \int_{\mathfrak X} \left[ \int_\Theta  \mathcal L(d(x), \theta) f(\theta | x; \pi)  d\theta\right] f(x; \pi) dx\\
                           &=  \int_{\mathfrak X} \psi(d(x), x) f(x; \pi) dx \end{aligned}\]
       where
       \[ \psi(a, x) = \int_\Theta \mathcal L(a, \theta) f(\theta | x; \pi) d\theta \]
     - Let $d^*(x) = \arg\min_{a\in\mathcal A} \psi(a,x)$
       + Immediate consequence: for any decision rule $d(\cdot)$,
         \[ \psi(d^*(x), x) \leq \psi(d(x), x) \] 
         #+BEAMER: \vspace{-0.5cm}
       + Important: optimization in space $\mathcal A$ is easier than in  the space of all $d:\mathfrak X \to \mathcal A$!
       
** Recitation 5
*** Estimation
    #+ATTR_beamer: :overlay +-
    - Estimation refers to the case when $\mathcal A = \Theta$
      - Interpretation: ``action'' is to guess the correct parameter
    - Common loss function:
      \[ \mathcal L(a, \theta) = \| a - \theta \|^2\]
    - Decision rule here called *estimator*
    - Example: *Maximum likelihood estimator* 
         \[ \hat\theta (x) = \max_{\theta \in \Theta} f_\theta(x) \]

*** Bayes estimation
    - A Bayes estimator for $\theta$ is a decision rule $\hat\theta$ that minimizes Bayes risk for some prior $\pi(\theta)$
**** Bayes rule for squared error loss
     If $\mathcal L(a,\theta) = \|a-\theta\|^2$, then the Bayes rule $\thetabayes(x)$ for a prior $\pi(\theta)$ satisfies 
     \[\thetabayes(x) =  \mathbf E \left( \theta | x \right) \]
     where the expectation above is taken with respect to 
     \[ f(\theta | x) := \frac{f_\theta(x) \pi(\theta)}{\int_\Theta f_\theta(x) \pi(\theta) d\theta } \]
     In other words, 
     \[ \thetabayes(x) = \int_\Theta \theta f(\theta|x) d\theta  \]
*** Bayes estimation
    #+attr_beamer: :overlay +-
    - What is the previous slide saying?
      1) The optimal decision rule is the integral shown
      2) One can *interpret* that as a posterior mean
*** Bayes estimation
    - Result shown for posterior mean is actually more general
    - Says that Bayes rule minimizes posterior loss
    - If loss function is different, minimal posterior loss could mean: 
      + Posterior mode
      + Posterior median
      + etc...
*** Bayes estimation
    #+attr_beamer: :overlay +-
    - Proof that $\thetabayes=\mathbf E(\theta|x)$
    - Relies on the following principle:
      + Let $F:\mathbf R^k \times \mathfrak X \to \mathbf R$ be some function
      + Suppose we're trying to find a *function* $u^*(x)$ such that
        \[ u^*(\cdot) = \arg\min_{u(\cdot)} \int_\mathfrak X F(u(x),x) \phi(x) dx   \]
      + Then one option is to pick for every $x \in \mathfrak X$,
        \[ u^*(x) =  \arg\min_{u \in \mathbf R^k} F(u,x) \]
    - Rest of the proof: board

    
** Unsorted
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/unsorted-notes.pdf
   :EXPORT_TITLE: Unsorted notes
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_basic.tex}
   :EXPORT_OPTIONS: toc:nil
   :END: 
   
   Here I store some random notes that I may or may note talk about during recitations.
   
*** Lectures 1 & 2
    - _Finite additivity_

      Let's define some notation. I can define the following for any indexed collection of sets $A_i$:
      \[A_1 + A_2 := A_1 \cup  A_2\]
      or, more generally
      \[
      \sum_i A_i := \bigcup_i A_i
      \]
      whenever the collection $A_i$ is pairwise disjoint.

      The idea of assuming additivity -- without any further qualification --
      is that set-function $\mathbf P$ satisfies some form of linearity, that is
      \[
      \mathbf P\left(   \sum A_i  \right) = \sum_i \mathbf P \left(  A_i \right)
      \]
      It turns out that the set of indices over which this assumption is made is
      consequential.

      We call $\mathbf P$ /finitely additive/ if the above is required to hold
      for all finite sets of indices. Similarly, if the relationship holds for
      countably many indices, $\mathbf P$ is called /countably additive/.
      
      Let's investigate an example of finitely, but not countably, additive
      measure. Here, we are working with a triple $(X, \mathcal A, \mathbf P)$.
      $\mathcal A$ is an /algebra/ of sets. Very similar to the usual
      $\sigma-\text{algebra}$ couterpart, but we don't require the assumptions
      of closedness under unions and intersections to hold for infinitely many
      set, only finitely many.

      We will work with the following algebra, which is not a
      $\sigma\text{-algebra}$. Let $X$ be the set of all natural numbers,
      $\mathbf N$. Define also 
      \[
      \mathcal A = \left\{ A \subset \mathbf N: A\text{ is finite or } A^c \text{ is finite} \right\}
      \]
      
      Example of sets in $\mathcal A$: $\{1, 2, 3\}$ and $\{5001,
      5002,\ldots\}$. Example of a sets /not/ in $\mathcal A$: the set of all
      odd/even/prime numbers.[fn:1] 
      
      It's not hard to see that this is satisfies: $\emptyset \in \mathcal A$
      (since $\emptyset$ is finite) and closedness under intersections/unions.
      The reason why $\mathcal A$ is not a $\sigma\text{-algebra}$ is that each
      $A_i = \{1, 3, \ldots, 2i + 1\}$ is in $\mathcal A$, but its infinite
      union, the set of all odd numbers, is not.
      
      Now consider the probability measure: $\mathbf P:\mathcal A \to [0,1]$: 
      \[ \mathbf P(A) = 
      \begin{cases} 1 &\text{if } A\text{ is infinite}  \\ 0 &\text{ otherwise} \end{cases} \]
      Thus, for example, $\mathbf P({1,2,3}) = 0$ and $P(\{1023, 1024, \ldots\}) = 1$.
      
      Such $\mathbf P$ trivially satisfies $\mathbf P(A + A') = \mathbf P(A) + \mathbf P(A')$ because
      the finite union of finite sets is finite.
      
      This probability measure is interesting because it provides a
      counter-example to continuity when $\mathbf P$ is only finitely, but not
      countably, additive.
      
      For example, it holds that $\{1,2,\ldots, n\} \uparrow \mathbf N$, but 
       \[\begin{aligned} 1 = \mathbf P(\mathbf N) &= \mathbf P\left( \bigcup_n \left\{ 1,2,\ldots, n \right\} \right)
       &\ne \lim_n \mathbf P\left( \left\{ 1,2,\ldots, n\right\} \right)  = 0
       \end{aligned}\]
       
       Moreover, $\{n+1, n+2, \ldots\} \downarrow \emptyset$, but 
       \[ 0 = \mathbf P(\emptyset) = \mathbf P\left( \bigcap_n \{n+1, n+2, \ldots\} \right) \ne
            \lim_n \mathbf P\left( \{n+1, n+2, \ldots \} \right) = 1 \]
            
       The CDF of the random variable $X:\mathbf N \to \mathbf N$, $X(n) = n$
       according to $\mathbf P$ will satisfy:
       \[ F_X(k) = \mathbf P\{n: X(n) \leq k\}=  0\]
       for all $n$, so $\lim F_X(k) = 0$ for $k\to\infty$. 
       
      
    
*** Best linear predictor, matrix version
    Let $M(n,k)$ denote the linear space of all matrix of dimension $n\times k$.
    
    Suppose we have random vectors $(\mathbf y(\omega), \mathbf z(\omega))'$. We
    know additionally that $\mathbf y \in M(n,1)$ and $\mathbf z \in M(k,1)$ and these vectors
    have finite mean and variance. Denote their mean by 
    \[ \begin{bmatrix} \mu_y \\ \mu_z \end{bmatrix} \]
    and their variance matrix by
    \[ \begin{bmatrix} \Sigma_{yy} & \Sigma_{yz} \\ \Sigma_{zy} & \Sigma_{zz} \end{bmatrix} \]

    We define the *best linear predictor* of $\mathbf y$ given $\mathbf z$ as the random variable $\mathbf w$ such that 
    \[ \mathbf w^* = \alpha^* + \beta^* (\mathbf z - \mu_z) \]
    where $\alpha^* \in M(n,1)$ and $\beta^* \in M(n,k)$ solve the minimzation problem 
    \[ \min_{\alpha, \beta} \mathbf E \left[ \| \mathbf y - \alpha - \beta(\mathbf z - \mu_z) \|^2 \right]  \] 
    
    You can solve it either by using calculus -- which can be cumbersome if you're
    not used to matrix derivatives -- or by noting that the minimand is a squared norm
    generated by the inner product
    \[ \langle \mathbf y, \mathbf w \rangle := \mathbf E[\mathbf w' \mathbf y] \]
    
    of all vectors of the type $\mathbf y - \mathbf w$ where $\mathbf w = \alpha + \beta(\mathbf z - \mu_z)$ for some $\alpha, \beta$.
    
    Let $\epsilon := \mathbf y - \mathbf w^*$ denote the residual of the
    minimization problem. Then $\epsilon$ must be orthogonal (by Hilbert's
    projection theorem) to every $\mathbf w = \alpha + \beta (\mathbf z - \mu_z)$.
    
    Taking $\beta=0$, we see that $\mathbf w^*$ must satisfy 
    \[  0 = \langle \mathbf y - \mathbf w^*, \alpha \rangle = \mathbf E\left[ \alpha' \mathbf y  \right] - \mathbf E\left[ \alpha' \alpha^*  \right] \]
    for all vectors $\alpha \in M(n,1)$. Taking these to be the elements of the canonical basis, we conclude that
    \[ \alpha^* = \mu_y\]
    
    Now take $\alpha=0$. The orthogonality condition now implies that for any $\beta \in M(n,k)$,
    \[ 0 = \langle \mathbf y - \beta^* (\mathbf z - \mu_z) , \beta ( \mathbf z - \mu_z ) \rangle  =  \mathbf E\left[ (\mathbf z -\mu_z)' \beta' y  \right] - \mathbf E\left[ (\mathbf z -\mu_z)' \beta' \beta^* (\mathbf z - \mu_z)  \right]  \]
    
    Use the properties of the trace -- namely, that it's linear and that matrix
    multiplication commutes inside it -- and of the expectation operator to
    conclude that
    \[ \tr \left(  \beta' \mathbf E\left[ \mathbf y(\mathbf z - \mu_z)' \right] \right)  = \tr \left(\beta' \beta^* \mathbf E\left[ \left( \mathbf z - \mu_z   \right) \left( \mathbf z - \mu_z  \right)' \right] \right) \] 
    
    note that $\mathbf E[\mathbf y(\mathbf z - \mu_z)'] = \Sigma_{yz}$ and
    $\mathbf E[(\mathbf z-\mu_z)(\mathbf z - \mu_z)'] = \Sigma_{zz}$. The equation above then implies that  
    \[ \tr (\beta' \Sigma_{yz} ) = \tr (\beta' \beta^* \Sigma_{zz}) \]
    
    should hold for all matrices $\beta \in M(n,k)$. That implies,[fn:2] 
    \[\Sigma_{yz} = \beta^* \Sigma_{zz} \]
    which in turn yields $\beta^* = \Sigma_{yz} \Sigma_{zz}^{-1}$ whenever
    $\Sigma_{zz}$ has an inverse. In that case, the BLP is 
    #+NAME: eq:expression_blp
    \begin{equation} \mathbf w^*  = \mu_y + \Sigma_{yz} \Sigma_{zz}^{-1} (\mathbf z - \mu_z)  \end{equation} 
    
**** Appendix: the Trace operator
     - let $A(i,j)$ denote the entry $(i,j)$ of any matrix
     - Let $A$ be a $m\times n$ matrix. The trace is defined as
       \[ \tr A = \sum_{i=1}^{\min\{m,n\}} {A(i,i)}\]
       in other words, it's just the sum of elements in the main diagonal.
     - Some properties of the trace: 
       1. $\tr(A + B) = \tr(A) + \tr(B)$ whenever $A$ and $B$ have similar dimensions
       2. $\tr(kA) = k\,\tr(A)$ for all scalars $k$
       3. $\tr(AB) = \tr(BA)$ whenever dimensions are such that both multiplications make sense
       Curiosity: any operation $\tilde\tr$ that satisfies the properties above
       is equal to $\tr$ (modulo multiplication by a constant)
     - The trace and expectation operators commute: 
       \[\tr (\mathbf EA) = \mathbf E (\tr A)\]
     - Suppose $A \in M(m,n)$ and you want to select element $(i,j)$ from it. Note that
       \[ A(i,j) =  e_i' A \varepsilon_j = tr(e_i' A \varepsilon_j) = tr(\varepsilon_j e_i' A) \]
       where $e_i$ is the i-th element in the canonical basis of $R^m$ and
       $\varepsilon_j$ is the j-th element of the canonical basis of $R^n$.
       
       Hence for any $(i,j)$, letting $B = \varepsilon_j e_i' \in M(n,m)$ we have 
       \[A(i,j) = \tr ( B A ) \]
     - This implies that if $A$ and $\tilde A$ are fixed $m\times n$ matrices, and  
       \[ \tr(BA) = tr(B\tilde A) \]
       holds for every $B \in M(n,m)$, then 
       \[ A = \tilde A\]

*** DISCARD: Comments on PS3, Q3: mixed vs behavioral strategies    
    
    
      Because of the abundance of zeros and ones, I'll change the notation to make
      my life a bit easier. The possible data realizations are denoted by
      $\mathcal X=\{a, b\}$. Let $\Theta = \{\alpha, \beta\}$ be the possible
      parameters. Since this is binary data, it is sufficient to know
      $p(a | \alpha)$ and $p(a | \beta)$ for the model to be fully specified.
      (Make sure you understand this.)


      Let's start with the action space $\mathcal A_0 = \{\hat\alpha, \hat\beta\}$.
      The loss function is given by
      $\mathcal L(\hat\alpha, \alpha) = L(\hat\beta, \beta) =0$ and $1$ otherwise.
      
      The interpretation is that you're trying to guess whether the true state
      of the world is $\alpha$ or $\beta$; if you guess $\alpha\hat$ when the
      state of the world is $\beta$, or vice-versa, you lose a point. Otherwise,
      you don't lose anything. 

      (Remark: just because I'm using hat notation, doesn't mean this is an estimation problem.)
      
      
      Very well. A decision rule maps data realizations in $\mathcal X$ to
      guesses in $\mathcal A_0$. Because both sets have two elements, there are
      four possible decision rules. 

      Let's write a decision rule as $(\hat\theta_a, \hat\theta_b)$. 

      Each rule can be written as a pair 
    #+end_src
    
*** Comments on PS3Q3
    
    Note that when the action space is $\mathcal A=\{a_0, a_1\}$ -- with associated loss
    function $\mathcal L(a_i, \theta_j) = \mathbf 1(i \ne j)$ -- we have four possible strategies. 
    
    Let's call $\tilde A$ the set of decision rules: $\tilde A = \{d_1, d_2, d_3, d_4\}$,
    as in the question.
    The risk function evaluated at a given decision rule $d$ is simply a vector 
    \[\begin{bmatrix} 
    R(d, \theta_0) \\ 
    R(d, \theta_1)
    \end{bmatrix} \]
    
    Let $D(\tilde A)$ denote the set of all points in $\mathbf R^2$ that are the
    risk function of some decision rule. It was your job to show that 
    \[ D(\tilde A) = \Big \{ \overbrace{\begin{bmatrix} 0 \\ 1\end{bmatrix}}^{d_1},
      \overbrace{\begin{bmatrix}  1-p_0 \\ p_1\end{bmatrix}}^{d_2}, 
      \overbrace{\begin{bmatrix} p_0 \\ 1-p_1\end{bmatrix}}^{d_3},
      \overbrace{\begin{bmatrix} 1 \\ 0\end{bmatrix}}^{d_4}
    \Big \}\]
    [Let's call this the /risk set/ associated with this decision problem.] 
    
    
    The problem set presented one way of ``randomizing risk''. Upon observing
    data, in part 3 you allowed, instead of choosing any particular action in
    $\mathcal A$, to choose a distribution over points in $\mathcal A$. Since
    $\mathcal A$ is binary, this amounts to choosing a number $\delta \in [0,1]$
    representing the probability of playing $a_0$. COMMENT: notation
    
    So the new action space is $\mathcal A' = [0,1]$.
    
    
    What is then a decision rule in this context? Again, any function
    $d:\mathcal X\to\mathcal A'$. Now $\mathcal A'$ is not binary anymore, so
    there are uncountably many possible decision rules. 

    In fact, each decision rule $(\delta_0, \delta_1)$ can be mapped to the
    square $[0,1]\times[0,1]$. 

    

      
    


* Footnotes

[fn:2] See the appendix on the trace operator for details.

[fn:1]  The sets whose complement is finite are called co-finite sets.


     

