#+TITLE: Fall 19: Intro to Econometrics TA material  
#+AUTHOR: Gustavo Pereira
#+STARTUP: beamer


* README page
  :PROPERTIES: 
  :EXPORT_FILE_NAME: README.org
  :EXPORT_TITLE: 
  :END:
** Intro to Econometrics: TA repo
   Welcome! This repository was created to store and maintain the materials
   used or referred to in the recitations. 
  
   Schedule: 
   | what           | when                | where                           |
   |----------------+---------------------+---------------------------------|
   | *Recitation*   | See syllabus        | 227 Mudd / 403 IAB              |
   | *Office hours* | Tuesday 09:00-10:00 | Lehman Library group study area |
  
   You can find lectures, slides and problem sets in the [[https://jm4474.github.io/Courses-IntroEconometrics-Ph.D/][class page]]. 

   Pull requests are encouraged!
  
*** Questions outside OH
    See [[outside_oh_questions.pdf][here]]!

      

   

* Questions outside office hours
  :PROPERTIES: 
  :EXPORT_FILE_NAME: outside_oh_questions.pdf
  :EXPORT_TITLE: Out-of-OH Q&A   
  :EXPORT_AUTHOR: Gustavo Pereira
  :EXPORT_LATEX_HEADER: \input{auxfiles/header_basic.tex}
  :EXPORT_OPTIONS: ^:nil
  :END: 
** Why
   The ideal time to ask questions about the material and problem sets is during
   office hours. However, many of you can't make it to office hours for valid
   reasons, so I also try to respond to questions sent by e-mail or other
   means.

   Keeping this document helps you (and yours truly) for a few reasons: 
   - More clarifications/explanations available
   - There is no `unfair advantage' to those who ask many questions
   - I don't have to answer the same thing over and over :)

     #+LATEX: \clearpage
** First half - pset 1
    
*** About limits of sets, and Durrett and Billingsley being wrong
    #+begin_quote
    ``Hi Gustavo,
     
    Thank you for the class today. Can I further clarify Q4 you briefly
    discussed in class? I initially directly used $\lim(-\infty,x) \to \Omega$ in
    the steps, but you pointed out it was wrong. I'm actually still a bit
    confused about why it is wrong. I referred to *Durrett* (who directly used
    $\lim(-\infty,x) \to \Omega$) and *Billingsley* (who states "clearly"). So,
    unfortunately, they both would get the proof wrong. Would it be possible
    for you to give me some hints on which theorems would be useful in the
    proof? Thank you so much!
    
    [screenshot of Durrett's book, theorem 1.1]
     
    Best,
    
    xx''
    
    (I kept the bold face from the e-mail)
    #+end_quote

    Dear xx, 
    
    I wouldn't dare to say that Durrett would get it wrong if he was answering
    the problem set! But definitely, if he just copied and pasted from his
    own book, he would be discounted.
    
    Here's the reason: earlier in the book, he states the continuity from above
    and below of probability measures in terms of collections of sets /indexed
    by natural numbers/. In order to do that, he defines what it means to say
    \[  A_i \uparrow A \]
    for that class of collections.
    
    Later, in the context of proving the limits of CDFs, he applies a
    similar statement to the collection $\{ (-\infty, x] \}_{x\in \mathbf R}$.
    The problem is that it's not indexed by natural numbers. So if he wanted
    full credit, he'd need to clarify he meant by
    \[ (-\infty, x] \uparrow \mathbf R \]
    and why continuity of $\mathbf P$ -- defined only for limits of
    ``increasing'' sets indexed by natural numbers -- also applies for these
    limits.

    
    The hint is the same one I gave in the recitation. Use the fact (no need to
    prove it) that
    \[ \lim_{x\to\infty} F(x) = 1 \]
    if, and only if, for every /increasing/ diverging sequence $x_n \uparrow \infty$, 
    \[  F(x_n) \to 1 \] 
    and try to apply countable additivity.
    

    Sincerely, 
    
    Gustavo

** First half - pset 4
   
*** Clarifying the meaning of posterior in Q3
    #+begin_quote
    In Q3 of PS4, we are asked to compute the posterior mean for $\beta$ and
    $\sigma$, and I assumed that usually meant the expectation of the conditional
    distribution of $\beta$, conditioned on data, and similarly, the conditional
    distribution of $\sigma$ on the data.

    In the hint, I'm questioning my understanding because $\beta$ was conditioned on
    $\sigma$ as well. Going forward, does that mean posterior distributions condition
    on data and all other parameters except the parameter in question? More
    specifically, do you know of any resources where I could read up on the
    mechanics behind this?
    #+end_quote
    
    Computing the distribution of beta given Y and sigma is only supposed to be an intermediate step to make calculations easiser. 

    The end goal is to find the joint distribution of beta and sigma given data. 
    

    
*** Q4 - all $\beta$ vs some $\beta$
    #+BEGIN_quote
    I was working on question 4 and I realized that I have a bit of a gap in my
    understanding about admissible decision rules. I know that a Ridge estimator
    is a Bayesian estimator, and thus is admissible. I know that an admissible
    decision rule is not dominated, so since the OLS estimator is another
    decision rule, it is not the case that the risk of the OLS estimator is less
    than or equal to the risk of the Ridge estimator for all beta and strictly
    less than for some beta. So then, as I understand it, this implies that
    there is *some* beta for which the OLS estimator has strictly higher risk
    than the Ridge estimator (but I know nothing about how they compare for any
    other beta). But, I don't think this tells me anything about how the risks
    compare for all the other betas, right?

    Since in this problem we'd have that the risk for each estimator is the mean
    squared error, and thus bias + tr(variance), I think I'd want to have an
    inequality the compares the MSE of the Ridge estimator to the MSE of the OLS
    estimator, but from the fact that the Ridge estimator is admissible I only
    see how to write that inequality for *some* beta -- how can I extrapolate to
    being able to write an inequality for *all* beta, in order to be able to
    make a statement about how tr(var(beta_ols)) compares to tr(var(beta_ridge))
    compare?
    #+end_quote
    
    You're in the right track, but let me point something out first. Admissibility guarantees that 
    \[ \text{MSE}(\hat\beta^{\text{Ridge}}, \beta) \leq \text{MSE}(\hat\beta^{\text{OLS}}, \beta) \] 
    holds for at least one $\beta$. Rigorously speaking, the inequality is not
    necessarily strict, because they could have the same risk all over the parameter
    space and that wouldn't violate admissibility. 

    Now with that caveat, I would suggest that instead of trying to get a
    comparison that holds for every possible $\beta$, try to use the one $\beta$
    you know exists for which the ranking of MSEs above holds. Write down both
    sides of the inequality for that particular $\beta$, and see if you can
    compare either side with $V_\beta(\hat\beta^{\text{Ridge}})$ -- note that
    this variance does not depend on $\beta$.
    
    
** Second half - pset 1
*** Approaching Q8 
    #+begin_quote
    Could you give us a hint about Q8? We've been trying to solve it for a while now and we don't even know how start.
    #+end_quote
   

    It might be useful to write 
    \[  (T_n - \theta) = A_n B_n  \]

    where $A_n = \frac{1}{\sqrt{n}}$ and $B_n = \sqrt{n} (T_n - \theta)$. 

    Any deterministic sequence $A_n \to A$ also satisfies $A_n \overset{d}{\to} A$. Can you say something about $A_nB_n$?
   
   
* Notes
** Recitation 1
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation1.pdf
   :EXPORT_TITLE: Recitation 1
   :EXPORT_OPTIONS: toc:nil
   :EXPORT_LATEX_HEADER: \input{auxfiles/header_basic.tex}
   :END: 

   In this recitation, I review the material presented in lectures 1 and 2. I
   also cover some things that might be challenging in the first problem sets. 
   
*** Review: lectures 1 and 2
    - Definition of probability space: $(\Omega, \mathcal F, \mathbf P:\mathcal F \to [0,1])$
      - The point of $(\Omega, \mathcal F)$ is to provide a model for the
        /randomness of some outcome/.
      - Remember: we don't observe randomness. We observe some outcome. Then, we
        use a model to infer what are more or less likely ``states of the world'',
        because that allows us to predict things
      - The reason we keep $\Omega$ abstract (instead of focusing on say
        $\Omega=[0,1]$) is that it allows us to deal with a variety of possible
        structures for the outcome space!
    - Random variables: /measurable/ functions $X:\Omega \to S$ where $S$ is some
      space of outcomes.
    - Probability space induced by a random variable
      - Original space: $(\Omega, \mathcal F, \mathbf P)$
      - RV `measurably' maps original space to $(S, \mathcal S)$
      - Induced measure: $\mathbf P_X(F) = \mathbf P\left\{ \omega: X(\omega) \in F \right\}$ for $F \in \mathcal S$
        - Curiosity: this is called a push-forward measure in mesasure theory
      - Probability space $(S, \mathcal S, \mathbf P_X)$ is typically some
        Euclidean space (though it can be more complicated)
    - Let's now focus on the case when $X:\Omega \to S$ is real valued, ie, $S=\mathbf R$.
    - CDF of a random variable: $F_X(x) = \mathbf P\left\{ \omega: X(\omega) \leq x \right\} = \mathbf P_X((-\infty, x])$
      - Result: all information in $\mathbf P_X$ is in $F_X$ and vice-versa.
      - Properties of CDF
        1. $F_X$ is non-decreasing
        2. $\lim_{x\to\infty} F_X(x) = 1$
        3. $\lim_{x\to-\infty} F_X(x) = 0$
        4. $F_X$ is right continuous
      - *First main result*: every function $F$ satisfying all four properties
        above is the CDF of some random variable.
    - Absolutely continuous random variable: $\exists f_X$ such that
      \[ F_X(x) = \int_{-\infty}^x f_X(z) dz \]
      + Weirdly enough, the non-obvious thing about the statement above is not
        the $\exists f_X$ but the $dz$. 
      + Measure theoretic details aside, the important thing is that $dz$ is
        never a jump.
        + If $X$ has a mass at some point $x_0$ in the real line -- meaning that
          the $\mathbf P_X(\{x_0\}) > 0$, there will be a jump in $F_X$ at $x_0$. 
        + We can't have that becasuse $F_X(x_0) - F_X(x_0 - \epsilon) \approx f_X(x_0)\epsilon$
        + For $\epsilon > 0$ small enough, mass at $x_0$ would imply the LHS is
          $\mathbf P\{x_0\}$ while the RHS should be zero
      + Optional comment: in fact every $F_X$ has an associated $f_X$ with
        respect to /some/ (generally non-uniform) measure. This is the
        consequence of a more general result called the /Radon-Nikodym theorem/.
    - Expectation of absolutely continuous RV: 
      \[ \mathbf E[g(X)] = \int_{\mathbf R} g(z) f_X(z) dz  \]
      + ``Law of the unconscious statistician''
    - Moment generating function
      \[ m_X(t) = \mathbf E\left[ e^{tX}\right]=\int_{\mathbf R} e^{tx} f_X(x)dx\]
      + The i-th moment of $X$ can be found by taking the $i-th$ derivative of
        $m_X(t)$ and evaluating it at zero.
        + For this to be meaningful, the MGF must be well defined in $(-\epsilon, \epsilon)$ for some $\epsilon$
        + Then for example $m_X'(t) = \mathbf E[X e^{tX}]$
    - *Second main result.* Let $X_1$ and $X_2$ be st 
      \[ m_{X_1}(t) = m_{X_2}(t) \]
      for all $t$. Then $F_{X_1} = F_{X_2}$.  
      + This essentially means that all information contained in $F_X$ is also
        contained in $m_X(t)$
    - Note: take the Taylor series of exponential around $0$ and take
      expectations,
      \[m_X(t) = \sum_{n=0}^\infty \frac{t^n \mathbf E(X^n)}{n!}\]
      + It is tempting to that knowledge of moments determines the distribution
        of $X$. This is not the case, however, because sometimes the series
        above doesn't converge even when all moments exist. 
        
    # Examples. 
    # 1) $\Omega = \{1,2,3\}, S=\{a,b,c\}$.

    #    What is the measurability requirement doing? Suppose we have
    #    $\sigma-\text{algebras}$ $\mathcal F=\{\emptyset, \{1\}, \{2,3\}, \Omega\}$ and $\mathcal S = 2^S$.
       
    #    Because neither $2$ nor $3$ show up separately in $\mathcal F$, observing
    #    a random variable $X:\Omega\to S$ should not allow us to distinguish them.

    #    For example, a random variable such as
    #    \[X(1) = a, X(2) = b, X(3)=c\]
    #    would allow us to distinguish $2$ and $3$! Indeed, if $2$ is observed, we
    #    know for sure that $\omega=2$, but $\{2\}$ isn't in $\mathcal F$.
       
    #    In a sense, the measurability requirement is imposing consistency in what
    #    we can learn about the underlying state based on observing an outcome.
    #    In the above example, measurability implies that $X(2) = X(3)$.
       
    # 2) Take $\Omega$ to be the $[0,1]$ interval with the uniform probability $\lambda$, ie, 
    #    \[ \lambda( [a,b] )  = b - a \]
    #    for all intervals $[a,b]$.  

*** Problem 4 is not as easy as it might seem
    
    Consider the proof, for example, that $F_X \to 1$ as $x\to\infty$. (The case
    of $x\to0$ is similar.)
    
    We know that: 
    1) $F(x) = \mathbf P\{\omega: X(\omega) \leq x \}$
    2) $\{\omega: X(\omega) \leq x\} \uparrow \Omega$
    3) $\mathbf P(\Omega) = 1$
       
    So it must be the case that $F(x) = P\{\omega: X(\omega) \leq x\} \uparrow \mathbf P(\Omega) = 1$,
    isn't that right? Well, *no*. While that reasoning is in some sense in the
    right direction, at the very least it's an incomplete argument for two reasons.
    
    - We haven't defined convergence of sets as in (2). Unless you can make that
      statement rigorous somehow, using it is not fair game. 
    - More importantly, when we took the statements together, we missed an
      important step: proving that (whatever the first arrow means)
      \[ A_x \uparrow \Omega \implies \mathbf P(A_x) \uparrow \mathbf P(\Omega) \]
    
    The second step above is essentially the point of the exercise. Hint for
    actually solving the problem:
    - Use the fact that 
      \[ \lim_{x\to\infty} F(x) = L\] 
      if, and only if $F(x_n) \to L$ for all increasing sequences $x_n \to \infty$
    - Show that for any probability measure, if $x_n \uparrow \infty$
      \[ \mathbf P\{ \omega: X(\omega) \leq x_n \} \to \mathbf P(\Omega) = 1 \] 
      
      You will need to use /countable/ additivity for this.
      
    For the right-continuity part, one useful way of checking your proof is to
    make sure you understand why your proof doesn't apply to the left limit. 
** Recitation 2
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation2.pdf
   :EXPORT_TITLE: Intro to Econometrics: Recitation 2
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_LATEX_HEADER: \input{auxfiles/header_beamer.tex}
   :END: 
*** Review Part
**** Review
     #+BEAMER: \framesubtitle{Random variables - \emph{univariate} case} 
     #+BEAMER: \begin{center} $(\Omega, \mathcal F, \mathbf P)$ \end{center}
     
     - $X:\Omega\to\mathbf R$
     - CDF:
       \[ F_X(x) = \mathbf P( \left\{\omega: X(\omega) \leq x\right\}) \]
       + Completely characterizes $\mathbf P\{X \in B\}$ for $B \subset \mathbf R$
     - Absolutely continuous: 
       \[F_X(x) = \int_{-\infty}^x f_X(x) dx\]
**** Review
     #+BEAMER: \framesubtitle{Random variables - \emph{multivariate} case} 
     #+BEAMER: \begin{center} $(\Omega, \mathcal F, \mathbf P)$ \end{center}
     
     - $X:\Omega\to\mathbf R^S$ where $X(\omega) = (X_1(\omega),\ldots, X_S(\omega))'$
     - CDF:
       \[ F_X(x_1, \ldots, x_S) = \mathbf P( \{\omega: X_1(\omega) \leq x_1, \ldots, X_S(\omega) \leq x_S  \}) \]
       + Completely characterizes $\mathbf P\{X \in B \}$ for $B\subset \mathbf R^S$
     - Absolutely continuous: 
       \[F_X(x_1, \ldots, x_S) = \int_{-\infty}^{x_1}\cdots\int_{-\infty}^{x_S} f_X(x_1, \ldots, x_S) dx_S \cdots dx_1\]
**** Review 
     #+BEAMER: \framesubtitle{Random variables - \emph{multivariate} case} 
     - <1-> Result: if $F:\mathbf R\to[0,1]$ is
       1. Increasing
       2. Right-continuous
       3. Satisfies $\lim_{x\to\infty} F(x) = 1 - \lim_{x\to-\infty} F(x) = 1$
       Then it is the CDF of some random variable $X:\Omega\to\mathbf R$
     - <2-> Can you think of (or prove?) an S-dimensional analog of the statement above?
**** Review 
     #+BEAMER: \framesubtitle{Random variables - \emph{multivariate} case} 
     - If $F:\mathbf R^2\to[0,1]$ is
       1. <1-> Increasing
       2. <1-> ``Continuous from above''
       3. <1-> Has the following limits:
          1. $\lim_{x_1 \to -\infty} F(x_1, x_2) = 0$ for all $x_2$
          2. $\lim_{x_2 \to -\infty} F(x_1, x_2) = 0$ for all $x_1$
          3. $\lim_{x_1 \to \infty} \lim_{x_2 \to \infty} F(x_1, x_2) = 1$
       4. <2-> Satisfies, for $x_1^* \geq x_1$ and $x_2^* \geq x_2$,
          \[ F(x_1^*, x_2^*) - F(x_1^*, x_2) - F(x_1, x_2^*) + F(x_1, x_2) \geq 0 \]
       Then $F$ is the CDF of a random variable $X:\Omega\to\mathbf R^2$
       
     (Durrett, sec 2.9)
**** Review
     #+BEAMER: \framesubtitle{Marginals} 
     
     - <1-> Marginal with respect to coordinate $s$, $F_s : \mathbf R \to [0,1]$
       \[ F_s(x) = \mathbf P(\left\{ \omega: X_s(\omega) \leq x \right\})  \] 
     - <2-> How do you obtain it?
     - <3-> Just take limits. Suppose $S=2$ and we want to recover first coordinate:
       \[ F_1(x_1) = \lim_{x_2 \to \infty}  F(x_1,x_2)  \]
       
       Proof? 
**** Review
     #+BEAMER: \framesubtitle{Marginals} 
     
     - How do you recover a marginal pdf? Suppose $X:\Omega\to\mathbf R^2$ has pdf $f(x_1,x_2)$:
       \[f_1(x_1) = \int_{-\infty}^\infty f(x_1, x_2) dx_2\]
     - Proof? 
**** Review
     #+BEAMER: \framesubtitle{Digression: marginals don't determine joints} 
     
     - A very useful counterexample: 
       - <1-> Let $X \sim N(0,1)$
       - <2-> Let $W$ be independent of $X$; 
         \[ \mathbf P(W = 1) = \mathbf P(W = -1) = \tfrac{1}{2}\]
       - <3-> Define $Y = WX$. Claim: $(X,Y)$ has normal marginals, but $(X,Y)$ is not jointly normal.
         \begin{align*}F_Y(y) = \mathbf P(WX \leq y) &= \frac{1}{2} \mathbf P(X \leq y) + \frac{1}{2} \mathbf P(-X\leq y) \\ 
           &= F_X(y)\end{align*}
         So marginals of $(X,Y)$ are the same
       - <4-> $(X,Y)$ is not multivariate normal. Why? 
       - <5-> $X+Y$ has a  mass at zero, with probability $\frac{1}{2}$!
**** Review
     #+BEAMER: \framesubtitle{Digression: marginals don't determine joints} 

     \centering \includegraphics[scale=0.4]{./codes/Notes_PS2_simunormal.pdf}    
**** Review
     #+BEAMER: \framesubtitle{Moments of multivariate RVs} 
     - Focus on the case when there is a pdf
     - <1-> ``Definition''
       \[  \mathbf Eg(X) =  \int_{\mathbf R^S} g(x) f_X(x)dx   \]
     - <2-> First moment: 
       \[ \mu_X =  \mathbf EX \]
     - <3-> Second moment: 
       \[ V(X) = \mathbf E \left[ (X - \mu_X)(X - \mu_X)' \right] \]
       #+BEAMER: \vspace{-0.3cm}
       - When is $V(X)$ finite?
     - <4-> Covariance btw X and Y: 
       \[ \cov(X,Y) = \mathbf E \left[ (X - \mu_X)(Y-\mu_Y)' \right] \]
**** Review
     #+BEAMER: \framesubtitle{Moment generating functions of multivariate RVs} 
     - <1-> MGF: 
       \[  m_X(\mathbf t) = \mathbf E\left[ e^{\mathbf t'X} \right] = \mathbf E\left[ e^{\sum_{i=1}^S t_i X_i} \right]  \]
     - <2-> Result: suppose  $X$ and $Y$ have a moment generating function, and 
       \[ m_X(\mathbf t) = m_Y(\mathbf t)\]
       for all $\mathbf t$. Then $F_X(\mathbf t) = F_Y(\mathbf t)$ for all $\mathbf t$.
     - <3-> Result (stronger):  suppose that, for all $\mathbf t \in \mathbf R^S$, $\alpha \in \mathbf R$, 
       \[ \mathbf P\{ \mathbf t'X \leq \alpha \} = \mathbf P\{ \mathbf t'Y \leq \alpha \} \]
       then $F_X(z) = F_Y(z)$ for all $z\in\mathbf R^S$
*** PSet
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections} 

     Let $(V, \langle\cdot,\cdot\rangle)$ be a vector space with an inner product. 
     - <2-> Orthogonal projection of $v$ into (closed) $W\subseteq V$:
       \[ v - \proj_W(v)\perp w \]
       for all $w\in W$
***** Projection in a Hilbert Space 
      :PROPERTIES: 
      :BEAMER_env: theorem
      :BEAMER_opt: shadow=true
      :BEAMER_act: 3
      :END:
      
      Let $W\subset V$ be a closed vector subspace of $V$. 

      For any $v \in V$, the distance minimization problem
      \[\min_{w\in W} \| v - w \|\]
      has a unique solution $w^* \in W$. Moreover, $w^* = \proj_W(v)$.
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections} 
     What if $W$ has a finite basis? 
     \[ W = \vsp \{w_1, \ldots, w_K\}\]
     - Orthogonal projection of $v$ into $W$ is 
      \[  \proj_W(v) = \sum_{i=1}^K \frac{\langle w_i, v\rangle}{\langle w_i, w_i\rangle} w_i  \]

     Using this result in the pset is fair game 
     
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections} 
     
     Space $V = \{ X:\Omega\to\mathbf R^S: \mathbf E\|X\|^2 < \infty \}$ is a Hilbert
     space with 
     \[ \langle X, Y\rangle = \mathbf E XY\]
      
     - <2-> Fix variables $X$, $Y$ in $V$ and consider the subspace
       \[ W = \{ Z: \Omega \to \mathbf R : Z = \alpha + \beta (X - \mu_X)\} \] 
       (Is there a finite basis for $W$?)
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections}
     The problem
     \[  \min_{(\alpha, \beta)} \left[ Y - \alpha - \beta(X-\mu_X) \right]^2 \]

     is equivalent to some norm minimization problem involving $Y, X$ and $W$.

     What is it?
** Recitation 3
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation3.pdf
   :EXPORT_TITLE: Intro to Econometrics: Recitation 3
   :EXPORT_BEAMER_THEME: Boadilla
   :EXPORT_LATEX_CLASS_OPTIONS: [presentation, smaller]
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_beamer.tex}
   :END:
   
*** Outline
**** Outline
     - Review: 
       + Statistical model
         * Definition
         * Examples
         * Identification, sufficiency 
       + Statistical decision problem
         * Definition
         * Examples
*** Statistical model
**** Statistical model
     #+BEAMER: \framesubtitle{Definition}
     - <1-> Idea: formalize statements such as
       1. Let $\{h_1, \ldots, h_{10}\}$ denote the outcome of $10$ independent
          coin flips with probability $p$ of landing heads
       2. <2-> ``Let ${X_1, X_2, X_3}$ be iid uniform in $[0,\theta]$ where $\theta$ is an unknown positive real number''
       3. <3-> ``Let $\{Y_t\}_{t\in1,2,\ldots, T}$ be an AR(1) process with gaussian innovations''

     - <4-> *Claim.* All statements equivalent to: ``let $\mathbf X$ be
       a draw from some cdf $F:\mathbf R^S \to [0,1]$ where $F$ is taken from some restricted set of CDFs, 
         \[F \in \mathfrak F\text{ ''}\]
**** Statistical model
     #+BEAMER: \framesubtitle{Definition}
     - <1-> It's common to write 
       \[ \mathfrak F = \{ F_\theta \}_{\theta \in \Theta} \]
     - <2-> For example: 
       \[\mathfrak F = \left\{ F:\mathbf R\to\mathbf R | F\text{ is the cdf of }  U[a,b] \text{ for some }a\leq b\right\}\]
       #+BEAMER: \vspace{-0.5cm}
       - Does this  represent a statistical model?
     - <3-> We can define for $\theta = (a,b)$, 
       \[ F_{\theta} = \frac{t-a}{b-a} \mathbf 1_{[a,b]}(t) \]
     - <4->  With that indexing, 
       \[ \mathfrak F = \{F_{\theta} \}_{\theta \in \Theta}\]
       where $\Theta = \{(x,y) \in \mathbf R^2 : x \leq y\}$
**** Statistical model 
     #+BEAMER: \framesubtitle{Comment}
     - <1-> Why do we specify models with CDFs?
     - <2-> Reason: in Euclidean spaces, distribution of random variables is fully characterized by CDF
     - <3-> However, if all CDFs in your model are absolutely continuous, it's
       equivalent to specify a family of PDFs
     - <4-> In the course, we will do this interchangeably; if a model is
       specified in terms of PDFs, it's understood that we're considering only absolutely continuous distributions
     - <5-> We can also specify the model with more general probability distributions: 
       \[ \{P_\theta: \mathcal B(\mathfrak X) \to [0,1]\}_{\theta \in \Theta} \]
       where $\mathfrak X$ a possibly more general space (e.g., a space of bounded continuous functions) 
**** Statistical model
     #+BEAMER: \framesubtitle{Example 1: ten coin flips}
     - <1-> Single coin flip: 
       \[ F_p^1(x) = \begin{cases} 0 & \text{if } x < 0 \\ 1 - p & \text{if } x \in [0,1) \\ 1 & \text{otherwise} \end{cases}\]
     - <2-> Then the joint is  $F_p(h_1, h_2, \ldots, h_{10}) = F_p^1(h_1) \cdots F_p^1(h_{10})$
     - <3-> Model: 
       \[ \{F_p\}_{p \in [0,1]} \]
       + What is $\Theta$ ? 
**** Statistical model
     #+BEAMER: \framesubtitle{Example 2: Uniform $[0,\theta]$}
     - <1-> Three independent uniform $[0,\theta]$. We know that for a given $\theta$
       \[ F_\theta^2(t) = \frac{t}{\theta} \mathbf 1_{[0,\theta]}(t) \]
       is the cdf of $U[0,\theta]$ for non-negative $\theta$.
     - <2-> Thus joint is 
       \[ F_\theta(x_1, x_2, x_3) = F^2_\theta(x_1) F^2_\theta(x_2) F^2_\theta(x_3)\]
       and statistical model is \[ \{ F_\theta \}_{\theta \in (0,\infty)} \]
**** Statistical model
     #+BEAMER: \framesubtitle{Example 3: AR(1) with Gaussian innovations}
     - <1-> An ``AR(1) with Gaussian innovations'' means that 
       \[ Y_t - \mu = \rho (Y_{t-1} - \mu) + \epsilon_t \] 
       where $\epsilon_t$ are drawn iid $N(0, \sigma^2)$. 
       - <1-> Note: need to make assumption about $Y_0$. Assume fixed.
     - <2-> Equivalently,
       \[ Y_t | Y_{t-1}, \ldots, Y_1 \sim N(\mu + \rho(Y_{t-1} - \mu) , \sigma^2) \] 
     - <3-> How do you write the joint CDF? By what parameters will it be indexed?
**** Statistical model 
     #+BEAMER: \framesubtitle{Identification \& sufficiency}
     
     - <1-> Summary of previous discussion: a statistical model is a family of
       distributions, $\{F_\theta: \mathbf R^S \to [0,1]\}_{\theta\in\Theta}$.
     - <2-> If each $\theta \in \Theta$ induces a unique distribution, the model is called *identified*.
     
       + <3-> Mathematically: the model is identified iff for every $\theta \ne
         \theta'$,
         there exists $x \in \mathbf R^S$ such that $F_\theta(x) \ne F_{\theta'}(x)$
       + <4-> What if the model was specified in terms of PDFs? What about general probability distributions?
     - <5-> A /statistic/ is any function $T:\mathbf R^S \to \mathbf R^K$. We
       say that $T$ is *sufficient* if \[ \mathbf P_\theta( \cdot | T(\cdot)) \]
       does not depend on $\theta$. Intuitively, if you condition on $T(X)$, the
       full data become uninformative about $\theta$.
       
**** Statistical model 
     #+BEAMER: \framesubtitle{Identification \& sufficiency}
     - <1->  Example: let $X_1$ and $X_2$ be iid $N(\mu, 1)$.
       + <2-> Model here is $\{F_\mu\}_{\mu \in \mathbf R}$ where $F_\mu$ is cdf
         of independent joint normal with mean $(\mu,\mu)$ and identity variance matrix
     - <2->  Then $T(X_1, X_2) = X_1 + X_2$ is sufficient.
     - <3-> Before proof: note that crucially the data is 2 dimensional, but the sufficient statistic is 1d 
     - <4-> Now: 
       \[\begin{bmatrix} X_1 \\ X_2 \\ T(X_1, X_2) \end{bmatrix} \sim \mathcal N_3 \left(  \begin{bmatrix} \mu \\ \mu \\ 2\mu \end{bmatrix}, 
         \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 1 & 2 \end{bmatrix} \right) \]
     - <5-> To find conditional distribution of $X_1$ and $X_2$ given $T(X_1, X_2)$, use the BLP trick. 
**** Statistical model 
     #+BEAMER: \framesubtitle{Identification \& sufficiency}
     - <1-> Math: 
       \[ E[X_1 | X_1 + X_2] = E[X_2 | X_1 + X_2] = \frac{X_1  + X_2 }{2} \] 
       moreover, conditional variance also doesn't depend on $\mu$ 
       
*** Statistical decision problem
**** Statistical decision problem
     #+BEAMER: \framesubtitle{Definition}
     - Definition: statistical decision problem is 
        \[ (\Theta, A, \mathcal L, \{F_{\theta}\}_{\theta\in\Theta}) \]
       where 
       1. <2-> $\Theta$ is a parameter space
       2. <3-> $A$ is a space of actions
       3. <4-> $\mathcal L$ is a utility/loss function
       4. <5-> $\{F_\theta\}$ is a statistical model
          + Remember: this can be alternatively specified as $\{P_\theta\}_{\theta\in\Theta}$ or $\{f_\theta\}_{\theta\in\Theta}$
**** Statistical decision problem 
     #+BEAMER: \framesubtitle{Interpretation}
      + Statistician is supposed to decide something. Examples: 
        1. <2-> Pick the $\theta$ that she thinks generated the data
           \[  A  = \Theta \] 
        2. <3-> Given a split $\Theta = \Theta_0 \sqcup \Theta_1$, pick which of
           $\Theta_0$ or $\Theta_1$ is more likely to contain the parameter that generated data
           \[ A = \{0, 1\} \] 
        3. <4-> Pick a subset $C \subseteq \Theta$ where she thinks the true $\theta$ falls in
           \[ A = \text{reasonable subsets of }\Theta \] 
**** Statistical decision problem 
     #+BEAMER: \framesubtitle{Interpretation}
     + <1-> Model this as a sequential game. 
       - <2-> /First stage:/ Nature picks $\theta \in \Theta$. This is not observable by statistician
       - <3-> /Stage $1\frac{1}{2}$:/ Nature randomly draws $X \sim F_\theta$
       - <4-> /Second stage:/ Statistician chooses action $a$
     + <5-> At the terminal nodes, statistician gets the loss $\mathcal L(a, \theta)$
     + <6-> Let $\mathfrak X \subset \mathbf R^S$ denote the (common) support of $F_\theta$. A
       *strategy* for the statistician in this game is a function 
       \[ d : \mathfrak X \to A  \]
       I.e. a specification of an action for every possible decision node she faces
       
       This strategy is called a decision rule in the mathematical statistical jargon 
**** Statistical decision problem 
     #+BEAMER: \framesubtitle{Risk function} 
     - <1-> What sort of criterion should we use to rank decision rules?
     - <2-> We use the expected utility paradigm. For fixed $\theta$, we postulate that
       \[  d_1(\cdot) \precsim_\theta d_2(\cdot) \iff \mathbf E_\theta \left[  \mathcal L(d_1(X), \theta)   \right]
       \geq \mathbf E_\theta \left[  \mathcal L(d_2(X), \theta)   \right]\] 
       #+BEAMER: \vspace{-0.5cm}
       - With respect to what are we taking the expectation?
     - <3-> This expectation is called /risk/. Notation: 
       \[ R(d, \theta) := \mathbf E_\theta \left[ \mathcal L(d(X, \theta) \right] = \int_{\mathbf R^S} d(x, \theta) dF_\theta(x) \]
     - <4-> Analogy  with game theory: /dominated/ strategies
       - A decision rule that is not weakly dominated is called admissible
** Recitation 4
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation4.pdf
   :EXPORT_TITLE: Intro to Econometrics: Recitation 4
   :EXPORT_BEAMER_THEME: Boadilla
   :EXPORT_LATEX_CLASS_OPTIONS: [presentation, smaller, handout]
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_beamer.tex}
   :END:
*** Recitation 4
**** Roadmap for today
     - Review:
       1) Statistical problem
       2) Bayes rules, expected posterior loss
     - PS3

**** Review: Statistical Problem 
     #+ATTR_beamer: :overlay +-
     - Components of a decision problem:
       + Statistical model: $\{P_\theta\}_{\theta \in \Theta}$
       + Action space $\mathcal A$
       + Loss function $\mathcal L:\mathcal A\times \Theta \to \mathbf R$
       + Decision rules: $d:\mathfrak X\to\mathcal A$
     - Risk function: expected loss from decision $d$ when parameter is $\theta$: 
       \[R(d(\cdot), \theta) = \int_{\mathfrak X} \mathcal L(d(x), \theta) f_\theta(x)dx \]
     
***** Comments                                                     :noexport:
      - Note that $\mathcal L(a, \theta)$ ranks actions for fixed $\theta$
      - Note that $R(d(\cdot), \theta)$ ranks decision rules (functions) for fixed $\theta$
        \[ d_1(\cdot) \succsim d_2(\cdot) \iff R(d_1(\cdot), \theta) \leq R(d_2(\cdot), \theta) \]
      - In that sense, "risk" is just like a "generalized loss" where the
        "generalized action space" is the space of all strategies
**** Review: Admissibility      
     #+ATTR_beamer: :overlay +-
     - Decision rule $d_1$ is /dominated/ by $d_2$ iff, for all $\theta \in \Theta$,
       \[ d_1 \precsim_\theta d_2  \]
       and $d_1 \prec_{\theta_0} d_2$
       for at least one $\theta_0$
       + What does it mean for a rule to be /not dominated/ by another rule? 
     - A rule $d$ that is not dominated by any other rule is called /admissible/
       + Expand the definition of admissible 
     - It is generally hard to find admissible rules.
**** Review: priors, posteriors, etc...  
     #+ATTR_beamer: :overlay +-
     - Suppose model is $\{f_\theta\}_{\theta \in \Theta}$, i.e., data has a density for all possible parameters
     - Suppose also $\Theta \subseteq \mathbf R^k$, and pdf $\pi(\theta)$ summarizes some prior belief about $\theta$
       - With this, we're interpreting the parameter $\theta$ as a /random variable/
       - Before the prior was introduced, $\theta$ was merely an index
     - With this structure, we can define the induced joint density of data and parameters,
       \[  f(x, \theta; \pi) = f_\theta(x) \pi(\theta)  \]
       + Does this integrate to one?
**** Review: priors, posteriors, etc...
     #+ATTR_beamer: :overlay +-
     - Given induced joint density, 
       \[ f(x | \theta; \pi) = \frac{f_\theta(x) \pi(\theta)}{\pi(\theta)} = f_\theta(x)\]
       #+BEAMER: \vspace{-0.3cm}
     - What about the marginal of data?
       - Recover it by integrating $\theta$ out:
         \[ f(x; \pi)  = \int_{\theta \in \Theta} f_\theta(x) \pi(\theta) d\theta \]
       #+BEAMER: \vspace{-0.3cm}
     - Conditional density of parameter given data?
       \[f(\theta | x; \pi) = \frac{f(x,\theta; \pi)}{f(x;\pi)} = \frac{f_\theta(x)\pi(\theta)}{\int_{\theta\in\Theta} f_\theta(x)\pi(\theta)d\theta}\]
       #+BEAMER: \vspace{-0.3cm}
       - This is called /posterior density/ in Bayesian jargon
**** Review: Bayes rules
     #+ATTR_beamer: :overlay +-
     - Let's go back to the statistical decision problem
     - Let $d(\cdot)$ be a decision rule, and $\pi$ a prior density over $\Theta$
     - Bayes risk of $d(\cdot)$ given $\pi$ is
       \[\begin{aligned} r(d(\cdot), \pi) &= \int_\Theta  R(d(\cdot), \theta) \pi(\theta)d\theta \\
                                          &= \int_\Theta \int_{\mathfrak X} \mathcal L(d(x), \theta) f(x, \theta; \pi)  dx \,d\theta  \end{aligned}\]
     - A /Bayes decision rule/ $d^*$ is one that minimizes Bayes risk given a prior $\pi$. 
       \[ d^*_\pi(\cdot) = \arg\min_{d(\cdot)} r(d(\cdot), \pi) \]
     - Important feature: /under mild assumptions, Bayes rules are admissible/
     
**** Review: finding Bayes rules
     #+ATTR_beamer: :overlay +-
     - Rewrite the Bayes risk using Fubini's theorem
       \[\begin{aligned} r(d(\cdot), \pi) &= \int_{\mathfrak X} \left[ \int_\Theta  \mathcal L(d(x), \theta) f(\theta | x; \pi)  d\theta\right] f(x; \pi) dx\\
                           &=  \int_{\mathfrak X} \psi(d(x), x) f(x; \pi) dx \end{aligned}\]
       where
       \[ \psi(a, x) = \int_\Theta \mathcal L(a, \theta) f(\theta | x; \pi) d\theta \]
     - Let $d^*(x) = \arg\min_{a\in\mathcal A} \psi(a,x)$
       + Immediate consequence: for any decision rule $d(\cdot)$,
         \[ \psi(d^*(x), x) \leq \psi(d(x), x) \] 
         #+BEAMER: \vspace{-0.5cm}
       + Important: optimization in space $\mathcal A$ is easier than in  the space of all $d:\mathfrak X \to \mathcal A$!
       
** Recitation 5
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation5.pdf
   :OPTIONS: toc:nil
   :EXPORT_TITLE: Intro to Econometrics: Recitation 5 
   :EXPORT_SUBTITLE: A quick introduction to vector calculus
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_basic.tex}
   :END:
   
*** Intro
    In these notes I try to introduce some notation regarding calculus with
    functions that map vectors into vectors. One reason why things get a bit
    messy is that when we write $x\in\mathbf R^N$ we don't distinguish between
    \[ \begin{bmatrix} x_1  & \cdots & x_N \end{bmatrix} \]
    and
    \[ \begin{bmatrix} x_1  \\ \vdots \\ x_N \end{bmatrix} \]
    but crucially, the operation $L(\mathbf x)$ where $L$ is a linear map is
    represented differently by means of matrix multiplication notation; in the
    top case, $L(x)$ corresponds to a matrix acting on $x$ ``on the right'',
    whereas in the bottom case, the matrix acts ``on the left''. Of course,
    there is an operation that takes us from the ``row world'' to the ``column
    world'': transposition.
    
    Since derivatives are in fact linear maps, losing track of which side the
    derivative matrix operates on can lead to dimension inferno. So here I
    provide a few examples that might shed light on how to deal with this. 
    
    
*** The meaning of a derivative
    It will be useful to recall how derivatives and linear maps are connected.
    Because these aren't notes in analysis, I won't be as general as I could,
    neither will I provide any proofs. For proofs and generalizations, check any
    undergraduate real analysis textbook. I also assume that you are familiar
    with linear maps and their connection with matrix operations.
    

    Now let's recall the definition of a derivative. 
    #+BEGIN_defi
    Let $F:\mathbf R^n\to\mathbf R^p$. The function $f$ is called differentiable
    at $x_0 \in \mathbf R^n$ if there exists a linear map $L:\mathbf R^n \to \mathbf R^p$ such that 
    \[  \lim_{h\to0} \frac{\|F(x_0+h) - F(x_0) - L(h)\|}{\|h\|} = 0    \] 
    
    we will denote $L = DF(x_0)$. The linear map $DF(x_0)$ is called the /derivative/ of $F$.
    #+END_defi
    
    An important thing to note is that $DF(x_0)$ is a linear map, so it applies
    to vectors in $\mathbf R^n$. This leaves us with the awkward notation 
    \[ L(h) = DF(x_0)(h) \]
    which becomes (maybe?) a bit less ambiguous by adding even more parentheses: 
    \[ L(h) = (DF(x_0))(h) \] 
    
    It is sometimes useful to divide $\mathbf R^n$ into two sets of coordinates,
    say $\mathbf R^n = \mathbf R^{n_1} \times \mathbf R^{n_2}$, so we study
    functions like $F(x,y)$. The partial derivative with respect to the first
    set of $n_1$ coordinates, evaluated at $(x_0, y_0)$, is denoted
    $D_1 F(x_0,y_0)$. It just means the derivative of the map 
    \[ x \mapsto F(x, y_0) \]
    evaluated at $x_0$ (whenever it exists). Whenever we consistently
    refer to the first set of coordinates as $x$, we can also write 
     \[ D_x F(x_0, y_0) \]
    to denote the same partial derivative.
    
    One important caveat is that both $D_x F(x_0, y_0)$ and $D_y F(x_0, y_0)$
    might be defined at a point where $F$ is not differentiable.
    
    
**** Facts about derivatives to have in mind
     I state a proposition that summarizes all that I will use about
     derivatives. As mentioned earlier, I don't give any proofs but they should
     be contained in any basic real analysis textbook.
     #+BEGIN_thm
     Let $F_1:\mathbf R^n\to\mathbf R^p$ and $F_2:\mathbf R^n\to\mathbf R^P$ be
     differentiable at $x_0 \in \mathbf R^n$, and let $G:\mathbf R^k\to\mathbf R^n$ be
     differentiable $z_0$, where $x_0 = G(z_0)$.
     Then:
     1. $F(x) = F_1(x) + F_2(x)$ is differentiable at $x_0$ and $DF(x_0) = DF_1(x_0) + DF_2(x_0)$
     2. $H(z) = F_1(G(z))$ is differentiable at $z_0$ and $DH(z_0) = DF_1(x_0)\circ DG(z_0)$
     #+END_thm

     [TODO: More in-depth about meaning of the two items. Especially the composition
     above, and the fact that $DH(z_0)$ is a linear map.]
     
     #+BEGIN_thm
     Let $g_1:\mathbf R^m\to\mathbf R^{n_1}$ and $g_2:\mathbf R^m\to\mathbf R^{n_2}$
     both be differentiable at $t_0 \in \mathbf R^m$. Let
     $F:\mathbf R^{n_1} \times \mathbf R^{n_2} \to \mathbf R^p$ be differentiable at
     $(x_0, y_0) = (g_1(t_0), g_2(t_0))$.

     Then 
     \[ \phi(t) = F(g_1(t), g_2(t)) \]
     is differentiable at $t_0$, and 
     \[ D\phi(t_0) = D_x F(x_0, y_0)\circ Dg_1(t_0) +  D_y F(x_0, y_0) \circ Dg_2(t_0) \]
     #+END_thm
     
*** Seven examples  
    TODO: finish
    
    1. Take $f_1(x) = Ax$. What is the derivative of $f_1$? Take $L(h) = Ah$.
       \[ f_1(x+h) - f_1(x) - L(h) \equiv 0\]
       hence $Df_1(x) = A$ for all $x$.
       
       Importantly, we specified the action above but it's good to repeat it: $(Df_1(x))(h) = Ah$. 
       
       That is, the derivative is $A$ and the action is on the left.
    2. Let $f_2(x) = x'B$. Take $T(h) = h' B$. 
    
** Recitation 6
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation6.pdf
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_TITLE: Intro to Econometrics: Recitation 6
   :EXPORT_SUBTITLE: Hansen chapters 1-5
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_beamer.tex}
   :END:
   
*** Intro
**** Roadmap
     Hansen chapters 2-5 overview
     * Chapter 2
       - Projection
       - Conditional expectation
       - Best linear predictor and linear regressions

*** Chapters 1-5
**** Chapter 2
     #+BEAMER: \framesubtitle{Projection}

     - <1-> Take $Y$ scalar rv and $\bfX = (X_1, \ldots, X_n)$. Consider following spaces:
       \[\mathcal L(\bfX) =  \{\hat Y : \hat Y = \sum_{i=1}^n  \beta_i X_i \} \]
       \[\mathcal E(\bfX) =  \{\hat Y : \hat Y = f(\bfX) \} \]
       #+BEAMER:\vspace{-0.4cm}
     - <2-> Let's restrict our analysis to variables $Y, X$ such that $E[|Y|^2] <\infty$ and $E[\|\bfX\|^2] < \infty$. Moreover, 
       assume $\mathbf E[\bfX\bfX'] > 0$
     - <3-> That way the inner product $\langle X, Y \rangle := \mathbf E(YX)$ is well defined
       
**** Chapter 2
     #+BEAMER: \framesubtitle{Projection}
     - <1-> With that inner product: what's projection of $Y$ (scalar valued) on $\mathcal L(\bfX)$?
     - <2-> Orthogonality condition: $\langle Y -  \bfX'\beta^*, \bfX'\beta\rangle = 0$ for all $\beta$
     - <3-> Implies:
       1. $\beta^* = \mathbf E[\bfX \bfX']^{-1} \mathbf E[\bfX' Y]$
       2. Error term associated with projection is uncorrelated with $\mathbf X$. Let $u^* = Y - \bfX'\beta^*$ 
          \[ \langle u^*, \bfX\rangle = \mathbf E[u^* \bfX] = 0\]
          
**** Chapter 2
     #+BEAMER: \framesubtitle{Projection}
     - <1-> How about the projection of $Y$ on $\mathcal E(\bfX) =  \{\hat Y : \hat Y = f(\bfX) \}$
     - <2-> It's a /function/ $f^*(\bfX)$ such that for any function $f$,
       \[  \langle Y - f^*(\bfX) , f(\bfX) \rangle = 0\] 
       #+BEAMER: \vspace{-0.4cm}
     - <3-> The function 
       \[ f^*(\bfX) = \mathbf E[ Y | \bfX ] \]
       satisfies the orthogonality conditions. (Check!)
     - <4-> Residual $u^*$ satisfies exogeneity $\mathbf E[u^* | X]= 0$
       
**** Chapter 2
     #+BEAMER: \framesubtitle{Projection}
     - Take away:
       1. <1-> For any variables $(Y, \bfX)$, you can /always/ find $\beta^*$ such that 
          \[ Y = \bfX\beta^* + u^* \] 
          and $E[u^* \bfX] = 0$ 
       2. <2-> You can always write 
          \[ Y = f^*(\bfX) + u^* \]
          where $\mathbf E[u^*|X] = 0$
**** Chapter 2
     #+BEAMER: \framesubtitle{Projection}
     - <1-> Wait a second: /how about all the resources people spend trying to argue for `exogeneity'/?
       - <2-> The point is /exactly/ that in empirical applications, estimating
         \[ y_i  = x_i'\beta + u_i\] 
         will give you $\beta^*$ in the limit
       - <3-> Sometimes $\beta^*$ is not the object of interest, but 
         \[ y_i  = x_i' \tilde \beta + v_i\]
         where $\mathbf E[x_i v_i] \ne 0$
**** Chapter 3 
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - <1-> Data $(y_i, \bfx_i)$, $i=1,\ldots, n$ identically distributed from some joint distribution $F$
     - <2-> Least squares problem: 
       \[ \min_\beta \sum_{i=1}^n (y_i - \bfx_i' \beta)^2 \] 
     - <3-> In matrix notation: 
       \[ \min_{\beta} \|\bfy - \bfX\beta\|^2 \]
     - <4-> Solution: $\hat\beta = (\bfX'\bfX)^{-1}\bfX'\bfy$
     - <5-> Orthogonality condition: $\bfX' [\bfy - \bfX \hat\beta] = 0$
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - Notation: 
       \[ \begin{aligned}
       \bfQxxhat &=\frac{1}{n} \bfX'\bfX = \sum_{i=1}^n \bfx_i x_i' \\
       \bfQxyhat &=\frac{1}{n} \bfX'\bfy = \sum_{i=1}^n \bfx_i y_i \\
       \bfP &= \bfX (\bfX'\bfX)^{-1} \bfX' \\
       \bfM &= \idd_n - \bfP = \idd_n - \bfX (\bfX ' \bfX)^{-1} \bfX'
       \end{aligned}\]
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - Note:
       \[\begin{aligned} \bfy &= \bfX\hat\beta + \overbrace{(\bfy - \bfX\hat\beta)}^{\text{LS residuals}} \\
            &= \bfX (\bfX'\bfX)^{-1} \bfX'\bfy + \left[\bfy - \bfX(\bfX'\bfX)^{-1} \bfy \right] \\ 
            &= \bfP \bfy + \bfM \bfy\end{aligned} \] 
     - Hence $\bfP \bfy$ is the predicted part and $\bfM \bfy$ is the residual
     - Matrices $\bfP$ and $\bfM$ are both /symmetric/, and satisfiy: 
       \[\begin{aligned} \bfP \bfP &= \bfP \\  
                         \bfM \bfM &= \bfM \\
                         \bfP \bfM &= \bfM \bfP = \mathbf 0 \\
                         \bfP \bfX &= \bfX  \\ 
                         \bfM \bfX &= \mathbf 0 \end{aligned}\]
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - Let's apply this machinery. Two components:
       \[ y_i = \bfx_{1i}' \beta_1 + \bfx_{2i}'\beta_2 + u_i \]
     - <2-> In matrix notation: 
       \[   \bfy = \bfX_1 \beta_1 + \bfX_2 \beta_2 + \bfu  \]
     - <3-> At the least squares solution, 
       \[ \bfy = \bfX_1 \hat\beta_1 + \bfX_2 \hat\beta_2 + \bfe \] 
       where 
       \[ \mathbf 0 = \bfX' \bfe = \begin{bmatrix} \bfX_1' \\ \bfX_2' \end{bmatrix} \bfe  \] 
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - Define $\bfP_j, \bfM_j$ for $j=1,2$ accordingly
     - Suppose we want to find expression for $\hat\beta_1$. Can get rid of $\bfX_2$ by multiplying $\bfM_2$!
       \[ \bfM_2 \bfy = \bfM_2 \bfX_1\hat\beta_1 + \bfM_2 \bfX_2 \hat\beta_2 + \bfM_2 \bfe  \]
     - Note: $\bfM_2 \bfe = \bfe - \bfX_2(\bfX_2'\bfX_2)^{-1} \bfX_2' \bfe = \bfe$
     - Hence 
       \[ \bfM_2 \bfy = \bfM_2 \bfX_1 \hat\beta_1 + \bfe \]
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - <1-> Denote $\tilde \bfy = \bfM_2 \bfy$ and $\tilde \bfX_1 = \bfM_2 \bfX_1$
     - <2-> We have 
       \[ \tilde \bfy = \tilde \bfX_1 \hat\beta_1 + \bfe \] 
       Moreover, 
       \[\tilde \bfX_1' \bfe = \bfX_1' \bfM_2 \bfe = \mathbf 0\]
       Thus (as long as $\tilde X_2$ is full row rank):
       \[\hat\beta_1 = (\tilde \bfX_1'\tilde \bfX_1)^{-1} \tilde \bfX_1' \bfy = (\bfX_1' \bfM_2 \bfX_1)^{-1} \bfX_1'\bfM_2 \bfy \]
     - <3-> Interpretation? Frisch-Waugh-Lovell
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: statistical models}
     - <1-> Data $(y_i, \bfx_i)$ independently drawn from $F(y, \bfx)$
     - <2-> Statistical model will put further restrictions on $F$.
     - <3-> Note: not assuming deterministic $\mathbf x_i$ anymore. Analysis will strongly rely on conditioning
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: statistical models}
     - <1-> Depending on what properties you want to get for OLS, different assumptions are required
       1. <2-> *Linear regression model.*
          - $\mathbf E[y_i | \bfx_i] = \bfx_i' \beta$ 
          - Finite second moments, and $\mathbf E[\bfx_i \bfx_i']$ invertible
     - <3-> With the above assumption, we get an unbiased OLS estimator.
     - <4-> What about `optimality' in any sense? Need restriction on second moments.
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: statistical models}
     - Another assumption: 
       2. [@2] *Homoskedasticity.* In addition to linear regression hypohtesis,
          \[ \mathbf V[y_i| \bfx_i] \equiv \sigma^2 \] 
          
     - Then we get the Gauss-Markov result.
***** Gauss-Markov 
      :PROPERTIES: 
      :BEAMER_env: theorem
      :BEAMER_opt: shadow=true
      :END:
      In the homoskedastic linear regression model, $\hat\beta$ is the best
      linear unbiased estimator of $\beta$ (with $L^2$ loss).
      
      That means that any other unbiased estimator $\tilde \beta = \tilde A\bf y$ satisfies 
      \[ \mathbf V[\tilde \beta | \bfX ] \geq \mathbf V[\hat\beta | \bfX]\]
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: statistical models}
     - When homoskedasticity is not assumed, we can sometimes do better than OLS
     - For example, if we abandon the iid assumption, and instead only impose finite second moments and 
       \[  \mathbf E[\bfy | \mathbf X] = \mathbf X\beta  \]
       \[  \mathbf V[\bfy | \mathbf X] = \Omega  \]
     - If $\Omega$ is known, the /Generalized Least Squares estimator/ is the way to go.
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: statistical models}
     - Another important case that we frequently find in applied work:
       \[  \mathbf V[y_i | \bfx_i] = \varsigma(x_i)^2 = \sigma_i^2 \]
     - With the above form for residual variace, we have a /heteroskedastic linear regression model/ 
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: variance estimation}
     - TBI
     
** Unsorted
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/unsorted-notes.pdf
   :EXPORT_TITLE: Unsorted notes
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_basic.tex}
   :EXPORT_OPTIONS: toc:nil
   :END: 
   
   Here I store some random notes that I may or may note talk about during recitations.
   
*** Lectures 1 & 2
    - _Finite additivity_

      Let's define some notation. I can define the following for any indexed collection of sets $A_i$:
      \[A_1 + A_2 := A_1 \cup  A_2\]
      or, more generally
      \[
      \sum_i A_i := \bigcup_i A_i
      \]
      whenever the collection $A_i$ is pairwise disjoint.

      The idea of assuming additivity -- without any further qualification --
      is that set-function $\mathbf P$ satisfies some form of linearity, that is
      \[
      \mathbf P\left(   \sum A_i  \right) = \sum_i \mathbf P \left(  A_i \right)
      \]
      It turns out that the set of indices over which this assumption is made is
      consequential.

      We call $\mathbf P$ /finitely additive/ if the above is required to hold
      for all finite sets of indices. Similarly, if the relationship holds for
      countably many indices, $\mathbf P$ is called /countably additive/.
      
      Let's investigate an example of finitely, but not countably, additive
      measure. Here, we are working with a triple $(X, \mathcal A, \mathbf P)$.
      $\mathcal A$ is an /algebra/ of sets. Very similar to the usual
      $\sigma-\text{algebra}$ couterpart, but we don't require the assumptions
      of closedness under unions and intersections to hold for infinitely many
      set, only finitely many.

      We will work with the following algebra, which is not a
      $\sigma\text{-algebra}$. Let $X$ be the set of all natural numbers,
      $\mathbf N$. Define also 
      \[
      \mathcal A = \left\{ A \subset \mathbf N: A\text{ is finite or } A^c \text{ is finite} \right\}
      \]
      
      Example of sets in $\mathcal A$: $\{1, 2, 3\}$ and $\{5001,
      5002,\ldots\}$. Example of a sets /not/ in $\mathcal A$: the set of all
      odd/even/prime numbers.[fn:1] 
      
      It's not hard to see that this is satisfies: $\emptyset \in \mathcal A$
      (since $\emptyset$ is finite) and closedness under intersections/unions.
      The reason why $\mathcal A$ is not a $\sigma\text{-algebra}$ is that each
      $A_i = \{1, 3, \ldots, 2i + 1\}$ is in $\mathcal A$, but its infinite
      union, the set of all odd numbers, is not.
      
      Now consider the probability measure: $\mathbf P:\mathcal A \to [0,1]$: 
      \[ \mathbf P(A) = 
      \begin{cases} 1 &\text{if } A\text{ is infinite}  \\ 0 &\text{ otherwise} \end{cases} \]
      Thus, for example, $\mathbf P({1,2,3}) = 0$ and $P(\{1023, 1024, \ldots\}) = 1$.
      
      Such $\mathbf P$ trivially satisfies $\mathbf P(A + A') = \mathbf P(A) + \mathbf P(A')$ because
      the finite union of finite sets is finite.
      
      This probability measure is interesting because it provides a
      counter-example to continuity when $\mathbf P$ is only finitely, but not
      countably, additive.
      
      For example, it holds that $\{1,2,\ldots, n\} \uparrow \mathbf N$, but 
       \[\begin{aligned} 1 = \mathbf P(\mathbf N) &= \mathbf P\left( \bigcup_n \left\{ 1,2,\ldots, n \right\} \right)
       &\ne \lim_n \mathbf P\left( \left\{ 1,2,\ldots, n\right\} \right)  = 0
       \end{aligned}\]
       
       Moreover, $\{n+1, n+2, \ldots\} \downarrow \emptyset$, but 
       \[ 0 = \mathbf P(\emptyset) = \mathbf P\left( \bigcap_n \{n+1, n+2, \ldots\} \right) \ne
            \lim_n \mathbf P\left( \{n+1, n+2, \ldots \} \right) = 1 \]
            
       The CDF of the random variable $X:\mathbf N \to \mathbf N$, $X(n) = n$
       according to $\mathbf P$ will satisfy:
       \[ F_X(k) = \mathbf P\{n: X(n) \leq k\}=  0\]
       for all $n$, so $\lim F_X(k) = 0$ for $k\to\infty$. 
       #+LATEX: \clearpage
*** Best linear predictor, matrix version
    Let $M(n,k)$ denote the linear space of all matrix of dimension $n\times k$.
    
    Suppose we have random vectors $(\mathbf y(\omega), \mathbf z(\omega))'$. We
    know additionally that $\mathbf y \in M(n,1)$ and $\mathbf z \in M(k,1)$ and these vectors
    have finite mean and variance. Denote their mean by 
    \[ \begin{bmatrix} \mu_y \\ \mu_z \end{bmatrix} \]
    and their variance matrix by
    \[ \begin{bmatrix} \Sigma_{yy} & \Sigma_{yz} \\ \Sigma_{zy} & \Sigma_{zz} \end{bmatrix} \]

    We define the *best linear predictor* of $\mathbf y$ given $\mathbf z$ as the random variable $\mathbf w$ such that 
    \[ \mathbf w^* = \alpha^* + \beta^* (\mathbf z - \mu_z) \]
    where $\alpha^* \in M(n,1)$ and $\beta^* \in M(n,k)$ solve the minimzation problem 
    \[ \min_{\alpha, \beta} \mathbf E \left[ \| \mathbf y - \alpha - \beta(\mathbf z - \mu_z) \|^2 \right]  \] 
    
    You can solve it either by using calculus -- which can be cumbersome if you're
    not used to matrix derivatives -- or by noting that the minimand is a squared norm
    generated by the inner product
    \[ \langle \mathbf y, \mathbf w \rangle := \mathbf E[\mathbf w' \mathbf y] \]
    
    of all vectors of the type $\mathbf y - \mathbf w$ where $\mathbf w = \alpha + \beta(\mathbf z - \mu_z)$ for some $\alpha, \beta$.
    
    Let $\epsilon := \mathbf y - \mathbf w^*$ denote the residual of the
    minimization problem. Then $\epsilon$ must be orthogonal (by Hilbert's
    projection theorem) to every $\mathbf w = \alpha + \beta (\mathbf z - \mu_z)$.
    
    Taking $\beta=0$, we see that $\mathbf w^*$ must satisfy 
    \[  0 = \langle \mathbf y - \mathbf w^*, \alpha \rangle = \mathbf E\left[ \alpha' \mathbf y  \right] - \mathbf E\left[ \alpha' \alpha^*  \right] \]
    for all vectors $\alpha \in M(n,1)$. Taking these to be the elements of the canonical basis, we conclude that
    \[ \alpha^* = \mu_y\]
    
    Now take $\alpha=0$. The orthogonality condition now implies that for any $\beta \in M(n,k)$,
    \[ 0 = \langle \mathbf y - \beta^* (\mathbf z - \mu_z) , \beta ( \mathbf z - \mu_z ) \rangle  =  \mathbf E\left[ (\mathbf z -\mu_z)' \beta' y  \right] - \mathbf E\left[ (\mathbf z -\mu_z)' \beta' \beta^* (\mathbf z - \mu_z)  \right]  \]
    
    Use the properties of the trace -- namely, that it's linear and that matrix
    multiplication commutes inside it -- and of the expectation operator to
    conclude that
    \[ \tr \left(  \beta' \mathbf E\left[ \mathbf y(\mathbf z - \mu_z)' \right] \right)  = \tr \left(\beta' \beta^* \mathbf E\left[ \left( \mathbf z - \mu_z   \right) \left( \mathbf z - \mu_z  \right)' \right] \right) \] 
    
    note that $\mathbf E[\mathbf y(\mathbf z - \mu_z)'] = \Sigma_{yz}$ and
    $\mathbf E[(\mathbf z-\mu_z)(\mathbf z - \mu_z)'] = \Sigma_{zz}$. The equation above then implies that  
    \[ \tr (\beta' \Sigma_{yz} ) = \tr (\beta' \beta^* \Sigma_{zz}) \]
    
    should hold for all matrices $\beta \in M(n,k)$. That implies,[fn:2] 
    \[\Sigma_{yz} = \beta^* \Sigma_{zz} \]
    which in turn yields $\beta^* = \Sigma_{yz} \Sigma_{zz}^{-1}$ whenever
    $\Sigma_{zz}$ has an inverse. In that case, the BLP is 
    #+NAME: eq:expression_blp
    \begin{equation} \mathbf w^*  = \mu_y + \Sigma_{yz} \Sigma_{zz}^{-1} (\mathbf z - \mu_z)  \end{equation} 
    
**** Appendix: the Trace operator
     - let $A(i,j)$ denote the entry $(i,j)$ of any matrix
     - Let $A$ be a $m\times n$ matrix. The trace is defined as
       \[ \tr A = \sum_{i=1}^{\min\{m,n\}} {A(i,i)}\]
       in other words, it's just the sum of elements in the main diagonal.
     - Some properties of the trace: 
       1. $\tr(A + B) = \tr(A) + \tr(B)$ whenever $A$ and $B$ have similar dimensions
       2. $\tr(kA) = k\,\tr(A)$ for all scalars $k$
       3. $\tr(AB) = \tr(BA)$ whenever dimensions are such that both multiplications make sense
       Curiosity: any operation $\tilde\tr$ that satisfies the properties above
       is equal to $\tr$ (modulo multiplication by a constant)
     - The trace and expectation operators commute: 
       \[\tr (\mathbf EA) = \mathbf E (\tr A)\]
     - Suppose $A \in M(m,n)$ and you want to select element $(i,j)$ from it. Note that
       \[ A(i,j) =  e_i' A \varepsilon_j = tr(e_i' A \varepsilon_j) = tr(\varepsilon_j e_i' A) \]
       where $e_i$ is the i-th element in the canonical basis of $R^m$ and
       $\varepsilon_j$ is the j-th element of the canonical basis of $R^n$.
       
       Hence for any $(i,j)$, letting $B = \varepsilon_j e_i' \in M(n,m)$ we have 
       \[A(i,j) = \tr ( B A ) \]
     - This implies that if $A$ and $\tilde A$ are fixed $m\times n$ matrices, and  
       \[ \tr(BA) = tr(B\tilde A) \]
       holds for every $B \in M(n,m)$, then 
       \[ A = \tilde A\]
     #+LATEX: \clearpage

*** DISCARD: Comments on PS3, Q3: mixed vs behavioral strategies   :noexport:
    Note that when the action space is $\mathcal A=\{a_0, a_1\}$ -- with
    associated loss function $\mathcal L(a_i, \theta_j) = \mathbf 1(i \ne j)$ --
    we have four possible strategies.

    Let's call $\tilde A$ the set of decision rules: $\tilde A = \{d_1, d_2, d_3, d_4\}$,
    as in the question.
    The risk function evaluated at a given decision rule $d$ is simply a vector
    \[\begin{bmatrix}
    R(d, \theta_0) \\
    R(d, \theta_1)
    \end{bmatrix} \]

    Let $D(\tilde A)$ denote the set of all points in $\mathbf R^2$ that are the
    risk function of some decision rule. It was your job to show that
    \[ D(\tilde A) = \Big \{ \overbrace{\begin{bmatrix} 0 \\ 1\end{bmatrix}}^{d_1},
    \overbrace{\begin{bmatrix}  1-p_0 \\ p_1\end{bmatrix}}^{d_2},
    \overbrace{\begin{bmatrix} p_0 \\ 1-p_1\end{bmatrix}}^{d_3},
    \overbrace{\begin{bmatrix} 1 \\ 0\end{bmatrix}}^{d_4}
    \Big \}\]
    [Let's call this the /risk set/ associated with this decision problem.]


    The problem set presented one way of ``randomizing risk''. Upon observing
    data, in part 3 you allowed, instead of choosing any particular action in
    $\mathcal A$, to choose a distribution over points in $\mathcal A$. Since
    $\mathcal A$ is binary, this amounts to choosing a number $\delta \in [0,1]$
    representing the probability of playing $a_0$. COMMENT: notation

    So the new action space is $\mathcal A' = [0,1]$.


    What is then a decision rule in this context? Again, any function
    $d:\mathcal X\to\mathcal A'$. Now $\mathcal A'$ is not binary anymore, so
    there are uncountably many possible decision rules.

    In fact, each decision rule $(\delta_0, \delta_1)$ can be mapped to the
    square $[0,1]\times[0,1]$.

    

*** Admissible tests and maximization of power subject to size (WIP)
    In lecture notes 9-10, Proposition 1 characterizes admissible tests in terms
    of the solution of an problem of maximizing power subject to a size
    constraint. I reproduce the statement of that proposition below.

    #+NAME: prop:admissible-test-characterization
    #+BEGIN_prop
    Suppose that for any set $A \subseteq \mathbf{X}$
    $$\int_{A} f(x,\theta_0)dx > 0 \implies \int_{A} f(x,\theta_1)dx > 0 .$$ 
    A randomized test $\phi$ is admissible if and only if there exists $\alpha \in [0,1]$ such that $\phi$ maximizes power subject to having size at most $\alpha$; that is
    #+NAME: equation:optimization
    \begin{equation}
    \phi \in \arg \max_{\phi} \left( 1-R(\phi, \theta_1) \right)
    \end{equation}
    \noindent s.t.
    #+NAME: equation:sizecontrol
    \begin{equation}
    R(\phi, \theta_0) \leq \alpha
    \end{equation}
    #+END_prop
    
    
    That proposition is actually really nice. In standard statistics courses, we
    sometimes take this maximization problem as the starting point, as if it's
    somehow self-evident that we should seek tests that /maximize power subject
    to size/. With the decision theoretic framework we built in the first few
    lectures, we can actually understand why tests that solve this maximization
    problem are of any interest to us. The reason is that this procedure yields
    tests that aren't dominated.

    Another way of framing the proposition is the following. For a fixed
    $\alpha\in[0,1]$, let $\Phi^*(\alpha)$ denote the set of all tests $\phi^*$
    that maximize ([[equation:optimization]]) subject to ([[equation:sizecontrol]]). 

    The correspondence $\Phi^*(\alpha)$ depends on a single parameter
    $\alpha \in [0,1]$. What proposition 1 says is that, as we vary $\alpha$,
    we cover all possible admissible tests. In other words, 
    \[ \mathcal A = \bigcup_{\alpha \in [0,1]} \Phi^*(\alpha) \] 
     is /exactly/ the set of all admissible tests. 
**** Elaborating Proposition [[prop:admissible-test-characterization]]
     I modify the proposition's exposition to make it a bit more digestible.
   
     First, let's define the following. 
     #+BEGIN_defi
     Let $\{f_\theta(x)\}_{\theta \in \Theta}$ be a statistical model. We say that 
      \[ f_{\theta_0} \ll f_{\theta_1} \]
      \noindent (in plain English: $f_{\theta_0}$ is /dominated/ by $f_{\theta_1}$) if, for every
      measurable set $A$,
      \[ \mathbf P_{\theta_1}(A) = 0 \implies \mathbf P_{\theta_0}(A) = 0 \]
     #+END_defi
     
     *Important remark.* The relation $\ll$ has /nothing/ to do with risk, loss,
     etc. It also has nothing to do with stochastic dominance.
     
     Let's translate the definition above. What it means for $f_{\theta_0}$ to
     be dominated by $f_{\theta_1}$ is that, if the statistical model under
     $\theta_1$ assigns zero probability to a set $A$ -- that is, there is a
     zero probability that we observe data in the set $A$ under the alternative
     -- then the probability that we observe data in the set $A$ under the null
     must also be zero.
     
     In other words, if that condition didn't hold, there would be a set of data
     realizations that are ``impossible'' under the alternative, but ``possible''
     under the null.

     Note that we can rewrite the definition in terms of integrals, since
     \[ \mathbf P_\theta(A) = \int_A f_\theta(x) dx \]
     
     Hence, $f_{\theta_0} \ll f_{\theta_1}$ if and only if
     \[ \int_{A} f_{\theta_1} (x) dx = 0 \implies \int_A f_{\theta_0}(x) dx = 0 \]
     
     Or yet (by contraposition): $f_{\theta_0} \ll f_{\theta_1}$ iff 
     \[ \int_{A} f_{\theta_0} (x) dx > 0 \implies \int_A f_{\theta_1}(x) dx > 0 \]
     
     All of these are restatements of the assumption that we can't observe
     under the null things that can't be observed under the alternative. 
     
     That assumption gives us an important result, that I state as a lemma. 
     #+NAME: lemma:trivialrisk
     #+BEGIN_lemma
     Let $\{f_\theta\}_{\theta \in \Theta}$ be a statistical model with $\Theta = \{\theta_0, \theta_1\}$.
     Suppose $f_{\theta_0} \ll f_{\theta_1}$. 
  
     Then any test $\phi$ achieving full power must have size equals one. Mathematically: 
     \[  \mathbf E_{\theta_1}[\phi(X)] = 1 \implies \mathbf E_{\theta_0}[\phi(X)] = 1\]
     
     Moreover, tests achieving zero size must have trivial power: 
     \[ \mathbf E_{\theta_0}[\phi(X)] = 0 \implies \mathbf E_{\theta_1}[\phi(X)] = 0\]
     #+END_lemma
     
     #+BEGIN_proof
     Since $\phi(X) \leq 1$, full power -- ie $\mathbf E_{\theta_1}\phi(X) = 1$ -- implies that the set 
     $A = \{x \in \mathcal X: \phi(x) < 1 \}$ has zero probability under $\theta_1$. Thus
     \[ \int_{\phi(x)<1} f_{\theta_1}(x) dx  = 0\] 
     
     Since $f_{\theta_0}$ is dominated by $f_{\theta_1}$,  
     \[\mathbf E_{\theta_0} \phi(X) = \int_{\{\phi(x) = 1\}} \phi(x) f_{\theta_0}(x)dx + \cancelto{0}{\int_{\{\phi(x) < 1\}} \phi(x) f_{\theta_0}(x)dx} = 1\]
     #+END_proof
     
     I'll now restate one directions of Proposition 1, for the particular case when $0 < \alpha < 1$.
     #+BEGIN_prop
     Let $\{f_\theta\}_{\theta \in \Theta}$ be a statistical model with $\Theta = \{\theta_0, \theta_1\}$.
     
     Suppose $f_{\theta_0} \ll f_{\theta_1}$. Then any (randomized) test
     $\phi^*$ that solves the problem below is admissible in a decision problem with 0-1
     loss, when $\alpha \in (0, 1)$.
     \[\tag{P}\begin{aligned} &\max_{\phi} && E_{\theta_1} \phi(X) \\
     & \text{s.t.} && E_{\theta_0} \phi(X) \leq \alpha 
     \end{aligned}\]
     #+END_prop
     
     #+BEGIN_proof
     Let's proceed by contradiction. Assume that $\phi^*$ solves the
     maximization problem but is not admissible. Then there exists some test
     $\phi$ that dominates $\phi^*$, that is:
     
     #+NAME: eq:lower_risk_null
     \begin{equation} 
      R(\phi, \theta_0) = \mathbf E_{\theta_0} [\phi(X)] \leq \mathbf E_{\theta_0}[\phi^*(X)] = R(\phi^*, \theta_0)
     \end{equation}
     
     #+NAME: eq:lower_risk_alt
     \begin{equation} 
      R(\phi, \theta_1) = 1 - \mathbf E_{\theta_1}[\phi(X)] \leq 1 - \mathbf E_{\theta_1}[\phi^*(X)] = R(\phi^*, \theta_1) 
     \end{equation}
     where one of the equalities holds strictly. We consider the two cases below. 
     1. Suppose [[eq:lower_risk_null]] holds strictly, and [[eq:lower_risk_alt]] holds weakly. 
        Since $\phi^*$ solves the maximization problem (P), the size constraint must
        be satisfied so
        \[ \mathbf E_{\theta_0} [\phi(X)] < \mathbf E_{\theta_0}[\phi^*(X)] \leq \alpha < 1  \]
        This first thing to note, which will only be used later on, is that since
        $\mathbf E_{\theta_0} [\phi(X)] < 1$, it must be that
        $\mathbf E_{\theta_1}[\phi(X)] < 1$ by the first part of
        Lemma [[lemma:trivialrisk]].
        
        The idea of the proof is to construct yet another test that will use up
        the slack that $\phi$ has in the size constraint,
        $\mathbf E_{\theta_0}[\phi(X)] < \alpha$, to achieve higher power.

        We can do that by mixing $\phi$ with the test that rejects the null
        for any realization,
         \[  \phi_R(X) \equiv 1  \]
        and by picking the right mix, we will increase power relative to $\phi$,
        while still controlling for size. By [[eq:lower_risk_alt]], we will also
        improve relative to $\phi^*$, a contradiction.
        
        Now how do we find that combination? Consider, for arbitrary
        $\lambda\in[0,1]$, the test
        \[ \phi_\lambda(X) \equiv \lambda \phi^R(X) + (1-\lambda) \phi(X)  \]
        (Make sure you understand why we combine $\phi$ with $\phi^R$, in
        particular why we don't combine $\phi^R$ with $\phi^*$.) Its rate of type I error is given by 
        \[  \mathbf E_{\theta_0} [ \phi_\lambda (X) ]  = \lambda + (1-\lambda) E_{\theta_0} [\phi(X)]  \]

        We pick $\bar\lambda$ that gives size exactly equal to $\alpha$ by setting 
        \[ \bar\lambda = \frac{\alpha - \mathbf E_{\theta_0}[\phi(X)]}{1 - \mathbf E_{\theta_0}[\phi(X)]} \]
        
        Since $0 \leq E_{\theta_0}[\phi(X)] < \alpha < 1$, we have $\bar\lambda\in(0,1)$. 
        
        By construction, $\phi_{\bar \lambda}$ has a rate of type I error of
        exactly $\alpha$. Its power on the other hand is given by 
        \[ \mathbf E_{\theta_1} [ \phi_{\bar\lambda}(X) ] = \bar\lambda \cdot 1 + (1-\bar\lambda) \cdot \mathbf E_{\theta_1} [\phi(X)] \]
        
        Because $\mathbf E_{\theta_1}[\phi(X)] < 1$ and $\bar\lambda \in (0,1)$, the above expression implies
        \[ \mathbf E_{\theta_1}[\phi_{\bar\lambda}(X)] > \mathbf E_{\theta_1}[\phi(X)] \geq \mathbf E_{\theta_1}[\phi^*(X)] \]
         
        Where the last inequality comes from the assumption ([[eq:lower_risk_alt]]). That is a contradiction with the
        fact that $\phi^*$ is solves problem (P).
     2. Suppose now that ([[eq:lower_risk_alt]]) holds strictly, while
        ([[eq:lower_risk_null]]) holds weakly. Then ([[eq:lower_risk_null]]) implies
        $\phi$ satisfies the size constraint, and 
        \[ \mathbf E_{\theta_1}[\phi(X)] > \mathbf E_{\theta_1}[\phi^*(X)] \]
        implies that $\phi$ achieves strictly higher power than $\phi^*$, in
        direct contradiction with the fact that $\phi^*$ solves problem (P).
     #+END_proof
      
      
     
* Footnotes

[fn:2] See the appendix on the trace operator for details.

[fn:1]  The sets whose complement is finite are called co-finite sets.


     

