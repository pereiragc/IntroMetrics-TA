#+TITLE: Fall 19: Intro to Econometrics TA material  
#+AUTHOR: Gustavo Pereira
#+STARTUP: beamer
#+STARTUP: org


* README page
** Intro to Econometrics: TA repo
  :PROPERTIES: 
  :EXPORT_FILE_NAME: README.org
  :END:
   Welcome! This repository was created to store and maintain the materials
   used or referred to in the recitations. 
  
   Schedule: 
   | what           | when                | where                           |
   |----------------+---------------------+---------------------------------|
   | *Recitation*   | See syllabus        | 227 Mudd / 403 IAB              |
   | *Office hours* | Tuesday 09:00-10:00 | Lehman Library group study area |
  
   Pull requests are encouraged!
   
*** Recitations 

**** Recitation 8
     [[file:notes/Recitation8.pdf][Here]].

     In this recitation I:
     1) Introduced (stochastic) order notation, which can be useful to simplify
        proofs about asymptotics later on
     2) Reviewed the basic asymptotic machinery: CMT, Slutzky's theorem, Delta
        method. 

        I didn't focus on the proofs but some of them can be found in [[file:other_notes/N04_asymptotic_order.pdf][these notes]].
     3) Talked about OLS asymptotics under different sets of assumptions. There
        was no time to cover this in much detail so I strongly suggest you read
        BH chapter 7
        
*** Questions outside OH
    See [[file:outside_oh_questions.pdf][here]]!

      

   
* Questions outside office hours
  :PROPERTIES: 
  :EXPORT_FILE_NAME: outside_oh_questions.pdf
  :EXPORT_TITLE: Out-of-OH Q&A   
  :EXPORT_AUTHOR: Gustavo Pereira
  :EXPORT_LATEX_HEADER: \input{auxfiles/header_basic.tex}
  :EXPORT_OPTIONS: ^:nil
  :END: 
** Why
   The ideal time to ask questions about the material and problem sets is during
   office hours. However, many of you can't make it to office hours for valid
   reasons, so I also try to respond to questions sent by e-mail or other
   means.

   Keeping this document helps you (and yours truly) for a few reasons: 
   - More clarifications/explanations available
   - There is no `unfair advantage' to those who ask many questions
   - I don't have to answer the same thing over and over :)

     #+LATEX: \clearpage
** First half - pset 1
    
*** About limits of sets, and Durrett and Billingsley being wrong
    #+begin_quote
    ``Hi Gustavo,
     
    Thank you for the class today. Can I further clarify Q4 you briefly
    discussed in class? I initially directly used $\lim(-\infty,x) \to \Omega$ in
    the steps, but you pointed out it was wrong. I'm actually still a bit
    confused about why it is wrong. I referred to *Durrett* (who directly used
    $\lim(-\infty,x) \to \Omega$) and *Billingsley* (who states "clearly"). So,
    unfortunately, they both would get the proof wrong. Would it be possible
    for you to give me some hints on which theorems would be useful in the
    proof? Thank you so much!
    
    [screenshot of Durrett's book, theorem 1.1]
     
    Best,
    
    xx''
    
    (I kept the bold face from the e-mail)
    #+end_quote

    Dear xx, 
    
    I wouldn't dare to say that Durrett would get it wrong if he was answering
    the problem set! But definitely, if he just copied and pasted from his
    own book, he would be discounted.
    
    Here's the reason: earlier in the book, he states the continuity from above
    and below of probability measures in terms of collections of sets /indexed
    by natural numbers/. In order to do that, he defines what it means to say
    \[  A_i \uparrow A \]
    for that class of collections.
    
    Later, in the context of proving the limits of CDFs, he applies a
    similar statement to the collection $\{ (-\infty, x] \}_{x\in \mathbf R}$.
    The problem is that it's not indexed by natural numbers. So if he wanted
    full credit, he'd need to clarify he meant by
    \[ (-\infty, x] \uparrow \mathbf R \]
    and why continuity of $\mathbf P$ -- defined only for limits of
    ``increasing'' sets indexed by natural numbers -- also applies for these
    limits.

    
    The hint is the same one I gave in the recitation. Use the fact (no need to
    prove it) that
    \[ \lim_{x\to\infty} F(x) = 1 \]
    if, and only if, for every /increasing/ diverging sequence $x_n \uparrow \infty$, 
    \[  F(x_n) \to 1 \] 
    and try to apply countable additivity.
    

    Sincerely, 
    
    Gustavo

** First half - pset 4
   
*** Clarifying the meaning of posterior in Q3
    #+begin_quote
    In Q3 of PS4, we are asked to compute the posterior mean for $\beta$ and
    $\sigma$, and I assumed that usually meant the expectation of the conditional
    distribution of $\beta$, conditioned on data, and similarly, the conditional
    distribution of $\sigma$ on the data.

    In the hint, I'm questioning my understanding because $\beta$ was conditioned on
    $\sigma$ as well. Going forward, does that mean posterior distributions condition
    on data and all other parameters except the parameter in question? More
    specifically, do you know of any resources where I could read up on the
    mechanics behind this?
    #+end_quote
    
    Computing the distribution of beta given Y and sigma is only supposed to be an intermediate step to make calculations easiser. 

    The end goal is to find the joint distribution of beta and sigma given data. 
    

    
*** Q4 - all $\beta$ vs some $\beta$
    #+BEGIN_quote
    I was working on question 4 and I realized that I have a bit of a gap in my
    understanding about admissible decision rules. I know that a Ridge estimator
    is a Bayesian estimator, and thus is admissible. I know that an admissible
    decision rule is not dominated, so since the OLS estimator is another
    decision rule, it is not the case that the risk of the OLS estimator is less
    than or equal to the risk of the Ridge estimator for all beta and strictly
    less than for some beta. So then, as I understand it, this implies that
    there is *some* beta for which the OLS estimator has strictly higher risk
    than the Ridge estimator (but I know nothing about how they compare for any
    other beta). But, I don't think this tells me anything about how the risks
    compare for all the other betas, right?

    Since in this problem we'd have that the risk for each estimator is the mean
    squared error, and thus bias + tr(variance), I think I'd want to have an
    inequality the compares the MSE of the Ridge estimator to the MSE of the OLS
    estimator, but from the fact that the Ridge estimator is admissible I only
    see how to write that inequality for *some* beta -- how can I extrapolate to
    being able to write an inequality for *all* beta, in order to be able to
    make a statement about how tr(var(beta_ols)) compares to tr(var(beta_ridge))
    compare?
    #+end_quote
    
    You're in the right track, but let me point something out first. Admissibility guarantees that 
    \[ \text{MSE}(\hat\beta^{\text{Ridge}}, \beta) \leq \text{MSE}(\hat\beta^{\text{OLS}}, \beta) \] 
    holds for at least one $\beta$. Rigorously speaking, the inequality is not
    necessarily strict, because they could have the same risk all over the parameter
    space and that wouldn't violate admissibility. 

    Now with that caveat, I would suggest that instead of trying to get a
    comparison that holds for every possible $\beta$, try to use the one $\beta$
    you know exists for which the ranking of MSEs above holds. Write down both
    sides of the inequality for that particular $\beta$, and see if you can
    compare either side with $V_\beta(\hat\beta^{\text{Ridge}})$ -- note that
    this variance does not depend on $\beta$.
    
    
** Second half - pset 1
*** Approaching Q8 
    #+begin_quote
    Could you give us a hint about Q8? We've been trying to solve it for a while now and we don't even know how start.
    #+end_quote
   

    It might be useful to write 
    \[  (T_n - \theta) = A_n B_n  \]

    where $A_n = \frac{1}{\sqrt{n}}$ and $B_n = \sqrt{n} (T_n - \theta)$. 

    Any deterministic sequence $A_n \to A$ also satisfies $A_n \overset{d}{\to} A$. Can you say something about $A_nB_n$?
   
    
* Recitation Notes
** Recitation 1
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation1.pdf
   :EXPORT_TITLE: Recitation 1
   :EXPORT_OPTIONS: toc:nil
   :EXPORT_LATEX_HEADER: \input{auxfiles/header_basic.tex}
   :END: 

   In this recitation, I review the material presented in lectures 1 and 2. I
   also cover some things that might be challenging in the first problem sets. 
   
*** Review: lectures 1 and 2
    - Definition of probability space: $(\Omega, \mathcal F, \mathbf P:\mathcal F \to [0,1])$
      - The point of $(\Omega, \mathcal F)$ is to provide a model for the
        /randomness of some outcome/.
      - Remember: we don't observe randomness. We observe some outcome. Then, we
        use a model to infer what are more or less likely ``states of the world'',
        because that allows us to predict things
      - The reason we keep $\Omega$ abstract (instead of focusing on say
        $\Omega=[0,1]$) is that it allows us to deal with a variety of possible
        structures for the outcome space!
    - Random variables: /measurable/ functions $X:\Omega \to S$ where $S$ is some
      space of outcomes.
    - Probability space induced by a random variable
      - Original space: $(\Omega, \mathcal F, \mathbf P)$
      - RV `measurably' maps original space to $(S, \mathcal S)$
      - Induced measure: $\mathbf P_X(F) = \mathbf P\left\{ \omega: X(\omega) \in F \right\}$ for $F \in \mathcal S$
        - Curiosity: this is called a push-forward measure in mesasure theory
      - Probability space $(S, \mathcal S, \mathbf P_X)$ is typically some
        Euclidean space (though it can be more complicated)
    - Let's now focus on the case when $X:\Omega \to S$ is real valued, ie, $S=\mathbf R$.
    - CDF of a random variable: $F_X(x) = \mathbf P\left\{ \omega: X(\omega) \leq x \right\} = \mathbf P_X((-\infty, x])$
      - Result: all information in $\mathbf P_X$ is in $F_X$ and vice-versa.
      - Properties of CDF
        1. $F_X$ is non-decreasing
        2. $\lim_{x\to\infty} F_X(x) = 1$
        3. $\lim_{x\to-\infty} F_X(x) = 0$
        4. $F_X$ is right continuous
      - *First main result*: every function $F$ satisfying all four properties
        above is the CDF of some random variable.
    - Absolutely continuous random variable: $\exists f_X$ such that
      \[ F_X(x) = \int_{-\infty}^x f_X(z) dz \]
      + Weirdly enough, the non-obvious thing about the statement above is not
        the $\exists f_X$ but the $dz$. 
      + Measure theoretic details aside, the important thing is that $dz$ is
        never a jump.
        + If $X$ has a mass at some point $x_0$ in the real line -- meaning that
          the $\mathbf P_X(\{x_0\}) > 0$, there will be a jump in $F_X$ at $x_0$. 
        + We can't have that becasuse $F_X(x_0) - F_X(x_0 - \epsilon) \approx f_X(x_0)\epsilon$
        + For $\epsilon > 0$ small enough, mass at $x_0$ would imply the LHS is
          $\mathbf P\{x_0\}$ while the RHS should be zero
      + Optional comment: in fact every $F_X$ has an associated $f_X$ with
        respect to /some/ (generally non-uniform) measure. This is the
        consequence of a more general result called the /Radon-Nikodym theorem/.
    - Expectation of absolutely continuous RV: 
      \[ \mathbf E[g(X)] = \int_{\mathbf R} g(z) f_X(z) dz  \]
      + ``Law of the unconscious statistician''
    - Moment generating function
      \[ m_X(t) = \mathbf E\left[ e^{tX}\right]=\int_{\mathbf R} e^{tx} f_X(x)dx\]
      + The i-th moment of $X$ can be found by taking the $i-th$ derivative of
        $m_X(t)$ and evaluating it at zero.
        + For this to be meaningful, the MGF must be well defined in $(-\epsilon, \epsilon)$ for some $\epsilon$
        + Then for example $m_X'(t) = \mathbf E[X e^{tX}]$
    - *Second main result.* Let $X_1$ and $X_2$ be st 
      \[ m_{X_1}(t) = m_{X_2}(t) \]
      for all $t$. Then $F_{X_1} = F_{X_2}$.  
      + This essentially means that all information contained in $F_X$ is also
        contained in $m_X(t)$
    - Note: take the Taylor series of exponential around $0$ and take
      expectations,
      \[m_X(t) = \sum_{n=0}^\infty \frac{t^n \mathbf E(X^n)}{n!}\]
      + It is tempting to that knowledge of moments determines the distribution
        of $X$. This is not the case, however, because sometimes the series
        above doesn't converge even when all moments exist. 
        
    # Examples. 
    # 1) $\Omega = \{1,2,3\}, S=\{a,b,c\}$.

    #    What is the measurability requirement doing? Suppose we have
    #    $\sigma-\text{algebras}$ $\mathcal F=\{\emptyset, \{1\}, \{2,3\}, \Omega\}$ and $\mathcal S = 2^S$.
       
    #    Because neither $2$ nor $3$ show up separately in $\mathcal F$, observing
    #    a random variable $X:\Omega\to S$ should not allow us to distinguish them.

    #    For example, a random variable such as
    #    \[X(1) = a, X(2) = b, X(3)=c\]
    #    would allow us to distinguish $2$ and $3$! Indeed, if $2$ is observed, we
    #    know for sure that $\omega=2$, but $\{2\}$ isn't in $\mathcal F$.
       
    #    In a sense, the measurability requirement is imposing consistency in what
    #    we can learn about the underlying state based on observing an outcome.
    #    In the above example, measurability implies that $X(2) = X(3)$.
       
    # 2) Take $\Omega$ to be the $[0,1]$ interval with the uniform probability $\lambda$, ie, 
    #    \[ \lambda( [a,b] )  = b - a \]
    #    for all intervals $[a,b]$.  

*** Problem 4 is not as easy as it might seem
    
    Consider the proof, for example, that $F_X \to 1$ as $x\to\infty$. (The case
    of $x\to0$ is similar.)
    
    We know that: 
    1) $F(x) = \mathbf P\{\omega: X(\omega) \leq x \}$
    2) $\{\omega: X(\omega) \leq x\} \uparrow \Omega$
    3) $\mathbf P(\Omega) = 1$
       
    So it must be the case that $F(x) = P\{\omega: X(\omega) \leq x\} \uparrow \mathbf P(\Omega) = 1$,
    isn't that right? Well, *no*. While that reasoning is in some sense in the
    right direction, at the very least it's an incomplete argument for two reasons.
    
    - We haven't defined convergence of sets as in (2). Unless you can make that
      statement rigorous somehow, using it is not fair game. 
    - More importantly, when we took the statements together, we missed an
      important step: proving that (whatever the first arrow means)
      \[ A_x \uparrow \Omega \implies \mathbf P(A_x) \uparrow \mathbf P(\Omega) \]
    
    The second step above is essentially the point of the exercise. Hint for
    actually solving the problem:
    - Use the fact that 
      \[ \lim_{x\to\infty} F(x) = L\] 
      if, and only if $F(x_n) \to L$ for all increasing sequences $x_n \to \infty$
    - Show that for any probability measure, if $x_n \uparrow \infty$
      \[ \mathbf P\{ \omega: X(\omega) \leq x_n \} \to \mathbf P(\Omega) = 1 \] 
      
      You will need to use /countable/ additivity for this.
      
    For the right-continuity part, one useful way of checking your proof is to
    make sure you understand why your proof doesn't apply to the left limit. 
** Recitation 2
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation2.pdf
   :EXPORT_TITLE: Intro to Econometrics: Recitation 2
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_LATEX_HEADER: \input{auxfiles/header_beamer.tex}
   :END: 
*** Review Part
**** Review
     #+BEAMER: \framesubtitle{Random variables - \emph{univariate} case} 
     #+BEAMER: \begin{center} $(\Omega, \mathcal F, \mathbf P)$ \end{center}
     
     - $X:\Omega\to\mathbf R$
     - CDF:
       \[ F_X(x) = \mathbf P( \left\{\omega: X(\omega) \leq x\right\}) \]
       + Completely characterizes $\mathbf P\{X \in B\}$ for $B \subset \mathbf R$
     - Absolutely continuous: 
       \[F_X(x) = \int_{-\infty}^x f_X(x) dx\]
**** Review
     #+BEAMER: \framesubtitle{Random variables - \emph{multivariate} case} 
     #+BEAMER: \begin{center} $(\Omega, \mathcal F, \mathbf P)$ \end{center}
     
     - $X:\Omega\to\mathbf R^S$ where $X(\omega) = (X_1(\omega),\ldots, X_S(\omega))'$
     - CDF:
       \[ F_X(x_1, \ldots, x_S) = \mathbf P( \{\omega: X_1(\omega) \leq x_1, \ldots, X_S(\omega) \leq x_S  \}) \]
       + Completely characterizes $\mathbf P\{X \in B \}$ for $B\subset \mathbf R^S$
     - Absolutely continuous: 
       \[F_X(x_1, \ldots, x_S) = \int_{-\infty}^{x_1}\cdots\int_{-\infty}^{x_S} f_X(x_1, \ldots, x_S) dx_S \cdots dx_1\]
**** Review 
     #+BEAMER: \framesubtitle{Random variables - \emph{multivariate} case} 
     - <1-> Result: if $F:\mathbf R\to[0,1]$ is
       1. Increasing
       2. Right-continuous
       3. Satisfies $\lim_{x\to\infty} F(x) = 1 - \lim_{x\to-\infty} F(x) = 1$
       Then it is the CDF of some random variable $X:\Omega\to\mathbf R$
     - <2-> Can you think of (or prove?) an S-dimensional analog of the statement above?
**** Review 
     #+BEAMER: \framesubtitle{Random variables - \emph{multivariate} case} 
     - If $F:\mathbf R^2\to[0,1]$ is
       1. <1-> Increasing
       2. <1-> ``Continuous from above''
       3. <1-> Has the following limits:
          1. $\lim_{x_1 \to -\infty} F(x_1, x_2) = 0$ for all $x_2$
          2. $\lim_{x_2 \to -\infty} F(x_1, x_2) = 0$ for all $x_1$
          3. $\lim_{x_1 \to \infty} \lim_{x_2 \to \infty} F(x_1, x_2) = 1$
       4. <2-> Satisfies, for $x_1^* \geq x_1$ and $x_2^* \geq x_2$,
          \[ F(x_1^*, x_2^*) - F(x_1^*, x_2) - F(x_1, x_2^*) + F(x_1, x_2) \geq 0 \]
       Then $F$ is the CDF of a random variable $X:\Omega\to\mathbf R^2$
       
     (Durrett, sec 2.9)
**** Review
     #+BEAMER: \framesubtitle{Marginals} 
     
     - <1-> Marginal with respect to coordinate $s$, $F_s : \mathbf R \to [0,1]$
       \[ F_s(x) = \mathbf P(\left\{ \omega: X_s(\omega) \leq x \right\})  \] 
     - <2-> How do you obtain it?
     - <3-> Just take limits. Suppose $S=2$ and we want to recover first coordinate:
       \[ F_1(x_1) = \lim_{x_2 \to \infty}  F(x_1,x_2)  \]
       
       Proof? 
**** Review
     #+BEAMER: \framesubtitle{Marginals} 
     
     - How do you recover a marginal pdf? Suppose $X:\Omega\to\mathbf R^2$ has pdf $f(x_1,x_2)$:
       \[f_1(x_1) = \int_{-\infty}^\infty f(x_1, x_2) dx_2\]
     - Proof? 
**** Review
     #+BEAMER: \framesubtitle{Digression: marginals don't determine joints} 
     
     - A very useful counterexample: 
       - <1-> Let $X \sim N(0,1)$
       - <2-> Let $W$ be independent of $X$; 
         \[ \mathbf P(W = 1) = \mathbf P(W = -1) = \tfrac{1}{2}\]
       - <3-> Define $Y = WX$. Claim: $(X,Y)$ has normal marginals, but $(X,Y)$ is not jointly normal.
         \begin{align*}F_Y(y) = \mathbf P(WX \leq y) &= \frac{1}{2} \mathbf P(X \leq y) + \frac{1}{2} \mathbf P(-X\leq y) \\ 
           &= F_X(y)\end{align*}
         So marginals of $(X,Y)$ are the same
       - <4-> $(X,Y)$ is not multivariate normal. Why? 
       - <5-> $X+Y$ has a  mass at zero, with probability $\frac{1}{2}$!
**** Review
     #+BEAMER: \framesubtitle{Digression: marginals don't determine joints} 

     \centering \includegraphics[scale=0.4]{./codes/Notes_PS2_simunormal.pdf}    
**** Review
     #+BEAMER: \framesubtitle{Moments of multivariate RVs} 
     - Focus on the case when there is a pdf
     - <1-> ``Definition''
       \[  \mathbf Eg(X) =  \int_{\mathbf R^S} g(x) f_X(x)dx   \]
     - <2-> First moment: 
       \[ \mu_X =  \mathbf EX \]
     - <3-> Second moment: 
       \[ V(X) = \mathbf E \left[ (X - \mu_X)(X - \mu_X)' \right] \]
       #+BEAMER: \vspace{-0.3cm}
       - When is $V(X)$ finite?
     - <4-> Covariance btw X and Y: 
       \[ \cov(X,Y) = \mathbf E \left[ (X - \mu_X)(Y-\mu_Y)' \right] \]
**** Review
     #+BEAMER: \framesubtitle{Moment generating functions of multivariate RVs} 
     - <1-> MGF: 
       \[  m_X(\mathbf t) = \mathbf E\left[ e^{\mathbf t'X} \right] = \mathbf E\left[ e^{\sum_{i=1}^S t_i X_i} \right]  \]
     - <2-> Result: suppose  $X$ and $Y$ have a moment generating function, and 
       \[ m_X(\mathbf t) = m_Y(\mathbf t)\]
       for all $\mathbf t$. Then $F_X(\mathbf t) = F_Y(\mathbf t)$ for all $\mathbf t$.
     - <3-> Result (stronger):  suppose that, for all $\mathbf t \in \mathbf R^S$, $\alpha \in \mathbf R$, 
       \[ \mathbf P\{ \mathbf t'X \leq \alpha \} = \mathbf P\{ \mathbf t'Y \leq \alpha \} \]
       then $F_X(z) = F_Y(z)$ for all $z\in\mathbf R^S$
*** PSet
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections} 

     Let $(V, \langle\cdot,\cdot\rangle)$ be a vector space with an inner product. 
     - <2-> Orthogonal projection of $v$ into (closed) $W\subseteq V$:
       \[ v - \proj_W(v)\perp w \]
       for all $w\in W$
***** Projection in a Hilbert Space 
      :PROPERTIES: 
      :BEAMER_env: theorem
      :BEAMER_opt: shadow=true
      :BEAMER_act: 3
      :END:
      
      Let $W\subset V$ be a closed vector subspace of $V$. 

      For any $v \in V$, the distance minimization problem
      \[\min_{w\in W} \| v - w \|\]
      has a unique solution $w^* \in W$. Moreover, $w^* = \proj_W(v)$.
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections} 
     What if $W$ has a finite basis? 
     \[ W = \vsp \{w_1, \ldots, w_K\}\]
     - Orthogonal projection of $v$ into $W$ is 
      \[  \proj_W(v) = \sum_{i=1}^K \frac{\langle w_i, v\rangle}{\langle w_i, w_i\rangle} w_i  \]

     Using this result in the pset is fair game 
     
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections} 
     
     Space $V = \{ X:\Omega\to\mathbf R^S: \mathbf E\|X\|^2 < \infty \}$ is a Hilbert
     space with 
     \[ \langle X, Y\rangle = \mathbf E XY\]
      
     - <2-> Fix variables $X$, $Y$ in $V$ and consider the subspace
       \[ W = \{ Z: \Omega \to \mathbf R : Z = \alpha + \beta (X - \mu_X)\} \] 
       (Is there a finite basis for $W$?)
**** PS2: Projections, conditioning, linear predictors
     #+BEAMER: \framesubtitle{Projections}
     The problem
     \[  \min_{(\alpha, \beta)} \left[ Y - \alpha - \beta(X-\mu_X) \right]^2 \]

     is equivalent to some norm minimization problem involving $Y, X$ and $W$.

     What is it?
** Recitation 3
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation3.pdf
   :EXPORT_TITLE: Intro to Econometrics: Recitation 3
   :EXPORT_BEAMER_THEME: Boadilla
   :EXPORT_LATEX_CLASS_OPTIONS: [presentation, smaller]
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_beamer.tex}
   :END:
   
*** Outline
**** Outline
     - Review: 
       + Statistical model
         * Definition
         * Examples
         * Identification, sufficiency 
       + Statistical decision problem
         * Definition
         * Examples
*** Statistical model
**** Statistical model
     #+BEAMER: \framesubtitle{Definition}
     - <1-> Idea: formalize statements such as
       1. Let $\{h_1, \ldots, h_{10}\}$ denote the outcome of $10$ independent
          coin flips with probability $p$ of landing heads
       2. <2-> ``Let ${X_1, X_2, X_3}$ be iid uniform in $[0,\theta]$ where $\theta$ is an unknown positive real number''
       3. <3-> ``Let $\{Y_t\}_{t\in1,2,\ldots, T}$ be an AR(1) process with gaussian innovations''

     - <4-> *Claim.* All statements equivalent to: ``let $\mathbf X$ be
       a draw from some cdf $F:\mathbf R^S \to [0,1]$ where $F$ is taken from some restricted set of CDFs, 
         \[F \in \mathfrak F\text{ ''}\]
**** Statistical model
     #+BEAMER: \framesubtitle{Definition}
     - <1-> It's common to write 
       \[ \mathfrak F = \{ F_\theta \}_{\theta \in \Theta} \]
     - <2-> For example: 
       \[\mathfrak F = \left\{ F:\mathbf R\to\mathbf R | F\text{ is the cdf of }  U[a,b] \text{ for some }a\leq b\right\}\]
       #+BEAMER: \vspace{-0.5cm}
       - Does this  represent a statistical model?
     - <3-> We can define for $\theta = (a,b)$, 
       \[ F_{\theta} = \frac{t-a}{b-a} \mathbf 1_{[a,b]}(t) \]
     - <4->  With that indexing, 
       \[ \mathfrak F = \{F_{\theta} \}_{\theta \in \Theta}\]
       where $\Theta = \{(x,y) \in \mathbf R^2 : x \leq y\}$
**** Statistical model 
     #+BEAMER: \framesubtitle{Comment}
     - <1-> Why do we specify models with CDFs?
     - <2-> Reason: in Euclidean spaces, distribution of random variables is fully characterized by CDF
     - <3-> However, if all CDFs in your model are absolutely continuous, it's
       equivalent to specify a family of PDFs
     - <4-> In the course, we will do this interchangeably; if a model is
       specified in terms of PDFs, it's understood that we're considering only absolutely continuous distributions
     - <5-> We can also specify the model with more general probability distributions: 
       \[ \{P_\theta: \mathcal B(\mathfrak X) \to [0,1]\}_{\theta \in \Theta} \]
       where $\mathfrak X$ a possibly more general space (e.g., a space of bounded continuous functions) 
**** Statistical model
     #+BEAMER: \framesubtitle{Example 1: ten coin flips}
     - <1-> Single coin flip: 
       \[ F_p^1(x) = \begin{cases} 0 & \text{if } x < 0 \\ 1 - p & \text{if } x \in [0,1) \\ 1 & \text{otherwise} \end{cases}\]
     - <2-> Then the joint is  $F_p(h_1, h_2, \ldots, h_{10}) = F_p^1(h_1) \cdots F_p^1(h_{10})$
     - <3-> Model: 
       \[ \{F_p\}_{p \in [0,1]} \]
       + What is $\Theta$ ? 
**** Statistical model
     #+BEAMER: \framesubtitle{Example 2: Uniform $[0,\theta]$}
     - <1-> Three independent uniform $[0,\theta]$. We know that for a given $\theta$
       \[ F_\theta^2(t) = \frac{t}{\theta} \mathbf 1_{[0,\theta]}(t) \]
       is the cdf of $U[0,\theta]$ for non-negative $\theta$.
     - <2-> Thus joint is 
       \[ F_\theta(x_1, x_2, x_3) = F^2_\theta(x_1) F^2_\theta(x_2) F^2_\theta(x_3)\]
       and statistical model is \[ \{ F_\theta \}_{\theta \in (0,\infty)} \]
**** Statistical model
     #+BEAMER: \framesubtitle{Example 3: AR(1) with Gaussian innovations}
     - <1-> An ``AR(1) with Gaussian innovations'' means that 
       \[ Y_t - \mu = \rho (Y_{t-1} - \mu) + \epsilon_t \] 
       where $\epsilon_t$ are drawn iid $N(0, \sigma^2)$. 
       - <1-> Note: need to make assumption about $Y_0$. Assume fixed.
     - <2-> Equivalently,
       \[ Y_t | Y_{t-1}, \ldots, Y_1 \sim N(\mu + \rho(Y_{t-1} - \mu) , \sigma^2) \] 
     - <3-> How do you write the joint CDF? By what parameters will it be indexed?
**** Statistical model 
     #+BEAMER: \framesubtitle{Identification \& sufficiency}
     
     - <1-> Summary of previous discussion: a statistical model is a family of
       distributions, $\{F_\theta: \mathbf R^S \to [0,1]\}_{\theta\in\Theta}$.
     - <2-> If each $\theta \in \Theta$ induces a unique distribution, the model is called *identified*.
     
       + <3-> Mathematically: the model is identified iff for every $\theta \ne
         \theta'$,
         there exists $x \in \mathbf R^S$ such that $F_\theta(x) \ne F_{\theta'}(x)$
       + <4-> What if the model was specified in terms of PDFs? What about general probability distributions?
     - <5-> A /statistic/ is any function $T:\mathbf R^S \to \mathbf R^K$. We
       say that $T$ is *sufficient* if \[ \mathbf P_\theta( \cdot | T(\cdot)) \]
       does not depend on $\theta$. Intuitively, if you condition on $T(X)$, the
       full data become uninformative about $\theta$.
       
**** Statistical model 
     #+BEAMER: \framesubtitle{Identification \& sufficiency}
     - <1->  Example: let $X_1$ and $X_2$ be iid $N(\mu, 1)$.
       + <2-> Model here is $\{F_\mu\}_{\mu \in \mathbf R}$ where $F_\mu$ is cdf
         of independent joint normal with mean $(\mu,\mu)$ and identity variance matrix
     - <2->  Then $T(X_1, X_2) = X_1 + X_2$ is sufficient.
     - <3-> Before proof: note that crucially the data is 2 dimensional, but the sufficient statistic is 1d 
     - <4-> Now: 
       \[\begin{bmatrix} X_1 \\ X_2 \\ T(X_1, X_2) \end{bmatrix} \sim \mathcal N_3 \left(  \begin{bmatrix} \mu \\ \mu \\ 2\mu \end{bmatrix}, 
         \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 1 & 2 \end{bmatrix} \right) \]
     - <5-> To find conditional distribution of $X_1$ and $X_2$ given $T(X_1, X_2)$, use the BLP trick. 
**** Statistical model 
     #+BEAMER: \framesubtitle{Identification \& sufficiency}
     - <1-> Math: 
       \[ E[X_1 | X_1 + X_2] = E[X_2 | X_1 + X_2] = \frac{X_1  + X_2 }{2} \] 
       moreover, conditional variance also doesn't depend on $\mu$ 
       
*** Statistical decision problem
**** Statistical decision problem
     #+BEAMER: \framesubtitle{Definition}
     - Definition: statistical decision problem is 
        \[ (\Theta, A, \mathcal L, \{F_{\theta}\}_{\theta\in\Theta}) \]
       where 
       1. <2-> $\Theta$ is a parameter space
       2. <3-> $A$ is a space of actions
       3. <4-> $\mathcal L$ is a utility/loss function
       4. <5-> $\{F_\theta\}$ is a statistical model
          + Remember: this can be alternatively specified as $\{P_\theta\}_{\theta\in\Theta}$ or $\{f_\theta\}_{\theta\in\Theta}$
**** Statistical decision problem 
     #+BEAMER: \framesubtitle{Interpretation}
      + Statistician is supposed to decide something. Examples: 
        1. <2-> Pick the $\theta$ that she thinks generated the data
           \[  A  = \Theta \] 
        2. <3-> Given a split $\Theta = \Theta_0 \sqcup \Theta_1$, pick which of
           $\Theta_0$ or $\Theta_1$ is more likely to contain the parameter that generated data
           \[ A = \{0, 1\} \] 
        3. <4-> Pick a subset $C \subseteq \Theta$ where she thinks the true $\theta$ falls in
           \[ A = \text{reasonable subsets of }\Theta \] 
**** Statistical decision problem 
     #+BEAMER: \framesubtitle{Interpretation}
     + <1-> Model this as a sequential game. 
       - <2-> /First stage:/ Nature picks $\theta \in \Theta$. This is not observable by statistician
       - <3-> /Stage $1\frac{1}{2}$:/ Nature randomly draws $X \sim F_\theta$
       - <4-> /Second stage:/ Statistician chooses action $a$
     + <5-> At the terminal nodes, statistician gets the loss $\mathcal L(a, \theta)$
     + <6-> Let $\mathfrak X \subset \mathbf R^S$ denote the (common) support of $F_\theta$. A
       *strategy* for the statistician in this game is a function 
       \[ d : \mathfrak X \to A  \]
       I.e. a specification of an action for every possible decision node she faces
       
       This strategy is called a decision rule in the mathematical statistical jargon 
**** Statistical decision problem 
     #+BEAMER: \framesubtitle{Risk function} 
     - <1-> What sort of criterion should we use to rank decision rules?
     - <2-> We use the expected utility paradigm. For fixed $\theta$, we postulate that
       \[  d_1(\cdot) \precsim_\theta d_2(\cdot) \iff \mathbf E_\theta \left[  \mathcal L(d_1(X), \theta)   \right]
       \geq \mathbf E_\theta \left[  \mathcal L(d_2(X), \theta)   \right]\] 
       #+BEAMER: \vspace{-0.5cm}
       - With respect to what are we taking the expectation?
     - <3-> This expectation is called /risk/. Notation: 
       \[ R(d, \theta) := \mathbf E_\theta \left[ \mathcal L(d(X, \theta) \right] = \int_{\mathbf R^S} d(x, \theta) dF_\theta(x) \]
     - <4-> Analogy  with game theory: /dominated/ strategies
       - A decision rule that is not weakly dominated is called admissible
** Recitation 4
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation4.pdf
   :EXPORT_TITLE: Intro to Econometrics: Recitation 4
   :EXPORT_BEAMER_THEME: Boadilla
   :EXPORT_LATEX_CLASS_OPTIONS: [presentation, smaller, handout]
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_beamer.tex}
   :END:
*** Recitation 4
**** Roadmap for today
     - Review:
       1) Statistical problem
       2) Bayes rules, expected posterior loss
     - PS3

**** Review: Statistical Problem 
     #+ATTR_beamer: :overlay +-
     - Components of a decision problem:
       + Statistical model: $\{P_\theta\}_{\theta \in \Theta}$
       + Action space $\mathcal A$
       + Loss function $\mathcal L:\mathcal A\times \Theta \to \mathbf R$
       + Decision rules: $d:\mathfrak X\to\mathcal A$
     - Risk function: expected loss from decision $d$ when parameter is $\theta$: 
       \[R(d(\cdot), \theta) = \int_{\mathfrak X} \mathcal L(d(x), \theta) f_\theta(x)dx \]
     
***** Comments                                                     :noexport:
      - Note that $\mathcal L(a, \theta)$ ranks actions for fixed $\theta$
      - Note that $R(d(\cdot), \theta)$ ranks decision rules (functions) for fixed $\theta$
        \[ d_1(\cdot) \succsim d_2(\cdot) \iff R(d_1(\cdot), \theta) \leq R(d_2(\cdot), \theta) \]
      - In that sense, "risk" is just like a "generalized loss" where the
        "generalized action space" is the space of all strategies
**** Review: Admissibility      
     #+ATTR_beamer: :overlay +-
     - Decision rule $d_1$ is /dominated/ by $d_2$ iff, for all $\theta \in \Theta$,
       \[ d_1 \precsim_\theta d_2  \]
       and $d_1 \prec_{\theta_0} d_2$
       for at least one $\theta_0$
       + What does it mean for a rule to be /not dominated/ by another rule? 
     - A rule $d$ that is not dominated by any other rule is called /admissible/
       + Expand the definition of admissible 
     - It is generally hard to find admissible rules.
**** Review: priors, posteriors, etc...  
     #+ATTR_beamer: :overlay +-
     - Suppose model is $\{f_\theta\}_{\theta \in \Theta}$, i.e., data has a density for all possible parameters
     - Suppose also $\Theta \subseteq \mathbf R^k$, and pdf $\pi(\theta)$ summarizes some prior belief about $\theta$
       - With this, we're interpreting the parameter $\theta$ as a /random variable/
       - Before the prior was introduced, $\theta$ was merely an index
     - With this structure, we can define the induced joint density of data and parameters,
       \[  f(x, \theta; \pi) = f_\theta(x) \pi(\theta)  \]
       + Does this integrate to one?
**** Review: priors, posteriors, etc...
     #+ATTR_beamer: :overlay +-
     - Given induced joint density, 
       \[ f(x | \theta; \pi) = \frac{f_\theta(x) \pi(\theta)}{\pi(\theta)} = f_\theta(x)\]
       #+BEAMER: \vspace{-0.3cm}
     - What about the marginal of data?
       - Recover it by integrating $\theta$ out:
         \[ f(x; \pi)  = \int_{\theta \in \Theta} f_\theta(x) \pi(\theta) d\theta \]
       #+BEAMER: \vspace{-0.3cm}
     - Conditional density of parameter given data?
       \[f(\theta | x; \pi) = \frac{f(x,\theta; \pi)}{f(x;\pi)} = \frac{f_\theta(x)\pi(\theta)}{\int_{\theta\in\Theta} f_\theta(x)\pi(\theta)d\theta}\]
       #+BEAMER: \vspace{-0.3cm}
       - This is called /posterior density/ in Bayesian jargon
**** Review: Bayes rules
     #+ATTR_beamer: :overlay +-
     - Let's go back to the statistical decision problem
     - Let $d(\cdot)$ be a decision rule, and $\pi$ a prior density over $\Theta$
     - Bayes risk of $d(\cdot)$ given $\pi$ is
       \[\begin{aligned} r(d(\cdot), \pi) &= \int_\Theta  R(d(\cdot), \theta) \pi(\theta)d\theta \\
                                          &= \int_\Theta \int_{\mathfrak X} \mathcal L(d(x), \theta) f(x, \theta; \pi)  dx \,d\theta  \end{aligned}\]
     - A /Bayes decision rule/ $d^*$ is one that minimizes Bayes risk given a prior $\pi$. 
       \[ d^*_\pi(\cdot) = \arg\min_{d(\cdot)} r(d(\cdot), \pi) \]
     - Important feature: /under mild assumptions, Bayes rules are admissible/
     
**** Review: finding Bayes rules
     #+ATTR_beamer: :overlay +-
     - Rewrite the Bayes risk using Fubini's theorem
       \[\begin{aligned} r(d(\cdot), \pi) &= \int_{\mathfrak X} \left[ \int_\Theta  \mathcal L(d(x), \theta) f(\theta | x; \pi)  d\theta\right] f(x; \pi) dx\\
                           &=  \int_{\mathfrak X} \psi(d(x), x) f(x; \pi) dx \end{aligned}\]
       where
       \[ \psi(a, x) = \int_\Theta \mathcal L(a, \theta) f(\theta | x; \pi) d\theta \]
     - Let $d^*(x) = \arg\min_{a\in\mathcal A} \psi(a,x)$
       + Immediate consequence: for any decision rule $d(\cdot)$,
         \[ \psi(d^*(x), x) \leq \psi(d(x), x) \] 
         #+BEAMER: \vspace{-0.5cm}
       + Important: optimization in space $\mathcal A$ is easier than in  the space of all $d:\mathfrak X \to \mathcal A$!
       
** Recitation 5
   :PROPERTIES: 
   :EXPORT_FILE_NAME: notes/Recitation5.pdf
   :OPTIONS: toc:nil
   :EXPORT_TITLE: Intro to Econometrics: Recitation 5 
   :EXPORT_SUBTITLE: A quick introduction to vector calculus
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_basic.tex}
   :END:
   
*** Intro
    In these notes I try to introduce some notation regarding calculus with
    functions that map vectors into vectors. One reason why things get a bit
    messy is that when we write $x\in\mathbf R^N$ we don't distinguish between
    \[ \begin{bmatrix} x_1  & \cdots & x_N \end{bmatrix} \]
    and
    \[ \begin{bmatrix} x_1  \\ \vdots \\ x_N \end{bmatrix} \]
    but crucially, the operation $L(\mathbf x)$ where $L$ is a linear map is
    represented differently by means of matrix multiplication notation; in the
    top case, $L(x)$ corresponds to a matrix acting on $x$ ``on the right'',
    whereas in the bottom case, the matrix acts ``on the left''. Of course,
    there is an operation that takes us from the ``row world'' to the ``column
    world'': transposition.
    
    Since derivatives are in fact linear maps, losing track of which side the
    derivative matrix operates on can lead to dimension inferno. So here I
    provide a few examples that might shed light on how to deal with this. 
    
    
*** The meaning of a derivative
    It will be useful to recall how derivatives and linear maps are connected.
    Because these aren't notes in analysis, I won't be as general as I could,
    neither will I provide any proofs. For proofs and generalizations, check any
    undergraduate real analysis textbook. I also assume that you are familiar
    with linear maps and their connection with matrix operations.
    

    Now let's recall the definition of a derivative. 
    #+BEGIN_defi
    Let $F:\mathbf R^n\to\mathbf R^p$. The function $f$ is called differentiable
    at $x_0 \in \mathbf R^n$ if there exists a linear map $L:\mathbf R^n \to \mathbf R^p$ such that 
    \[  \lim_{h\to0} \frac{\|F(x_0+h) - F(x_0) - L(h)\|}{\|h\|} = 0    \] 
    
    we will denote $L = DF(x_0)$. The linear map $DF(x_0)$ is called the /derivative/ of $F$.
    #+END_defi
    
    An important thing to note is that $DF(x_0)$ is a linear map, so it applies
    to vectors in $\mathbf R^n$. This leaves us with the awkward notation 
    \[ L(h) = DF(x_0)(h) \]
    which becomes (maybe?) a bit less ambiguous by adding even more parentheses: 
    \[ L(h) = (DF(x_0))(h) \] 
    
    It is sometimes useful to divide $\mathbf R^n$ into two sets of coordinates,
    say $\mathbf R^n = \mathbf R^{n_1} \times \mathbf R^{n_2}$, so we study
    functions like $F(x,y)$. The partial derivative with respect to the first
    set of $n_1$ coordinates, evaluated at $(x_0, y_0)$, is denoted
    $D_1 F(x_0,y_0)$. It just means the derivative of the map 
    \[ x \mapsto F(x, y_0) \]
    evaluated at $x_0$ (whenever it exists). Whenever we consistently
    refer to the first set of coordinates as $x$, we can also write 
     \[ D_x F(x_0, y_0) \]
    to denote the same partial derivative.
    
    One important caveat is that both $D_x F(x_0, y_0)$ and $D_y F(x_0, y_0)$
    might be defined at a point where $F$ is not differentiable.
    
    
**** Facts about derivatives to have in mind
     I state a proposition that summarizes all that I will use about
     derivatives. As mentioned earlier, I don't give any proofs but they should
     be contained in any basic real analysis textbook.
     #+BEGIN_thm
     Let $F_1:\mathbf R^n\to\mathbf R^p$ and $F_2:\mathbf R^n\to\mathbf R^P$ be
     differentiable at $x_0 \in \mathbf R^n$, and let $G:\mathbf R^k\to\mathbf R^n$ be
     differentiable $z_0$, where $x_0 = G(z_0)$.
     Then:
     1. $F(x) = F_1(x) + F_2(x)$ is differentiable at $x_0$ and $DF(x_0) = DF_1(x_0) + DF_2(x_0)$
     2. $H(z) = F_1(G(z))$ is differentiable at $z_0$ and $DH(z_0) = DF_1(x_0)\circ DG(z_0)$
     #+END_thm

     [TODO: More in-depth about meaning of the two items. Especially the composition
     above, and the fact that $DH(z_0)$ is a linear map.]
     
     #+BEGIN_thm
     Let $g_1:\mathbf R^m\to\mathbf R^{n_1}$ and $g_2:\mathbf R^m\to\mathbf R^{n_2}$
     both be differentiable at $t_0 \in \mathbf R^m$. Let
     $F:\mathbf R^{n_1} \times \mathbf R^{n_2} \to \mathbf R^p$ be differentiable at
     $(x_0, y_0) = (g_1(t_0), g_2(t_0))$.

     Then 
     \[ \phi(t) = F(g_1(t), g_2(t)) \]
     is differentiable at $t_0$, and 
     \[ D\phi(t_0) = D_x F(x_0, y_0)\circ Dg_1(t_0) +  D_y F(x_0, y_0) \circ Dg_2(t_0) \]
     #+END_thm
     
*** Seven examples  
    TODO: finish
    
    1. Take $f_1(x) = Ax$. What is the derivative of $f_1$? Take $L(h) = Ah$.
       \[ f_1(x+h) - f_1(x) - L(h) \equiv 0\]
       hence $Df_1(x) = A$ for all $x$.
       
       Importantly, we specified the action above but it's good to repeat it: $(Df_1(x))(h) = Ah$. 
       
       That is, the derivative is $A$ and the action is on the left.
    2. Let $f_2(x) = x'B$. Take $T(h) = h' B$. 
    
** Recitation 6
   :PROPERTIES:
   :EXPORT_FILE_NAME: notes/Recitation6.pdf
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_TITLE: Intro to Econometrics: Recitation 6
   :EXPORT_SUBTITLE: Hansen chapters 1-5
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_beamer.tex}
   :END:

*** Intro
**** Roadmap
     Hansen chapters 2-5 overview
     * Chapter 2
       - Projection
       - Conditional expectation
       - Best linear predictor and linear regressions

*** Chapters 1-5
**** Chapter 2
     #+BEAMER: \framesubtitle{Projection}

     - <1-> Take $Y$ scalar rv and $\bfX = (X_1, \ldots, X_n)$. Consider following spaces:
       \[\mathcal L(\bfX) =  \{\hat Y : \hat Y = \sum_{i=1}^n  \beta_i X_i \} \]
       \[\mathcal E(\bfX) =  \{\hat Y : \hat Y = f(\bfX) \} \]
       #+BEAMER:\vspace{-0.4cm}
     - <2-> Let's restrict our analysis to variables $Y, X$ such that $E[|Y|^2] <\infty$ and $E[\|\bfX\|^2] < \infty$. Moreover,
       assume $\mathbf E[\bfX\bfX'] > 0$
     - <3-> That way the inner product $\langle X, Y \rangle := \mathbf E(YX)$ is well defined

**** Chapter 2
     #+BEAMER: \framesubtitle{Projection}
     - <1-> With that inner product: what's projection of $Y$ (scalar valued) on $\mathcal L(\bfX)$?
     - <2-> Orthogonality condition: $\langle Y -  \bfX'\beta^*, \bfX'\beta\rangle = 0$ for all $\beta$
     - <3-> Implies:
       1. $\beta^* = \mathbf E[\bfX \bfX']^{-1} \mathbf E[\bfX' Y]$
       2. Error term associated with projection is uncorrelated with $\mathbf X$. Let $u^* = Y - \bfX'\beta^*$
          \[ \langle u^*, \bfX\rangle = \mathbf E[u^* \bfX] = 0\]

**** Chapter 2
     #+BEAMER: \framesubtitle{Projection}
     - <1-> How about the projection of $Y$ on $\mathcal E(\bfX) =  \{\hat Y : \hat Y = f(\bfX) \}$
     - <2-> It's a /function/ $f^*(\bfX)$ such that for any function $f$,
       \[  \langle Y - f^*(\bfX) , f(\bfX) \rangle = 0\]
       #+BEAMER: \vspace{-0.4cm}
     - <3-> The function
       \[ f^*(\bfX) = \mathbf E[ Y | \bfX ] \]
       satisfies the orthogonality conditions. (Check!)
     - <4-> Residual $u^*$ satisfies exogeneity $\mathbf E[u^* | X]= 0$

**** Chapter 2
     #+BEAMER: \framesubtitle{Projection}
     - Take away:
       1. <1-> For any variables $(Y, \bfX)$, you can /always/ find $\beta^*$ such that
          \[ Y = \bfX\beta^* + u^* \]
          and $E[u^* \bfX] = 0$
       2. <2-> You can always write
          \[ Y = f^*(\bfX) + u^* \]
          where $\mathbf E[u^*|X] = 0$
**** Chapter 2
     #+BEAMER: \framesubtitle{Projection}
     - <1-> Wait a second: /how about all the resources people spend trying to argue for `exogeneity'/?
       - <2-> The point is /exactly/ that in empirical applications, estimating
         \[ y_i  = x_i'\beta + u_i\]
         will give you $\beta^*$ in the limit
       - <3-> Sometimes $\beta^*$ is not the object of interest, but
         \[ y_i  = x_i' \tilde \beta + v_i\]
         where $\mathbf E[x_i v_i] \ne 0$
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - <1-> Data $(y_i, \bfx_i)$, $i=1,\ldots, n$ identically distributed from some joint distribution $F$
     - <2-> Least squares problem:
       \[ \min_\beta \sum_{i=1}^n (y_i - \bfx_i' \beta)^2 \]
     - <3-> In matrix notation:
       \[ \min_{\beta} \|\bfy - \bfX\beta\|^2 \]
     - <4-> Solution: $\hat\beta = (\bfX'\bfX)^{-1}\bfX'\bfy$
     - <5-> Orthogonality condition: $\bfX' [\bfy - \bfX \hat\beta] = 0$
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - Notation:
       \[ \begin{aligned}
       \bfQxxhat &=\frac{1}{n} \bfX'\bfX = \sum_{i=1}^n \bfx_i x_i' \\
       \bfQxyhat &=\frac{1}{n} \bfX'\bfy = \sum_{i=1}^n \bfx_i y_i \\
       \bfP &= \bfX (\bfX'\bfX)^{-1} \bfX' \\
       \bfM &= \idd_n - \bfP = \idd_n - \bfX (\bfX ' \bfX)^{-1} \bfX'
       \end{aligned}\]
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - Note:
       \[\begin{aligned} \bfy &= \bfX\hat\beta + \overbrace{(\bfy - \bfX\hat\beta)}^{\text{LS residuals}} \\
       &= \bfX (\bfX'\bfX)^{-1} \bfX'\bfy + \left[\bfy - \bfX(\bfX'\bfX)^{-1} \bfy \right] \\
       &= \bfP \bfy + \bfM \bfy\end{aligned} \]
     - Hence $\bfP \bfy$ is the predicted part and $\bfM \bfy$ is the residual
     - Matrices $\bfP$ and $\bfM$ are both /symmetric/, and satisfiy:
       \[\begin{aligned} \bfP \bfP &= \bfP \\
       \bfM \bfM &= \bfM \\
       \bfP \bfM &= \bfM \bfP = \mathbf 0 \\
       \bfP \bfX &= \bfX  \\
       \bfM \bfX &= \mathbf 0 \end{aligned}\]
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - Let's apply this machinery. Two components:
       \[ y_i = \bfx_{1i}' \beta_1 + \bfx_{2i}'\beta_2 + u_i \]
     - <2-> In matrix notation:
       \[   \bfy = \bfX_1 \beta_1 + \bfX_2 \beta_2 + \bfu  \]
     - <3-> At the least squares solution,
       \[ \bfy = \bfX_1 \hat\beta_1 + \bfX_2 \hat\beta_2 + \bfe \]
       where
       \[ \mathbf 0 = \bfX' \bfe = \begin{bmatrix} \bfX_1' \\ \bfX_2' \end{bmatrix} \bfe  \]
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - Define $\bfP_j, \bfM_j$ for $j=1,2$ accordingly
     - Suppose we want to find expression for $\hat\beta_1$. Can get rid of $\bfX_2$ by multiplying $\bfM_2$!
       \[ \bfM_2 \bfy = \bfM_2 \bfX_1\hat\beta_1 + \bfM_2 \bfX_2 \hat\beta_2 + \bfM_2 \bfe  \]
     - Note: $\bfM_2 \bfe = \bfe - \bfX_2(\bfX_2'\bfX_2)^{-1} \bfX_2' \bfe = \bfe$
     - Hence
       \[ \bfM_2 \bfy = \bfM_2 \bfX_1 \hat\beta_1 + \bfe \]
**** Chapter 3
     #+BEAMER: \framesubtitle{Least Squares Algebra}
     - <1-> Denote $\tilde \bfy = \bfM_2 \bfy$ and $\tilde \bfX_1 = \bfM_2 \bfX_1$
     - <2-> We have
       \[ \tilde \bfy = \tilde \bfX_1 \hat\beta_1 + \bfe \]
       Moreover,
       \[\tilde \bfX_1' \bfe = \bfX_1' \bfM_2 \bfe = \mathbf 0\]
       Thus (as long as $\tilde X_2$ is full row rank):
       \[\hat\beta_1 = (\tilde \bfX_1'\tilde \bfX_1)^{-1} \tilde \bfX_1' \bfy = (\bfX_1' \bfM_2 \bfX_1)^{-1} \bfX_1'\bfM_2 \bfy \]
     - <3-> Interpretation? Frisch-Waugh-Lovell
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: statistical models}
     - <1-> Data $(y_i, \bfx_i)$ independently drawn from $F(y, \bfx)$
     - <2-> Statistical model will put further restrictions on $F$.
     - <3-> Note: not assuming deterministic $\mathbf x_i$ anymore. Analysis will strongly rely on conditioning
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: statistical models}
     - <1-> Depending on what properties you want to get for OLS, different assumptions are required
       1. <2-> *Linear regression model.*
          - $\mathbf E[y_i | \bfx_i] = \bfx_i' \beta$
          - Finite second moments, and $\mathbf E[\bfx_i \bfx_i']$ invertible
     - <3-> With the above assumption, we get an unbiased OLS estimator.
     - <4-> What about `optimality' in any sense? Need restriction on second moments.
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: statistical models}
     - Another assumption:
       2. [@2] *Homoskedasticity.* In addition to linear regression hypohtesis,
          \[ \mathbf V[y_i| \bfx_i] \equiv \sigma^2 \]

     - Then we get the Gauss-Markov result.
***** Gauss-Markov
      :PROPERTIES:
      :BEAMER_env: theorem
      :BEAMER_opt: shadow=true
      :END:
      In the homoskedastic linear regression model, $\hat\beta$ is the best
      linear unbiased estimator of $\beta$ (with $L^2$ loss).

      That means that any other unbiased estimator $\tilde \beta = \tilde A\bf y$ satisfies
      \[ \mathbf V[\tilde \beta | \bfX ] \geq \mathbf V[\hat\beta | \bfX]\]
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: statistical models}
     - When homoskedasticity is not assumed, we can sometimes do better than OLS
     - For example, if we abandon the iid assumption, and instead only impose finite second moments and
       \[  \mathbf E[\bfy | \mathbf X] = \mathbf X\beta  \]
       \[  \mathbf V[\bfy | \mathbf X] = \Omega  \]
     - If $\Omega$ is known, the /Generalized Least Squares estimator/ is the way to go.
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: statistical models}
     - Another important case that we frequently find in applied work:
       \[  \mathbf V[y_i | \bfx_i] = \varsigma(x_i)^2 = \sigma_i^2 \]
     - With the above form for residual variace, we have a /heteroskedastic linear regression model/
**** Chapter 4
     #+BEAMER: \framesubtitle{Least squares: variance estimation}
     - TBI

** Recitation 7
** Recitation 8
   :PROPERTIES:
   :EXPORT_FILE_NAME: notes/Recitation8.pdf
   :EXPORT_TITLE: Intro to Econometrics: Recitation 8
   :EXPORT_BEAMER_THEME: Boadilla
   :EXPORT_LATEX_CLASS_OPTIONS: [presentation, smaller]
   :EXPORT_OPTIONS: toc:nil H:2
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_beamer.tex}
   :END:
*** Intro
**** Roadmap for today
     - Chapter 6 quick review:
       + Asymptotic order notation
       + The basic toolkit: CMT, delta method and limit theorems
     - BH Chapter 7:
       + Asymptotic properties in the LP/LR model
*** Asymptotic order notation
**** Asymptotic order notation
     #+BEAMER: \framesubtitle{Deterministic sequences}
     - <1-> Consider the claim:
       #+BEAMER: \vspace{0.3cm}

       #+begin_quote
       ``The sequence $a_n=n^2$ diverges to infinity an order of magnitude faster than $b_n=n$''
       #+end_quote
       - <1-> How do we make this rigorous?
     - <2-> ``$|a_n - b_n|$ gets large'' is not a good idea; e.g. $a_n = 5n$, $b_n=n$
     - <3-> Let $r_n = \frac{a_n}{b_n}$:
       + <4-> $a_n \in O(b_n) \iff |r_n|$ is bounded
       + <5-> $a_n \in o(b_n) \iff |r_n| \to 0$
     - <6-> Note: we're assuming for simplicity that $b_n > 0$

**** Asymptotic order notation
     #+BEAMER: \framesubtitle{Deterministic sequences: examples}
     - <1-> Convince yourself (or even better, give a proof) that:

       1. $n \in o(n^2)$

       2. $log(n) \in O(n)$

       3. $sin(n) \in O(1)$

     - <2-> Also, for arbitrary (positive) $x_n$ and $y_n$,
       1. <3-> $x_n \in O(x_n)$ but the same doesn't hold for $o(x_n)$
       2. <4-> If $x_n \in O(An^2 + Bn + C)$, then $x_n \in O(n^2)$; moreover, $x_n \in o(n^3)$
          - Similar fact for higher degree polynomials goes through
       3. <5-> $x_n \in O(y_n)$ iff $x_n/y_n \in o(1)$; same for $O$
       4. <6-> If $x_n \to x$, then $x_n \in O(1)$

**** Asymptotic order notation
     #+BEAMER: \framesubtitle{Deterministic sequences: results}
***** Lemma                                                         :B_block:
      :PROPERTIES:
      :BEAMER_env: block
      :END:
      For any positive sequences $x_n$, $y_n$, $z_n$ and $w_n$,
      1. If $x_n \in o(z_n)$ and $y_n \in o(w_n)$, then
         \[ x_n y_n \in o(z_n w_n) \]
         \[ x_n + y_n \in o(z_n + w_n) \]
         and the same holds for $O$
      2. If $x_n \in o(z_n)$ and $y_n \in O(w_n)$, then
         \[ x_n y_n \in o(z_n w_n) \]
         \[ x_n + y_n \in O(z_n + w_n) \]
**** Asymptotic order notation
     #+BEAMER: \framesubtitle{Deterministic sequences: results}
     - Proof:
       + First part of (1) follows from limit arithmetic
       + First part of (2) is a consequence of the fact that $a_n \to 0$ and
         $b_n$ bounded implies $a_n b_n \to 0$
       + Second part of (1) and (2): use
         \[ \frac{x_n + y_n}{z_n + w_n} = \frac{x_n}{z_n} \frac{z_n}{z_n + w_n} + \frac{y_n}{w_n} \frac{w_n}{z_n + w_n} \]

         the ratio $z_n / (z_n + w_n) \in [0,1]$ is bounded
**** Asymptotic order notation
     #+BEAMER: \framesubtitle{Sequences of random variables}

     - <1-> Analogs: let $X_n$ be a sequence of random variables and $a_n$ a positive deterministic sequence
       - $X_n \in O_p(a_n)$ iff $\dfrac{X_n}{a_n}$ is /stochastically/ bounded
       - $X_n \in o_p(a_n)$ iff $\dfrac{X_n}{a_n} \probto 0$
     - <2-> Definition of stochastically bounded.

       For every $\delta>0$ there exists $M_\delta$ such that
       \[ \Pr\{|X_n| \leq M_\delta \}  > 1-\delta \]
     - <3-> Result: lemma goes through with $o$ and $O$ replaced with $o_p$ and $O_p$
**** Asymptotic order notation
     #+BEAMER: \framesubtitle{Sequences of random variables: an useful result}
     A sequence $(X_n)$ that converges in distribution is necessarily $O_p(1)$
     - <2-> _Proof_. Let $Y$ be st $X_n \distto Y$. Denote $F_Y$ and $F_n$ the CDF of
       $Y$ and $X_n$, respectively. Fix $\delta > 0$.
       - Let $M_\delta$ be such that
         \[ F_Y(M_\delta) - F_Y(-M_\delta) = \Pr\{Y \in (-M_\delta, M_\delta]\} > 1 - \frac{\delta}{2} \]
         #+BEAMER: \vspace{-0.4cm}
       - In addition, we can choose $M_\delta$ st $F_Y$ continuous at $M_\delta$ and $-M_\delta$ (why?)
     - <3-> Convergence in distribution implies $F_n(M_\delta) \to F_Y(M_\delta)$ and $F_n(-M_\delta) \to F_Y(-M_\delta)$
     - <4-> Hence for $n_0$ sufficiently large, $F_n(M_\delta) > F_Y(M_\delta) - \delta/4$ and
       $F_n(-M_\delta) < F_Y(-M_\delta) + \delta/4$ for all $n \geq n_0$
     - <5-> That implies
       \[ \Pr\{ | X_n | \leq M_\delta \} \geq F_n(M_\delta) - F_n(-M_\delta) > 1 - \delta\]
*** Asymptotics toolkit
**** The asymptotics toolkit
     - You need to know the following by heart:
       1. General results
          - Continuous mapping  theorem
          - Slutzky's theorem
          - Delta method
       2. Results about averages
          - Law of large numbers
          - Central Limit Theorem
**** Continuous mapping theorem
     - Let $F:U\to V$ have a set of discontinuities $U_0$
     - If $X_n \distto X$ or $X_n \probto X$, then $F(X_n) \to F(X)$ as long as
       \[ \Pr \{ X \in U_0 \} = 0\]
**** Slutzky's theorem
     - If $X_n \distto X$ and $Y_n \probto c$ where $c$ is non-random, then
       1. $X_n + Y_n \distto X+c$
       2. $X_n Y_n \distto cX$
       3. $X_n/Y_n \distto X/c$ as long as $c \ne 0$
     - These results follow from CMT, plus the fact that
       \[ (X_n, Y_n) \distto (X,c)\]
       provided that $c$ is non-random.
**** Delta method
     - If $\mu$ is non-random,
       \[ \sqrt{n}(X_n - \mu) \distto Z \]
       and $g$ is differentiable at $\mu$, then
       \[ \sqrt{n} (g(X_n) - g(\mu)) \distto g'(\mu) Z \]
     - Remarks.
       1. No assumption about normality is needed for $Z$.
       2. The theorem goes through in the multivariate case
          - Example: $F(\gamma_1, \gamma_2)=\gamma_1\gamma_2$
**** Results about averages
     - <1-> Let $\{X_i\}$ be sampled iid from some distribution. Denote
       \[ \bar X_n = \frac{1}{n} \sum_{i=1}^n X_i \]
     - <2-> _Law of large numbers._ If $\{X_i\}$ are sampled iid from distribution with finite mean, then
       \[  \bar X_n \probto \expval X_1 \]
     - <3-> _Central limit theorem._ If $\{X_i\}$ are sampled iid from distribution
       with finite mean $\mu$ and variance $\sigma^2$,
       \[ \sqrt{n} \left( \bar X_n - \mu \right) \distto Z \sim N(0, \sigma^2) \]
**** Results about averages
     - *Remark.* If $X_i$ have finite mean and variance, the central limit
       theorem gives a bound on the rate of convergence of $\bar X_n$
     - <2-> In fact:
       \[ \bar X_n - \mu = \frac{1}{\sqrt{n}} \left[  \sqrt{n} (\bar X_n - \mu ) \right] = O_p(n^{-1/2}) O_p(1) = O_p(n^{-1/2}) \]

     - <3-> Hence (with finite second moments) the convergence of $\bar X_n$ to
       $\mu$ is no slower than $n^{-1/2}$ goes to $0$.
*** OLS asymptotics
**** OLS asymptotics: the linear projection model
     - Consider the following assumptions:
       - (1) :: $(y_i, x_i)$ sampled iid
       - (2) :: $\expval(y_i^2) < \infty$
       - (2') :: $\expval(y_i^4) < \infty$
       - (3) :: $\expval(\|x_i\|^2) < \infty$
       - (3') :: $\expval(\|x_i\|^4) < \infty$
       - (4) :: $\expval[x_i x_i']$ positive definite
     - Notation:
       \[ \expval[x_i x_i' ] =: Q_{xx}  \]
       \[ \expval[x_i y_i ] =: Q_{xy}  \]
**** OLS asymptotics: the linear projection model
     - Results:
       1. Under (1), (2), (3) and (4), OLS is consistent for the population projection coefficient:
          \[ \hat\beta = \left(\sum_{i=1}^n x_ix_i' \right)^{-1} \sum_{i=1}^n x_iy_i \probto  \beta := Q_{xx}^{-1} Q_{xy} \]
          #+BEAMER: \vspace{-0.4cm}
       2. Under (1), (2'), (3') and (4), OLS is asymptotically normal:
          \[ \sqrt{n}\left[ \left(\sum_{i=1}^n x_ix_i' \right)^{-1} \sum_{i=1}^n x_iy_i - \beta\right] \distto N\left( 0, Q_{xx}^{-1} \Omega Q_{xx}^{-1} \right)\]
          - $\beta$ is again the population projection coefficient
          - $\Omega = \expval[u_i^2 x_ix_i']$
          - $u_i := y_i - x_i' \beta$
     - Notation:
       \[ V_\beta = Q_{xx}^{-1} \Omega Q_{xx}^{-1} \]
**** OLS asymptotics: the linear regression model
     - <1-> Consider the following assumptions:
       - (1) :: <1-> $(y_i, x_i)$ sampled iid
       - (2) :: <1,3> $\expval(y_i^2) < \infty$
       - (2') :: <2-> $\expval(y_i^4) < \infty$
       - (3) :: <1,3> $\expval(\|x_i\|^2) < \infty$
       - (3') :: <2-> $\expval(\|x_i\|^4) < \infty$
       - (4) :: <1-> $\expval[x_i x_i']$ positive definite
       - (5) :: <1-> $\expval(y_i | x_i) = x_i' \beta$
       - (6) :: <3-> $\mathbf V\left[ y_i | x_i\right] \equiv \sigma^2$
     - <1-> Assumptions (1), (2), (3), (4) and (5) are called the linear regression model
     - <2-> Again (2') and (3') are required for asymptotic normality
     - <3-> If (6) is assumed, we have the *homoskedastic* linear regression model
**** OLS asymptotics: the linear regression model
     - <1-> Finite sample variance of OLS:
       \[ \hat\beta = (X'X)^{-1} X'Y \implies \mathbf V(\hat\beta | X) = (X'X)^{-1} X' \mathbf V(Y|X) X (X'X)^{-1}  \]
     - <2-> $\mathbf V(Y|X)$ is a diagonal matrix. Why?
     - <3-> Denote $D = \mathbf V(Y|X)$. Hansen defines:
       \[ V_{\hat\beta} = (X'X)^{-1} X'DX (X'X)^{-1} \]
     - <4-> This is *not* the same thing as
       \[ V_\beta = Q_{xx}^{-1} \expval[u_i^2 x_ix_i'] Q_{xx}^{-1}\]

     - <5-> It is true however that
       \[ n V_{\hat\beta} \probto V_\beta\]
       (why?)
**** OLS asymptotics: estimating the limiting variance
     - We know that the limiting variance of OLS in the LPM is
       \[  V_\beta = Q_{xx}^{-1} \Omega Q_{xx}^{-1} \]
     - A valid estimator for $\Omega = \expval[u_i^2 x_i x_i']$ is
       \[ \hat \Omega =\frac{1}{n} \sum_{i=1}^n \hat u_i^2 x_i x_i' \]
     - One estimator for $V_\beta$ is
       \[ \hat{V}_{\beta}^{\text{HC0}}  = \hat{Q}_{xx}^{-1} \hat\Omega \hat{Q}_{xx}^{-1} \]
**** Prediction: regression intervals
     - One object of interest in the LRM is
       \[ m(x) = \expval[y_i | x] = x'\beta \]
       #+BEAMER: \vspace{-0.4cm}
     - <2-> How does one estimate this function? Reasonable candidate is:
       \[ \hat m(x)  = x' \hat\beta \]
       #+BEAMER: \vspace{-0.7cm}
     - <3-> Note that for fixed $x$,
       \[ \sqrt{n} (\hat m(x) - m(x)) = x' \sqrt{n} (\hat\beta - \beta)  \distto N(0, x'V_\beta x) \]
       so that
       \[ \left( x' \hat V_\beta x \right)^{-1/2} \sqrt{n} [\hat m(x) - m(x)] \distto N(0,1)\]
     - <4-> That motivates the approximation
       \[  \hat m(x) \approx N\left(m(x), \frac{1}{n} x' \hat V_\beta x\right) \]
**** Prediction: forecast errors
     - <1-> Suppose you expect to obtain one additional data point: $(y_{n+1}, x_{n+1})$
     - <2-> How would you predict $y_{n+1}$? Best predictor (under quadratic loss):
       \[ y_{n+1}^* = \expval [y_i | x_{n+1}] = m(x_{n+1}) = x_{n+1}'\beta\]
     - <3-> The above is not feasible; use plug-in estimate $\hat m(x)$
       \[\tilde y_{n+1} =  x_{n+1}' \hat\beta \]
     - <4-> Forecast error is
       \[ \tilde e_{n+1} = y_{n+1} - x' \hat\beta = x' \beta + e_i - x' \hat\beta  = e_{n+1} + x' (\beta - \hat\beta) \]
       - The true (population) regression has an error $e_{n+1}$
       - In addition to that, there is an estimation noise coming from $\hat\beta$ being different than $\beta$
**** Prediction: forecast errors
     - Finite sample variance of $e_{n+1}$:
       \[ \expval [\hat e_{n+1}^2 | x_{n+1} ] = \sigma^2(x_{n+1}) + x_{n+1}' V_{\hat\beta} x_{n+1} \]
     - Can we build a confidence interval for the forecast?
* Other notes
** Lectures 1 & 2
   :PROPERTIES: 
   :EXPORT_FILE_NAME: other_notes/N01_Finite_additivity_comment.pdf
   :EXPORT_TITLE: Comments 
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_basic.tex}
   :EXPORT_OPTIONS: toc:nil
   :END: 
*** Finite additivity
      Let's define some notation. I can define the following for any indexed collection of sets $A_i$:
      \[A_1 + A_2 := A_1 \cup  A_2\]
      or, more generally
      \[
      \sum_i A_i := \bigcup_i A_i
      \]
      whenever the collection $A_i$ is pairwise disjoint.

      The idea of assuming additivity -- without any further qualification --
      is that set-function $\mathbf P$ satisfies some form of linearity, that is
      \[
      \mathbf P\left(   \sum A_i  \right) = \sum_i \mathbf P \left(  A_i \right)
      \]
      It turns out that the set of indices over which this assumption is made is
      consequential.

      We call $\mathbf P$ /finitely additive/ if the above is required to hold
      for all finite sets of indices. Similarly, if the relationship holds for
      countably many indices, $\mathbf P$ is called /countably additive/.
      
      Let's investigate an example of finitely, but not countably, additive
      measure. Here, we are working with a triple $(X, \mathcal A, \mathbf P)$.
      $\mathcal A$ is an /algebra/ of sets. Very similar to the usual
      $\sigma-\text{algebra}$ couterpart, but we don't require the assumptions
      of closedness under unions and intersections to hold for infinitely many
      set, only finitely many.

      We will work with the following algebra, which is not a
      $\sigma\text{-algebra}$. Let $X$ be the set of all natural numbers,
      $\mathbf N$. Define also 
      \[
      \mathcal A = \left\{ A \subset \mathbf N: A\text{ is finite or } A^c \text{ is finite} \right\}
      \]
      
      Example of sets in $\mathcal A$: $\{1, 2, 3\}$ and $\{5001,
      5002,\ldots\}$. Example of a sets /not/ in $\mathcal A$: the set of all
      odd/even/prime numbers.[fn:1] 
      
      It's not hard to see that this is satisfies: $\emptyset \in \mathcal A$
      (since $\emptyset$ is finite) and closedness under intersections/unions.
      The reason why $\mathcal A$ is not a $\sigma\text{-algebra}$ is that each
      $A_i = \{1, 3, \ldots, 2i + 1\}$ is in $\mathcal A$, but its infinite
      union, the set of all odd numbers, is not.
      
      Now consider the probability measure: $\mathbf P:\mathcal A \to [0,1]$: 
      \[ \mathbf P(A) = 
      \begin{cases} 1 &\text{if } A\text{ is infinite}  \\ 0 &\text{ otherwise} \end{cases} \]
      Thus, for example, $\mathbf P({1,2,3}) = 0$ and $P(\{1023, 1024, \ldots\}) = 1$.
      
      Such $\mathbf P$ trivially satisfies $\mathbf P(A + A') = \mathbf P(A) + \mathbf P(A')$ because
      the finite union of finite sets is finite.
      
      This probability measure is interesting because it provides a
      counter-example to continuity when $\mathbf P$ is only finitely, but not
      countably, additive.
      
      For example, it holds that $\{1,2,\ldots, n\} \uparrow \mathbf N$, but 
       \[\begin{aligned} 1 = \mathbf P(\mathbf N) &= \mathbf P\left( \bigcup_n \left\{ 1,2,\ldots, n \right\} \right)
       &\ne \lim_n \mathbf P\left( \left\{ 1,2,\ldots, n\right\} \right)  = 0
       \end{aligned}\]
       
       Moreover, $\{n+1, n+2, \ldots\} \downarrow \emptyset$, but 
       \[ 0 = \mathbf P(\emptyset) = \mathbf P\left( \bigcap_n \{n+1, n+2, \ldots\} \right) \ne
            \lim_n \mathbf P\left( \{n+1, n+2, \ldots \} \right) = 1 \]
            
       The CDF of the random variable $X:\mathbf N \to \mathbf N$, $X(n) = n$
       according to $\mathbf P$ will satisfy:
       \[ F_X(k) = \mathbf P\{n: X(n) \leq k\}=  0\]
       for all $n$, so $\lim F_X(k) = 0$ for $k\to\infty$. 
       #+LATEX: \clearpage
** Best linear predictor: matrix version
   :PROPERTIES: 
   :EXPORT_FILE_NAME: other_notes/N02_BLP_matrix.pdf
   :EXPORT_TITLE: Best linear predictor: matrix version
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_basic.tex}
   :EXPORT_OPTIONS: toc:nil
   :END:
   Let $M(n,k)$ denote the linear space of all matrix of dimension $n\times k$.

   Suppose we have random vectors $(\mathbf y(\omega), \mathbf z(\omega))'$. We
   know additionally that $\mathbf y \in M(n,1)$ and $\mathbf z \in M(k,1)$ and these vectors
   have finite mean and variance. Denote their mean by
   \[ \begin{bmatrix} \mu_y \\ \mu_z \end{bmatrix} \]
   and their variance matrix by
   \[ \begin{bmatrix} \Sigma_{yy} & \Sigma_{yz} \\ \Sigma_{zy} & \Sigma_{zz} \end{bmatrix} \]

   We define the *best linear predictor* of $\mathbf y$ given $\mathbf z$ as the random variable $\mathbf w$ such that
   \[ \mathbf w^* = \alpha^* + \beta^* (\mathbf z - \mu_z) \]
   where $\alpha^* \in M(n,1)$ and $\beta^* \in M(n,k)$ solve the minimzation problem
   \[ \min_{\alpha, \beta} \mathbf E \left[ \| \mathbf y - \alpha - \beta(\mathbf z - \mu_z) \|^2 \right]  \]

   You can solve it either by using calculus -- which can be cumbersome if you're
   not used to matrix derivatives -- or by noting that the minimand is a squared norm
   generated by the inner product
   \[ \langle \mathbf y, \mathbf w \rangle := \mathbf E[\mathbf w' \mathbf y] \]

   of all vectors of the type $\mathbf y - \mathbf w$ where $\mathbf w = \alpha + \beta(\mathbf z - \mu_z)$ for some $\alpha, \beta$.

   Let $\epsilon := \mathbf y - \mathbf w^*$ denote the residual of the
   minimization problem. Then $\epsilon$ must be orthogonal (by Hilbert's
   projection theorem) to every $\mathbf w = \alpha + \beta (\mathbf z - \mu_z)$.

   Taking $\beta=0$, we see that $\mathbf w^*$ must satisfy
   \[  0 = \langle \mathbf y - \mathbf w^*, \alpha \rangle = \mathbf E\left[ \alpha' \mathbf y  \right] - \mathbf E\left[ \alpha' \alpha^*  \right] \]
   for all vectors $\alpha \in M(n,1)$. Taking these to be the elements of the canonical basis, we conclude that
   \[ \alpha^* = \mu_y\]

   Now take $\alpha=0$. The orthogonality condition now implies that for any $\beta \in M(n,k)$,
   \[ 0 = \langle \mathbf y - \beta^* (\mathbf z - \mu_z) , \beta ( \mathbf z - \mu_z ) \rangle  =  \mathbf E\left[ (\mathbf z -\mu_z)' \beta' y  \right] - \mathbf E\left[ (\mathbf z -\mu_z)' \beta' \beta^* (\mathbf z - \mu_z)  \right]  \]

   Use the properties of the trace -- namely, that it's linear and that matrix
   multiplication commutes inside it -- and of the expectation operator to
   conclude that
   \[ \tr \left(  \beta' \mathbf E\left[ \mathbf y(\mathbf z - \mu_z)' \right] \right)  = \tr \left(\beta' \beta^* \mathbf E\left[ \left( \mathbf z - \mu_z   \right) \left( \mathbf z - \mu_z  \right)' \right] \right) \]

   note that $\mathbf E[\mathbf y(\mathbf z - \mu_z)'] = \Sigma_{yz}$ and
   $\mathbf E[(\mathbf z-\mu_z)(\mathbf z - \mu_z)'] = \Sigma_{zz}$. The equation above then implies that
   \[ \tr (\beta' \Sigma_{yz} ) = \tr (\beta' \beta^* \Sigma_{zz}) \]

   should hold for all matrices $\beta \in M(n,k)$. That implies,[fn:2]
   \[\Sigma_{yz} = \beta^* \Sigma_{zz} \]
   which in turn yields $\beta^* = \Sigma_{yz} \Sigma_{zz}^{-1}$ whenever
   $\Sigma_{zz}$ has an inverse. In that case, the BLP is
   #+NAME: eq:expression_blp
   \begin{equation} \mathbf w^*  = \mu_y + \Sigma_{yz} \Sigma_{zz}^{-1} (\mathbf z - \mu_z)  \end{equation}

**** Appendix: the Trace operator
     - let $A(i,j)$ denote the entry $(i,j)$ of any matrix
     - Let $A$ be a $m\times n$ matrix. The trace is defined as
       \[ \tr A = \sum_{i=1}^{\min\{m,n\}} {A(i,i)}\]
       in other words, it's just the sum of elements in the main diagonal.
     - Some properties of the trace:
       1. $\tr(A + B) = \tr(A) + \tr(B)$ whenever $A$ and $B$ have similar dimensions
       2. $\tr(kA) = k\,\tr(A)$ for all scalars $k$
       3. $\tr(AB) = \tr(BA)$ whenever dimensions are such that both multiplications make sense
       Curiosity: any operation $\tilde\tr$ that satisfies the properties above
       is equal to $\tr$ (modulo multiplication by a constant)
     - The trace and expectation operators commute:
       \[\tr (\mathbf EA) = \mathbf E (\tr A)\]
     - Suppose $A \in M(m,n)$ and you want to select element $(i,j)$ from it. Note that
       \[ A(i,j) =  e_i' A \varepsilon_j = tr(e_i' A \varepsilon_j) = tr(\varepsilon_j e_i' A) \]
       where $e_i$ is the i-th element in the canonical basis of $R^m$ and
       $\varepsilon_j$ is the j-th element of the canonical basis of $R^n$.

       Hence for any $(i,j)$, letting $B = \varepsilon_j e_i' \in M(n,m)$ we have
       \[A(i,j) = \tr ( B A ) \]
     - This implies that if $A$ and $\tilde A$ are fixed $m\times n$ matrices, and
       \[ \tr(BA) = tr(B\tilde A) \]
       holds for every $B \in M(n,m)$, then
       \[ A = \tilde A\]
     #+LATEX: \clearpage

** Admissible tests and maximization of power subject to size (WIP)
   :PROPERTIES:
   :EXPORT_FILE_NAME: other_notes/N03_admissibility_maxpower.pdf
   :EXPORT_TITLE: Admissible tests and maximization of power subject to size (WIP)
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_basic.tex}
   :EXPORT_OPTIONS: toc:nil
   :END:

   In lecture notes 9-10, Proposition 1 characterizes admissible tests in terms
   of the solution of a problem of maximizing power subject to a size
   constraint. I reproduce the statement of that proposition below.

   #+NAME: prop:admissible-test-characterization
   #+BEGIN_prop
   Suppose that for any set $A \subseteq \mathbf{X}$
   $$\int_{A} f(x,\theta_0)dx > 0 \implies \int_{A} f(x,\theta_1)dx > 0 .$$
   A randomized test $\phi$ is admissible if and only if there exists $\alpha \in [0,1]$ such that $\phi$ maximizes power subject to having size at most $\alpha$; that is
   #+NAME: equation:optimization
   \begin{equation}
   \phi \in \arg \max_{\phi} \left( 1-R(\phi, \theta_1) \right)
   \end{equation}
   \noindent s.t.
   #+NAME: equation:sizecontrol
   \begin{equation}
   R(\phi, \theta_0) \leq \alpha
   \end{equation}
   #+END_prop


   That proposition is actually really nice. In standard statistics courses, we
   sometimes take this maximization problem as the starting point, as if it's
   somehow self-evident that we should seek tests that /maximize power subject
   to size/. With the decision theoretic framework we built in the first few
   lectures, we can actually understand why tests that solve this maximization
   problem are of any interest to us. The reason is that this procedure yields
   tests that aren't dominated.

   Another way of framing the proposition is the following. For a fixed
   $\alpha\in[0,1]$, let $\Phi^*(\alpha)$ denote the set of all tests $\phi^*$
   that maximize ([[equation:optimization]]) subject to ([[equation:sizecontrol]]).

   The correspondence $\Phi^*(\alpha)$ depends on a single parameter
   $\alpha \in [0,1]$. What proposition 1 says is that, as we vary $\alpha$,
   we cover all possible admissible tests. In other words,
   \[ \mathcal A = \bigcup_{\alpha \in [0,1]} \Phi^*(\alpha) \]
   is /exactly/ the set of all admissible tests.
**** Understanding Proposition [[prop:admissible-test-characterization]]
     I modify the proposition's exposition to make it a bit more digestible.

     First, let's define the following.
     #+BEGIN_defi
     Let $\{f_\theta(x)\}_{\theta \in \Theta}$ be a statistical model. We say that
     \[ f_{\theta_0} \ll f_{\theta_1} \]
     \noindent (in plain English: $f_{\theta_0}$ is /dominated/ by $f_{\theta_1}$) if, for every
     measurable set $A$,
     \[ \mathbf P_{\theta_1}(A) = 0 \implies \mathbf P_{\theta_0}(A) = 0 \]
     #+END_defi

     *Important remark.* The relation $\ll$ has /nothing/ to do with risk, loss,
     etc. It also has nothing to do with stochastic dominance.

     Let's translate the definition above. What it means for $f_{\theta_0}$ to
     be dominated by $f_{\theta_1}$ is that, if the statistical model under
     $\theta_1$ assigns zero probability to a set $A$ -- that is, there is a
     zero probability that we observe data in the set $A$ under the alternative
     -- then the probability that we observe data in the set $A$ under the null
     must also be zero.

     In other words, if that condition didn't hold, there would be a set of data
     realizations that are ``impossible'' under the alternative, but ``possible''
     under the null.

     Note that we can rewrite the definition in terms of integrals, since
     \[ \mathbf P_\theta(A) = \int_A f_\theta(x) dx \]

     Hence, $f_{\theta_0} \ll f_{\theta_1}$ if and only if
     \[ \int_{A} f_{\theta_1} (x) dx = 0 \implies \int_A f_{\theta_0}(x) dx = 0 \]

     Or yet (by contraposition): $f_{\theta_0} \ll f_{\theta_1}$ iff
     \[ \int_{A} f_{\theta_0} (x) dx > 0 \implies \int_A f_{\theta_1}(x) dx > 0 \]

     All of these are restatements of the assumption that we can't observe
     under the null things that can't be observed under the alternative.

     That assumption gives us an important result, that I state as a lemma.
     #+NAME: lemma:trivialrisk
     #+BEGIN_lemma
     Let $\{f_\theta\}_{\theta \in \Theta}$ be a statistical model with $\Theta = \{\theta_0, \theta_1\}$.
     Suppose $f_{\theta_0} \ll f_{\theta_1}$.

     Then any test $\phi$ achieving full power must have size equals one. Mathematically:
     \[  \mathbf E_{\theta_1}[\phi(X)] = 1 \implies \mathbf E_{\theta_0}[\phi(X)] = 1\]

     Moreover, tests achieving zero size must have trivial power:
     \[ \mathbf E_{\theta_0}[\phi(X)] = 0 \implies \mathbf E_{\theta_1}[\phi(X)] = 0\]
     #+END_lemma

     #+BEGIN_proof
     Since $\phi(X) \leq 1$, full power -- ie $\mathbf E_{\theta_1}\phi(X) = 1$ -- implies that the set
     $A = \{x \in \mathcal X: \phi(x) < 1 \}$ has zero probability under $\theta_1$. Thus
     \[ \int_{\phi(x)<1} f_{\theta_1}(x) dx  = 0\]

     Since $f_{\theta_0}$ is dominated by $f_{\theta_1}$,
     \[\mathbf E_{\theta_0} \phi(X) = \int_{\{\phi(x) = 1\}} \phi(x) f_{\theta_0}(x)dx + \cancelto{0}{\int_{\{\phi(x) < 1\}} \phi(x) f_{\theta_0}(x)dx} = 1\]
     #+END_proof

     I'll now restate one directions of Proposition 1, for the particular case when $0 < \alpha < 1$.
     #+BEGIN_prop
     Let $\{f_\theta\}_{\theta \in \Theta}$ be a statistical model with $\Theta = \{\theta_0, \theta_1\}$.

     Suppose $f_{\theta_0} \ll f_{\theta_1}$. Then any (randomized) test
     $\phi^*$ that solves the problem below is admissible in a decision problem with 0-1
     loss, when $\alpha \in (0, 1)$.
     \[\tag{P}\begin{aligned} &\max_{\phi} && E_{\theta_1} \phi(X) \\
     & \text{s.t.} && E_{\theta_0} \phi(X) \leq \alpha
     \end{aligned}\]
     #+END_prop

     #+BEGIN_proof
     Let's proceed by contradiction. Assume that $\phi^*$ solves the
     maximization problem but is not admissible. Then there exists some test
     $\phi$ that dominates $\phi^*$, that is:

     #+NAME: eq:lower_risk_null
     \begin{equation}
      R(\phi, \theta_0) = \mathbf E_{\theta_0} [\phi(X)] \leq \mathbf E_{\theta_0}[\phi^*(X)] = R(\phi^*, \theta_0)
     \end{equation}

     #+NAME: eq:lower_risk_alt
     \begin{equation}
      R(\phi, \theta_1) = 1 - \mathbf E_{\theta_1}[\phi(X)] \leq 1 - \mathbf E_{\theta_1}[\phi^*(X)] = R(\phi^*, \theta_1)
     \end{equation}
     where one of the equalities holds strictly. We consider the two cases below.
     1. Suppose [[eq:lower_risk_null]] holds strictly, and [[eq:lower_risk_alt]] holds weakly.
        Since $\phi^*$ solves the maximization problem (P), the size constraint must
        be satisfied so
        \[ \mathbf E_{\theta_0} [\phi(X)] < \mathbf E_{\theta_0}[\phi^*(X)] \leq \alpha < 1  \]
        This first thing to note, which will only be used later on, is that since
        $\mathbf E_{\theta_0} [\phi(X)] < 1$, it must be that
        $\mathbf E_{\theta_1}[\phi(X)] < 1$ by the first part of
        Lemma [[lemma:trivialrisk]].

        The idea of the proof is to construct yet another test that will use up
        the slack that $\phi$ has in the size constraint,
        $\mathbf E_{\theta_0}[\phi(X)] < \alpha$, to achieve higher power.

        We can do that by mixing $\phi$ with the test that rejects the null
        for any realization,
        \[  \phi_R(X) \equiv 1  \]
        and by picking the right mix, we will increase power relative to $\phi$,
        while still controlling for size. By [[eq:lower_risk_alt]], we will also
        improve relative to $\phi^*$, a contradiction.

        Now how do we find that combination? Consider, for arbitrary
        $\lambda\in[0,1]$, the test
        \[ \phi_\lambda(X) \equiv \lambda \phi^R(X) + (1-\lambda) \phi(X)  \]
        (Make sure you understand why we combine $\phi$ with $\phi^R$, in
        particular why we don't combine $\phi^R$ with $\phi^*$.) Its rate of type I error is given by
        \[  \mathbf E_{\theta_0} [ \phi_\lambda (X) ]  = \lambda + (1-\lambda) E_{\theta_0} [\phi(X)]  \]

        We pick $\bar\lambda$ that gives size exactly equal to $\alpha$ by setting
        \[ \bar\lambda = \frac{\alpha - \mathbf E_{\theta_0}[\phi(X)]}{1 - \mathbf E_{\theta_0}[\phi(X)]} \]

        Since $0 \leq E_{\theta_0}[\phi(X)] < \alpha < 1$, we have $\bar\lambda\in(0,1)$.

        By construction, $\phi_{\bar \lambda}$ has a rate of type I error of
        exactly $\alpha$. Its power on the other hand is given by
        \[ \mathbf E_{\theta_1} [ \phi_{\bar\lambda}(X) ] = \bar\lambda \cdot 1 + (1-\bar\lambda) \cdot \mathbf E_{\theta_1} [\phi(X)] \]

        Because $\mathbf E_{\theta_1}[\phi(X)] < 1$ and $\bar\lambda \in (0,1)$, the above expression implies
        \[ \mathbf E_{\theta_1}[\phi_{\bar\lambda}(X)] > \mathbf E_{\theta_1}[\phi(X)] \geq \mathbf E_{\theta_1}[\phi^*(X)] \]

        Where the last inequality comes from the assumption ([[eq:lower_risk_alt]]). That is a contradiction with the
        fact that $\phi^*$ is solves problem (P).
     2. Suppose now that ([[eq:lower_risk_alt]]) holds strictly, while
        ([[eq:lower_risk_null]]) holds weakly. Then ([[eq:lower_risk_null]]) implies
        $\phi$ satisfies the size constraint, and
        \[ \mathbf E_{\theta_1}[\phi(X)] > \mathbf E_{\theta_1}[\phi^*(X)] \]
        implies that $\phi$ achieves strictly higher power than $\phi^*$, in
        direct contradiction with the fact that $\phi^*$ solves problem (P).
     #+END_proof

** Asymptotic order notation & more
   :PROPERTIES:
   :EXPORT_FILE_NAME: other_notes/N04_asymptotic_order.pdf
   :EXPORT_TITLE: Asymptotic order notation & more
   :EXPORT_LATEX_HEADER: \input{../auxfiles/header_basic.tex}
   :EXPORT_OPTIONS: toc:nil
   :END:
   
   These notes are a slight modification of Recitation 7 from Fall 2018 intro to metrics.
   
*** Asymptotic order notation
    It is intuitive to argue that the sequence $x_n = n$ diverges to infinity
    faster than the sequence $y_n = \sqrt{n}$. But why is that the case? One way
    to frame this would be to look at the difference between $y_n$ and $x_n$. As
    $n$ grows, the values of $|x_n - y_n|$ become larger and larger. 

    However, this is not the only way of comparing ``orders of magnitude''. Take
    for example the sequences $a_n = n$ and $b_n = 5n$. Would you say that $b_n$
    diverges to infinity ``an order of magnitude faster'' than $a_n$? That would
    of course depend on what criterion you are using, but perhaps it would make
    sense to say that since they're both ``diverging linearly'' infinity, their
    asymptotic order is the same.
   
    The standard way of comparing asymptotic orders of $x_n$ and $y_n$ depends on
    the ratio $x_n/y_n$ instead of the difference. That way, for example, the
    asymptotic order of $x_n$ is the same as $C x_n$ for any constant $C$. Define
    the ratio $r_n := x_n/y_n$. Modulo some technical qualifications, we say that
    1. $x_n$ is dominated by (or: is an order of magnitude below) $y_n$ if $r_n \to 0$;
    2. $x_n$ is at most the same order as $y_n$ if $r_n$ is bounded after some index $n_0$; that is 
       there exists $M$ such that 
       \[ |r_n| \leq M \]
       for all $n \geq n_0$

    The common way of designating 1 and 2 above is, respectively: 
    1. $x_n \in o(y_n)$
    2. $x_n \in O(y_n)$
    and in English would say that (1) $x_n$ is ``little oh'' of $y_n$, or (2)
    $x_n$ is ``big oh'' of $y_n$. 

    The technical qualification omitted above is one that deals with the signs of
    $x_n$ and $y_n$. To avoid any technical complications, we typically require
    $y_n$ to be strictly positive (at least after some index $n_1$). This is not
    a big deal because the right hand side of statements such as $x_n \in O(y_n)$
    typically involve a class of sequences such as $n$, $log(n)$, $1/n$, etc,
    which are always positive anyway.

    Also, if you are crazy about limits, one equivalent way of saying that $x_n \in
    O(y_n)$ is that
    \[ \limsup \left| \frac{x_n}{y_n} \right| < \infty \]

    It will be useful to familiarize yourself with asymptotic order notation.
    First, convince yourself that $n \in o(n^2)$, $\log(n) \in O(n)$, etc. Then,
    solve the following exercises (which should follow very easily from the
    definition):
    - $x_n \in O(x_n)$ for every sequence $x_n$; the same isn't true for $o(x_n)$.
    - If $x_n \in An^2 + Bn + C$ with $A > 0$, then $x_n \in O(n^2)$. Also, $x_n \in o(x^3)$.
    - The fact above extends to degree $k$ polynomials.
    - $x_n \in o(y_n)$ if and only if $x_n/y_n \in o(1)$; same holds for $O$.
    - $x_n \in o(1)$ if and only if $x_n \to 0$
    - If $x_n \to x$ then $x_n \in O(1)$
     
    The properties below are stated as a Lemma just because they connect with
    later sections, but are also not hard to prove.
   
    #+NAME: lem:determ_O_prop
    #+BEGIN_lemma
    Regarding sequences $x_n$, $y_n$, $z_n$ and $w_n$, where $z_n$ and $w_n$ are positive: 
    1) If $x_n \in o(z_n)$ and $y_n \in o(w_n)$, then $x_n y_n \in o(z_n w_n)$
       and $x_n + y_n \in o(z_n + w_n)$; same holds for $O$
    2) If $x_n \in o(z_n)$ and $y_n \in O(w_n)$, then $x_n y_n \in o(z_n w_n)$ and $x_n + y_n \in O(z_n+w_n)$
    #+END_lemma
   
   One hint about proving the above lemma: since $w_n$ and $z_n$ are always
   positive, the ratio 
   \[ \frac{w_n}{w_n + y_n}\]
   is necessarily bounded (in fact, it is in $(0,1)$). This may be helpful if one writes 
   \[ \frac{x_n + y_n}{z_n + w_n} = \frac{x_n}{z_n}\frac{z_n}{z_n+w_n} + \frac{y_n}{w_n}\frac{w_n}{z_n+w_n}\]
   because then the terms multiplying $x_n/z_n$ and $y_n/w_n$ are both bounded.
  
*** Limit superior and limit inferior                              :noexport:

    For any sequence $a_n$, we can define /limit superior/ and /limit inferior/:
    \[\limsup a_n = \lim_{N\to\infty} \sup_{n\geq N} a_n \]
    \[\liminf a_n = \lim_{N\to\infty} \inf_{n\geq N} a_n \]

    It is not hard to show that they satisfy the properties below.
   
    #+BEGIN_lemma
    Let $(a_n)$ and $(b_n)$ be sequences of real numbers.
    1) $\limsup a_n$ and $\liminf a_n$ exist; if $a_n$ is bounded from above
       (below) then the limit superior (inferior) is finite.
    2) There exist subsequences $a_{n_k}$, $a_{m_j}$ such that
       \[\lim a_{n_k} = \limsup a_n \]
       \[ \lim a_{m_j} = \liminf a_n\]
       moreover, for any convergent subsequence $a_{r_p}$,
       \[ \liminf a_n \leq \lim_p a_{r_p} \leq \limsup a_n \]
    3) Whenever $a_n \to a$, then $\limsup a_n = \liminf a_n = a$
    4) The limit superior (inferior) is subadditive (superadditive). That is:
       \[\liminf a_n + \liminf b_n \leq \liminf a_n + b_n \leq \limsup a_n + b_n \leq \limsup a_n + \limsup b_n\]
    #+END_lemma
   
   
*** Convergence in Probability
    Random variables are more complicated objects than real numbers. This is
    because random variables are /measurable maps between measurable spaces/;
    generally speaking, these are infinite dimensional spaces and defining
    notions such as ``distance'' and ``convergence'' there is sometimes tricky.

    What we do here is to specialize. We will define what it means for a
    sequences of random variables $X_n$ to *converge in probability* to a
    (constant) real number $a$. Before that, I'll introduce a weaker concept the
    stochastic analog of the $O(1)$ notation. Remember that a sequence of real
    numbers $x_n \sim O(1)$ if $x_n$ is bounded after $n\geq n_0$.

    #+BEGIN_defi
    The sequence of random variables $(X_n)$ is said to be *stochastically
    bounded*, denoted in short by $X_n \in O_p(1)$, if for every $\delta > 0$,
    there exists $M_\delta$ and $n_0$ such that
    \[ \Pr\left\{ |X_n| \leq M_\delta \right\} > 1 - \delta \]
    holds for every $n \geq n_0$. 
    #+END_defi

    Instead of requiring $|X_n| \leq M$ to hold strictly for large $n$, we
    require it to hold /with some probability/ after for large $n$. Moreover,
    perhaps by choosing a large enough bound, this probability can be made
    arbitrarily close to $1$.

    The reason why this is analogous to the non-stochastic $O$ notation is that
    if $X_n$ is a deterministic sequence[fn:3] then $X_n \in O_p(1)$ if, and only
    if, $X_n = O(1)$. In analogy with the deterministic case, we define $X_n$ to
    be $O_p(Y_n)$ if $X_n/Y_n \in O_p(1)$.

    The proposition below provides one type of sequence that is always
    stochastically bounded: the ones that converge in distribution.
   
    #+NAME: prop:convdist_Op1
    #+BEGIN_prop
    Let $(X_n)$ be a sequence of random variables and $Y$ any random variable. If
    $X_n$ converges in distribution to $Y$, then $X_n \in O_p(1)$
    #+END_prop
    #+BEGIN_proof
    Let $F_Y$ be the cdf of $Y$. For fixed $\delta > 0$, take $M_\delta$ such that: 
     1) $F_Y$ is continuous at $M_\delta$ and $-M_\delta$. 
     2) $F_Y(M_\delta) - F_Y(-M_\delta) > 1 - \delta/2$
       
        The existence of such $M_\delta$ comes from the ``continuity property''
        of probability measures: since $(-n, n] \uparrow \mathbf R$, $F_Y(n) - F_Y(-n) \uparrow 1$; hence, 
        $F_Y(n_0) - F_Y(-n_0) > 1-\delta/2$ for some $n_0$. 
    
     The two requirements above can be met simultaneously because $F_Y$ is
     monotone, and a monotone function in the real line can have at most a
     countable number of discontinuities. Given that, and that the fact that
     there are uncountably many sets $[-x, x]$ where $x > n_0$; one of them has
     to be such that $x$ and $-x$ are continuity points of $F_Y$. (Otherwise
     $F_Y$ would have uncountably many discontinuities.)
    
     The definition of convergence in distribution then implies that
     $F_{X_n}(M_\delta) \to F_Y(M_\delta)$ and $F_{X_n}(-M_\delta) \to F_Y(-M_\delta)$. Thus 
     we can pick $n_0$ such that for $n\geq n_0$, 
     \[ F_{X_n}(M_\delta) > F_Y(M_\delta) - \delta/4\]
     and 
     \[ F_{X_n}(-M_\delta) < F_Y(-M_\delta) + \delta/4\]

     Thus 
     \[\begin{aligned} \Pr\left\{ |X_n| \leq M_\delta \right\} &\geq F_{X_n}(M_\delta) - F_{X_n}(-M_\delta) \\
                       &> F_Y(M_\delta) - F_Y(-M_\delta) - \delta/2 \\
                       &> 1 - \delta \end{aligned}\]
    #+END_proof

    We now move to convergence in probability.
    #+BEGIN_defi
    Let $(X_n)$ be a sequence of real valued random variables. The sequence
    *converges in probability* to $a$ if, for every $\epsilon > 0$, and
    $\delta>0$, there exists $n_0$ such that
    \[ \Pr \left\{ |X_n - a| \leq \epsilon \right\} \geq 1 - \delta\]
    if $n\geq n_0$.
    #+END_defi
   
    Again, we can't make $|X_n - a|$ arbitrarily small with certainty, but we can
    make it arbitrarily small with some probability, and we can make this
    probability close to $1$. Convergence in probability is denoted by the
    $\probto$ symbol; that is, convergence in probability of $X_n$ in to $a$ is
    denoted by
    \[X_n \probto a\]

    Whenever $X_n$ is a deterministic sequence, convergence in probability is
    equivalent to ``regular'' convergence. That is because the definition of
    convergence of real numbers states that for any $\epsilon > 0$, there exists
    $n_0$ such that
    \[  |X_n  - a | \leq \epsilon \]
    happens for all $n \geq n_0$. In particular, starting from $n_0$, 
    \[ \Pr\{ | X_n - a | \leq \epsilon \} = 1 \]
    which implies convergence in probability.

    Convergence in probability is all we need to define the `stochastic little
    o' notation. So without further ado:
    #+BEGIN_defi
    Let $(X_n)$ and $(Y_n)$ be sequences of real valued random variables. We say
    that: 
    1) $X_n \in o_p(1)$ if $X_n \probto 0$.
    2) $X_n \in o_p(Y_n)$ if $X_n/Y_n \in o_p(1)$
    #+END_defi
   
    The facts of Lemma [[lem:determ_O_prop]] go through substituting $O$ with $O_p$
    and $o$ with $o_p$, but proving them is slightly more difficult here. Because
    it may be instructive, I'll restate it in the stochastic form, and provide a
    proof.
   
    #+NAME: prop:Oarithmetics
    #+BEGIN_prop
    Let $(X_n)$, $(Y_n)$, $(Z_n)$ and $(W_n)$ be sequences of real valued random
    variables where $Z_n, W_n$ are positive. Then:
    1) If $X_n \in o_p(Z_n)$ and $Y_n \in o_p(W_n)$, then $X_nY_n \in
       o_p(Z_nW_n)$ and $X_n + Y_n \in o_p(Z_n + W_n)$. Same holds for $O_p$.
    2) If $X_n \in o_p(Z_n)$ and $Y_n \in O_p(W_n)$, then $X_nY_n \in o_p(Z_nW_n)$
       and $X_n + Y_n \in O_p(Z_n + W_n)$.
    #+END_prop
   
    #+BEGIN_proof
    For part (1), I'll do the proof for $o_p$ only; the $O_p$ analog is very similar.

    Start with $X_n + Y_n$. For any $n \in \mathbf N$, we can write 
    \[\begin{aligned} \frac{ | X_n + Y_n | }{ Z_n + W_n } &\leq \frac{ |X_n| }{Z_n + W_n} + \frac{ |Y_n| }{Z_n + W_n}\\
                                                          & = \frac{|X_n|}{Z_n}\frac{Z_n}{Z_n + W_n} + \frac{|Y_n|}{W_n}\frac{ W_n }{Z_n + W_n} \end{aligned}\]
                                                         
    If $|X_n| \leq \epsilon Z_n$ and $|Y_n| \leq \epsilon W_n$, then the above
    inequality implies $|X_n + Y_n| \leq \epsilon (Z_n + W_n)$. In other words,
    the following relationship holds between events: 
    #+NAME: setineq:i1
    \begin{equation} \left\{ |X_n| \leq \epsilon Z_n \right\} \cap \left\{ |Y_n| \leq \epsilon W_n \right\} \subseteq \left\{ |X_n + Y_n| \leq \epsilon (Z_n + W_n) \right\} \end{equation}
   
    Let $\delta > 0$ and $\epsilon > 0$. Fix $n_0$ such that, for $n \geq n_0$, 
    \[ \Pr \{|X_n| \leq \epsilon Z_n \} > 1 - \frac{\delta}{2} \]
    and similarly
    \[ \Pr \{|Y_n| \leq \epsilon W_n \} > 1 - \frac{\delta}{2} \]

    Using the relationship $P(A_n \cap B_n) = 1 - P(A_n^c) - P(B_n^c)$, we get 
    \[ \Pr \left(\{|X_n|\leq \epsilon Z_n\} \cap \{|Y_n| \leq \epsilon W_n\} \right) > 1-\delta \]
   
    If $n \geq n_0$, from [[setineq:i1]] and the above inequality we conclude that 
    \[ \Pr \{ |X_n + Y_n| \leq \epsilon(Z_n + W_n) \} \geq 1 - \delta\]
    which establishes that $X_n + Y_n \in o_p(Z_n + W_n)$.

    Proving that $X_n Y_n \in o_p(Z_n W_n)$ is a quite similar
    exercise. Just note that 
    \[ \left\{ |X_n| \leq \sqrt{\epsilon} Z_n \right\} 
        \cap \left\{ |Y_n| \leq \sqrt{\epsilon} W_n \right\} 
        \subseteq \left\{ |X_n Y_n| \leq \epsilon Z_nW_n \right\} \]
    so for $\delta,\epsilon >0$ one can make 
    $\Pr\{|X_nY_n| \leq \epsilon Z_nW_n\} > 1-\delta$ by taking $n \geq n_0$ where $n_0$ 
    satisfies 
    \[
    \Pr\left\{|X_n| \leq \sqrt{\epsilon} Z_n \right \} > 1-\frac{\delta}{2}
    \]
    and 
    \[
    \Pr\left\{|X_n| \leq \sqrt{\epsilon} Z_n \right \} > 1-\frac{\delta}{2}
    \]
    for $n\geq n_0$. 
   
    I proceed to part (2). Let $X_n \in o_p(Z_n)$ and $Y_n \in O_p(W_n)$. For
    positive $\epsilon, \delta$, we can find: 
    1) $M_\delta > 0$ and $n_0$ such that $\Pr \{|Y_n| \leq M_\delta W_n\} > 1-\frac{\delta}{2}$
    2) $n_1$ such that $\Pr\{|X_n| \leq \epsilon Z_n/M_\eta\} > 1 - \frac{\delta}{2}$
    Taking $n_0 = \max\{n_1, n_2\}$ we get, for $n\geq n_0$, 
    \[\begin{aligned} \Pr\{ |X_nY_n| \leq \epsilon Z_nW_n \} 
       &\geq  \Pr \left( \left\{|X_n| \leq \frac{\epsilon}{M_\eta} Z_n \right\} 
              \cap \left\{ |Y_n| \leq M_\eta W_n \right\}  \right) \\ 
       &> 1-\delta\end{aligned}\]
    Hence $X_n Y_n \in o_p(1)$. The fact that $X_n + Y_n \in O_p(Z_n + W_n)$ is
    left as an exercise.
    #+END_proof
   
   
**** Applications
    
     The LLN states that if $(X_i)$ is an iid sequence of variables with finite
     first moment, then 
     \[ \bar X_n \probto \mu \]
   
     In terms of our stochastic order notation, this implies 
     \[ \bar X_n - \mu = o_p(1) \]
     it is common to rewrite this as 
     \[ \bar X_n = \mu + o_p(1)\]
    
     If in addition to the first moment, the second moments are also finite, we have 
     \[ \sqrt{n} (\bar X_n - \mu) \distto N(0, \sigma^2)\]
    
     By Proposition [[prop:convdist_Op1]], this implies $\sqrt{n}(\bar X_n -  \mu)= O_p(1)$
     hence using $O_p$ properties from Proposition [[prop:Oarithmetics]], we conclude that
     \[ \bar X_n - \mu = O_p\left(\frac{1}{\sqrt{n}}\right)  \]
     so the rate of convergence of $\bar X_n$ to $\mu$ is ``no slower than''
     $n^{-1/2}$. 
    
    
     It has been proved in class that if $X_n \distto X$ and $a_n \probto a$
     where $a$ is constant, 
     \[ X_n + a_n \distto X + a\]
     The fact above implies that if $X_n \distto X$, then 
     $X_n + o_p(1) \distto X$. 

     One important use of the $O_p$ machinery is when dealing with Taylor
     expansions. This is exemplified by the Lemma below: 
     #+NAME: lem:taylor_op
     #+BEGIN_lemma
     Let $X_n = \mu + o_p(1)$, and $g$ be a function that is differentiable at $\mu$. Then 
     \[ g(X_n) - g(\mu) =  (X_n - \mu)[ g'(\mu) + o_p(1)] \]
     #+END_lemma
     #+BEGIN_proof

     Let 
      \[r(x) = \begin{cases} 0 & \text{if }x = \mu \\ 
     \frac{g(x) - g(\mu) - g'(\mu)(x-\mu)}{x - \mu} & \text{if }|x - \mu| > 0\end{cases} \]
   
     Then $g(X_n) - g(\mu) = (X_n-\mu)[g'(\mu) + r(X_n)]$; we will show that $r(X_n) \in o_p(1)$.

     Let $\epsilon > 0$ and $\delta > 0$ be arbitrary. By differentiability,
     there exists $\eta>0$ such that $|X_n - \mu| < \eta$ implies
     $|r(X_n)| \leq \epsilon$. Take $n_1$ such that $n\geq n_1$ implies 
     \[ \Pr \{ |X_n - \mu| < \eta \} > 1-\delta \]
    
     It follows from the inclusion $\{ |X_n - \mu | < \eta \} \subseteq \{ |r(X_n)|\leq \epsilon \}$ that if $n\geq n_1$,
     \[ \Pr\{|r(X_n)| \leq \epsilon\} \geq \Pr\{|X_n - \mu| < \eta\} > 1 - \delta \]
     #+END_proof

     We are now equipped to prove what is generally called the Delta method; this
     involves computing limiting distributions of sequences such as 
     \[ Z_n = \sqrt{n} \left( \bar x_n^2 - \mu^2 \right)  \]
     when we know that the CLT applies to $\bar x_n$. For $\mu \ne 0$, we'll show that
     \[ Z_n \distto N(0, 4\mu^2 \sigma^2) \]
 
     That will be a consequence of the following theorem.
     #+BEGIN_thm
     Suppose $(X_n)$ is a sequence of random variables satisfying 
     \[ \sqrt{n} [X_n - \mu] \distto N(0, \sigma^2) \]
     for some $\mu$, $\sigma^2$. Let $g:\mathbf R\to\mathbf R$ be differentiable at $\mu$. If $\mu\ne0$, 
     \[ \sqrt{n} [g(X_n) - g(\mu)] \distto N(0, \sigma^2 g'(\mu)^2 ) \]
     #+END_thm

     #+BEGIN_proof
     From Lemma [[lem:taylor_op]] we can write $g(X_n) - g(\mu) = (X_n -
     \mu)g'(\mu) + (X_n - \mu) o_p(1)$. Multiplying both sides by $\sqrt{n}$
     yields
     \[ \sqrt{n} [g(X_n) - g(\mu)] = \sqrt{n} (X_n - \mu) g'(\mu) + \sqrt{n} (X_n - \mu) o_p(1)\]
    
     We know that $\sqrt{n}(X_n - \mu) \in O_p(1)$ by [[prop:convdist_Op1]], so that     
     \[ \sqrt{n}[g(X_n) - g(\mu)] = \sqrt{n} (X_n - \mu) g'(\mu) + o_p(1) \distto N(0, g'(\mu)^2 \sigma^2)\]
     #+END_proof

*** The Law of Large Numbers
    The statement of the weak law of large numbers is copied below.

    #+NAME: thm:weaklln
    #+BEGIN_thm 
    Let $(X_n)_{n\in\mathbf N}$ be a sequence of iid random variables for which $E|X_n|<\infty$. Define $\mu := \mathbf EX_n$ and $\bar X_N = \sum_{n=1}^N X_n/N$.
   
    Then $\bar{X}_N \probto \mu$.
    #+END_thm
   
   
    It basically says that if one draws a random sample from an iid
    distribution, the probability that the sample mean deviates from the true
    mean (by any fixed threshold $\epsilon>0$) can be made arbitrarily small.

   The proof of this theorem is easier if one assumes bounded second moments. The
   reason is that imposing bounded second moments restricts how fat the tail of
   the distribution of $X$ can plausibly be. 

   One inequality that generally relates the tail of random variables to their
   moments is the *Chebyshev* inequality. It provides the following tail bound,
   for every random variable $Y$ with bounded first moment: for any $\lambda>0$,
   \[\Pr \{ |Y| > \lambda \} \leq \frac{\mathbf E|Y|}{\lambda}\]
   (of course the inequality still holds if $Y$ has infinite first moment, but it 
   does not provide a useful bound.)
   
   If $|Y|$ has a bounded second moment, one can apply the bound to 
   $\{|Y|^2 > \lambda^2\}$ and then use the fact that $\{|Y|^2 > \lambda^2\} = \{ |Y| > \lambda\}$ 
   to conclude that
   \[ \Pr \{ |Y| > \lambda \} \leq \frac{\mathbf E(Y^2)}{\lambda^2}\]
  
   The above bounds can be applied to the sample mean $\bar X_N$. It follows from
   the linearity of expectations that $\mathbf E(\bar X_N) = \mu$, so that 
   \[ \Pr\left( \bar X_N \geq \lambda \right) = \frac{\mu}{\lambda} \]

   The second moment version of the tail bound suffices to prove convergence
   of $\bar X_N$ to $\mu$. We apply the bound to $Y_N = \bar X_N - \mu$; by the iid assumption and some
   algebra, it follows that 
   \[ \mathbf E(Y_N^2) = \mathbf V(\bar X_N) = \frac{\mathbf V(X_1)}{N} \]
   hence for each $\lambda > 0$, 
   \[ \Pr \{ |\bar X_N - \mu| > \lambda\} \leq \frac{V(X_1)}{N\lambda^2} \to 0 \]
   which establishes the WLLN when variance is assumed to be finite. 
  
   What if $\mathbf EX_i^2 = \infty$? While we can't exactly proceed as before,
   we can do a little trick. The idea is to /truncate/ the variables $X_i$ at
   $K$, to $\xism_i(K)$, where
      \[ \xism_i(K) = \begin{cases} X_i & \text{if } |X_i| \leq K \\ K & \text{otherwise}\end{cases} \]
     
   We can succintly express $\xism_i(K)$ as $\xism_i(K) = X_i \mathbf 1 \{|X_i| \leq K \}$. 

   The LLN follows from the facts below, whose proofs will be provided later on.
   1) For fixed $K$, $\xism_i(K)$ are iid and have finite second moments.
      By the LLN for variables with bounded variance, we have
      \[ \Pr\{| \overline{ \xism(K) }_N -  \mu_K |\geq \lambda \}  \to 0\]
      where of course $\overline{\xism(K)}_N = \frac{1}{N}\sum_{n=1}^N \xism_i(K)$ and $\mu_K = \mathbf E(\xism_i(K))$.
     
   2) Define $\xigt_i := X_i \mathbf 1\{ | X_i | > K \}$, so that $X_i = \xism_i(K) + \xigt_i(K)$
      for any threshold $K$. The following fact holds for any: 
      \[ \mathbf E|\xigt_i(K)| \to 0 \]
      as $K \to \infty$ (take $K$ to be positive integers).
   3) Taking $K\to\infty$, 
      \[ \mu_K = \mathbf E(\xism_i(K)) \to \mu \]
   4) We can write 
      #+NAME: eq:boundmean_threshold
      \begin{equation} \Pr \left\{ |\bar X_N - \mu | > \lambda \right\} \leq \Pr \left\{ |\overline{\xism(K)}_N - \mu_K| > \frac{\lambda}{3} \right\} + \Pr\left\{ |\mu_K - \mu| > \frac{\lambda}{3}\right\} + \Pr\left\{ | \overline{\xigt(K)}_N | > \frac{\lambda}{3} \right\} \end{equation}
   We can bound each of the terms in [[eq:boundmean_threshold]] conveniently. The third one involves the first
   moment tail bound: 
   \[ \Pr\left\{ | \overline{\xigt(K)}_N | > \frac{\lambda}{3} \right\} \leq 3\frac{ \mathbf E | \xigt_i(K) | }{\lambda}\] 
   By Fact 2 above, this can be made arbitrarily small if $K$ is chosen to be large enough.
  
   The second term in [[eq:boundmean_threshold]] can be actually set to $0$ by
   choosing $K$ large enough, since $\mu_K \to \mu$ as $K\to\infty$ (Fact 3). The
   bound on the first term comes from Fact 1.
  
  
  

* Footnotes
[fn:3] That is, for every $n$ there exists some $x_n$ such that the only
possible outcome is $X_n = x_n$.

[fn:2] See the appendix on the trace operator for details.

[fn:1]  The sets whose complement is finite are called co-finite sets.


